<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[爬取猫眼电影数据（一）]]></title>
    <url>%2F2017%2F09%2F24%2F%E7%88%AC%E5%8F%96%E7%8C%AB%E7%9C%BC%E7%94%B5%E5%BD%B1%E6%95%B0%E6%8D%AE-1%2F</url>
    <content type="text"><![CDATA[爬取猫眼电影数据(一) 概述：近期由于业务需要，需要将爬取猫眼网站中的部分数据作为公众号数据源，猫眼的电影数据相当之全备，大概收录了几十万部电影。当然这些数据我们也没那么容易就能爬取到的。遂将整个爬取过程记录如下。 1、使用chrome进入开发者模式，观察html结构、元素，找到我们所需数据，第一步我们需要现获取每一个电影的详情页面url。 注意，在爬取猫眼的电影数据时并不需要登录和携带cookie去请求html 2、 这里我们结合BeautifulSoup和xpath来实现对数据的过滤。实现代码如下： 12345678910111213def get_detail_url(self, targetUrl): """从概览页面中去获取剧透影片的详情页面""" html = self.getHtml(targetUrl) print html soup = BeautifulSoup(html, 'lxml') urls = soup.find_all('a', attrs=&#123;'data-act':'movie-click'&#125;) dd = soup.find_all('dd') print dd for url in urls: detailPath = etree.HTML(str(url)) detail_url = detailPath.xpath(r'//a/@href') for durl in detail_url: self.getContents('https://maoyan.com' + durl, proxys) 然后获取到对应每一个电影的详情url，进入详情页，分析所需数据对应html元素。在这里我们只需要三个字段、电影名称、电影海报url、电影上映时间。 提示:使用xpath时，最简单的方式是在chrome中点击选中具体某一个元素，然后右键——&gt; copy –&gt; copy xpath就能获取到对应的xpath路径 实现代码： 12345678910def getContents(self, url, proxys): """从详情页面中获取所需data""" html = self.getHtml(url,proxys) detailPath = etree.HTML(html) #电影名称 film_name = detailPath.xpath(r'/html/body/div[3]/div/div[2]/div[1]/h3/text()') #图片url pic_url = detailPath.xpath(r'/html/body/div[3]/div/div[1]/div/img/@src') #上映时间 release_time = detailPath.xpath(r'/html/body/div[3]/div/div[2]/div[1]/ul/li[3]/text()') 3、 这个时候获取到数据了，就该存库了。嗯，就存到mysql中吧。没啥说的，上代码。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748def insertData(self, my_dict): try: self.db.set_character_set('utf8') cols = ', '.join(my_dict.keys()) print cols values = '","'.join(my_dict.values()) print values sql = "INSERT INTO maoyan (%s) VALUES (%s)" % (cols, '"' + values + '"') try: result = self.cur.execute(sql) insert_id = self.db.insert_id() self.db.commit() # 判断是否执行成功 if result: return insert_id else: return 0 except MySQLdb.Error, e: # 发生错误时回滚 self.db.rollback() # 主键唯一，无法插入 if "key 'PRIMARY'" in e.args[1]: print self.getCurrentTime(), "数据已存在，未插入数据" else: print self.getCurrentTime(), "插入数据失败，原因 %d: %s" % (e.args[0], e.args[1]) except MySQLdb.Error, e: print self.getCurrentTime(), "数据库错误，原因%d: %s" % (e.args[0], e.args[1]) def dealData(self, my_dict): """先从数据库中查询这个影片名的记录，如果没有直接插入； 若是数据库中已经存在数据，那么什么都不做""" self.db.set_character_set('utf8') filmTitile = my_dict.get('film_title') result = self.findFilmByfilmTitle(filmTitile) if len(result) == 0: self.insertData(my_dict) return else: pass def findFilmByfilmTitle(self, filmTitile): """通过影片名称去数据库中查询""" self.db.set_character_set('utf8') sql = "select * from maoyan where film_title='%s'" % (filmTitile) self.cur.execute(sql) results = self.cur.fetchall() self.db.commit() return results 4、 万事俱备只欠启动了，跑啊跑啊。想必大家都想到了，猫眼肯定是不可能随便让人这样爬取他们的数据，而且短时间大量的请求对服务器的负载也是相当大的，不出意料爬取了不到300条数据，就被停止了。(ಥ _ ಥ)处理这种情况一般有两种方式，一种通过延缓请求，一种是通过代理ip的方式。第一种我尝试延缓5、10s，但并没有用。因而尝试用第二种方式，代理ip池的方式。代理ip如何使用在下一篇文章中记录。 下面是本次的完成代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133#!/usr/bin/env python# coding: utf-8import urllib2import urllibimport requestsfrom bs4 import BeautifulSoupfrom lxml import etreeimport MySQLdbimport timeimport randomimport sysreload(sys)sys.setdefaultencoding('utf8')"""猫眼不需要http请求的头部信息，添加上反而会出错"""class Maoyan(): # 获取当前时间 def getCurrentTime(self): return time.strftime('[%Y-%m-%d %H:%M:%S]', time.localtime(time.time())) def __init__(self): self.time = time self.base_url = "https://maoyan.com/films?showType=1&amp;sortId=2" try: self.db = MySQLdb.connect('localhost', 'flyer_user', 'Tu4a0X9hOPKz6jS!e', 'flyer_db',charset='utf8') self.cur = self.db.cursor() except MySQLdb.Error, e: print self.getCurrentTime(), "连接数据库错误，原因%d: %s" % (e.args[0], e.args[1]) # 插入数据 def insertData(self, my_dict): try: self.db.set_character_set('utf8') cols = ', '.join(my_dict.keys()) print cols values = '","'.join(my_dict.values()) print values sql = "INSERT INTO maoyan (%s) VALUES (%s)" % (cols, '"' + values + '"') try: result = self.cur.execute(sql) insert_id = self.db.insert_id() self.db.commit() # 判断是否执行成功 if result: return insert_id else: return 0 except MySQLdb.Error, e: # 发生错误时回滚 self.db.rollback() # 主键唯一，无法插入 if "key 'PRIMARY'" in e.args[1]: print self.getCurrentTime(), "数据已存在，未插入数据" else: print self.getCurrentTime(), "插入数据失败，原因 %d: %s" % (e.args[0], e.args[1]) except MySQLdb.Error, e: print self.getCurrentTime(), "数据库错误，原因%d: %s" % (e.args[0], e.args[1]) def dealData(self, my_dict): """先从数据库中查询这个影片名的记录，如果没有直接插入； 若是数据库中已经存在数据，那么什么都不做""" self.db.set_character_set('utf8') filmTitile = my_dict.get('film_title') result = self.findFilmByfilmTitle(filmTitile) if len(result) == 0: self.insertData(my_dict) return else: pass def findFilmByfilmTitle(self, filmTitile): """通过影片名称去数据库中查询""" self.db.set_character_set('utf8') sql = "select * from maoyan where film_title='%s'" % (filmTitile) self.cur.execute(sql) results = self.cur.fetchall() self.db.commit() return results def getHtml(self,url,proxys): """获取html""" req = urllib2.Request(url) html = urllib2.urlopen(req).read().decode('utf-8') return html def get_detail_url(self, targetUrl, proxys): """从概览页面中去获取剧透影片的详情页面""" html = self.getHtml(targetUrl, proxys) print html soup = BeautifulSoup(html, 'lxml') urls = soup.find_all('a', attrs=&#123;'data-act':'movie-click'&#125;) dd = soup.find_all('dd') print dd for url in urls: detailPath = etree.HTML(str(url)) detail_url = detailPath.xpath(r'//a/@href') for durl in detail_url: self.getContents('https://maoyan.com' + durl, proxys) def getContents(self, url, proxys): """从详情页面中获取所需data""" tmp1 = [] tmp2 = [] html = self.getHtml(url,proxys) detailPath = etree.HTML(html) film_name = detailPath.xpath(r'/html/body/div[3]/div/div[2]/div[1]/h3/text()') for name in film_name: tmp1.append("film_title") tmp2.append(name) pic_url = detailPath.xpath(r'/html/body/div[3]/div/div[1]/div/img/@src') for url in pic_url: strs = url.split('@') tmp1.append("pic_url") tmp2.append(strs[0]) release_time = detailPath.xpath(r'/html/body/div[3]/div/div[2]/div[1]/ul/li[3]/text()') for time in release_time: tmp1.append("release_time") tmp2.append(time[:10]) #将两个列表转换为字典，方便插入数据库 result_dict = dict(zip(tmp1, tmp2)) self.dealData(result_dict) def main(self): self.get_detail_url(self.base_url, proxys) for i in range(50): targetUrl = self.base_url + '&amp;offset=' + str((i+1)*30) self.get_detail_url(targetUrl, proxys)if __name__ == '__main__': maoyan = Maoyan() maoyan.main()]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>%2F2017%2F09%2F24%2Fpython%E7%88%AC%E8%99%AB%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96%E5%88%B0%E5%8F%AA%E9%9C%80%E8%A6%81%E7%9A%84%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[python爬虫如何获取到只需要的数据 概述：在爬虫的学习过程中，最麻烦的也许就是如何从一大堆html标签中过滤出我们所需要的，在这里有三种方法：re正则表达式、BeautifulSoup、使用lxml中的xpath。 1 、先介绍如何使用lxml中的xpath(1) 我们需要安装相应的依赖，然后在项目中导入相应的模块 12from lxml import etreeimport lxml (2) 之后再代码中转化成xpath12detailPath = etree.HTML(html) questions_titile = detailPath.xpath('//ul[@class="list-group"]/li/div/div[@class="question-title"]/a/text()') 注意上边的两个单引号之间的一串，前面两个’//‘是表示从这里开始，然后一级一级的去寻找所需要的元素，如果想获取属性值，例如是图片的url，最后可以使用@href，如果想获取里边的文本值，那就使用text()。 (3) 特殊用法12345678910from lxml import etreehtml=&quot;&quot;&quot; &lt;body&gt; &lt;div id=&quot;aa&quot;&gt;aa&lt;/div&gt; &lt;div id=&quot;ab&quot;&gt;ab&lt;/div&gt; &lt;div id=&quot;ac&quot;&gt;ac&lt;/div&gt; &lt;/body&gt; &quot;&quot;&quot;selector=etree.HTML(html)content=selector.xpath(&apos;//div[starts-with(@id,&quot;a&quot;)]/text()&apos;) #这里使用starts-with方法提取div的id标签属性值开头为a的div标签 这篇文章有简单应用：http://blog.csdn.net/winterto1990/article/details/47903653介绍 最后一个大绝招：直接在chrome中开发者模式，右键找到你想寻找的元素标签，copy xpath就能将这个元素对应的xpath找到！ 1 、python中提供的re模块 正则表达式是一种更为强大的字符串匹配、字符串查找、字符串替换等操作工具。上篇讲解了正则表达式的基本概念和语法以及re模块的基本使用方式，这节来详细说说 re 模块作为 Python 正则表达式引擎提供了哪些便利性操作。正则表达式的所有操作都是围绕着匹配对象(Match)进行的，只有表达式与字符串匹配才有可能进行后续操作。判断匹配与否有两个方法，分别是 re.match() 和 re.search()，两者有什么区别呢？ 区别：match 方法从字符串的起始位置开始检查，如果刚好有一个子字符串与正则表达式相匹配，则返回一个Match对象，只要起始位置不匹配则退出，不再往后检查了，返回 None 1234&gt;&gt;&gt; re.match(r&quot;b.r&quot;, &quot;foobar&quot;) # 不匹配&gt;&gt;&gt; re.match(r&quot;b.r&quot;, &quot;barfoo&quot;) # 匹配&lt;_sre.SRE_Match object at 0x102f05b28&gt;&gt;&gt;&gt; search 方法虽然也是从起始位置开始检查，但是它在起始位置不匹配的时候会一直尝试往后检查，直到匹配为止，如果到字符串的末尾还没有匹配，则返回 None123&gt;&gt;&gt; re.search(r&quot;b.r&quot;, &quot;foobar&quot;) # 匹配&lt;_sre.SRE_Match object at 0x000000000254D578&gt;&gt;&gt;&gt; re.match(r&quot;b.r&quot;, &quot;foobr&quot;) # 不匹配 但是二者都是匹配到就停止匹配了，哪怕还有更多能匹配的也不管。 参考：https://foofish.net/re-tutorial.html 、https://foofish.net/crawler-re-second.html]]></content>
  </entry>
  <entry>
    <title><![CDATA[上传本地jar包到nexus仓库中]]></title>
    <url>%2F2017%2F09%2F16%2F%E4%B8%8A%E4%BC%A0%E6%9C%AC%E5%9C%B0jar%E5%8C%85%E5%88%B0nexus%E4%BB%93%E5%BA%93%E4%B8%AD%2F</url>
    <content type="text"><![CDATA[#上传本地jar包到nexus仓库中 概述：有时在我们的nexus仓库中没有项目中所需的jar包，这个时候就需要从网络中下载，然后上传到nexus仓库中，然后我们就可以嗨森的敲代码啦。 1、首先需要是对应的repository有上传的权限 2、我们这里讲jar包上传到3rd party仓库中。先点击artifact upload然后选择GAV definition，手动将group id、artifact、version填入，然后packaging选择jar。 3、然后点击select artifact（s） to upload将本地的jar加入，点击add artifact，然后点击upload artifact。 4、最后刷新就能看到刚才加入的jar包。 此外需要注意的是，将入到3rd仓库之后可能对应的坐标会发生变化。如果前后坐标没有对应上，那么在pom文件中还是找不到相应的依赖。]]></content>
      <categories>
        <category>java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[hive如何实现自定义函数]]></title>
    <url>%2F2017%2F09%2F14%2Fhive%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[hive如何实现自定义函数 概述：我们在使用hive的时候，很多时候执行复杂的sql时，hive提供的函数无法满足我们的需求。这个时候就需要我们自己去定义一个函数。幸好，hive为我们允许使用java来子线自定义函数。 1、实现自定义UDF函数必须满足两个条件。 （1） 必须是org.apache.Hadoop.hive.ql.exec.UDF的子类 （2） 必须实现evaluate函数。 2、实现方法具体如下： （1）将$HIVE_HOME/lib中的两个jar包，分别是hive-contrib-2.1.1.jar和hive-exec-2.1.1.jar，把两个jar包放置到java的目录下，然后新建一个类，继承UDF。 123456789101112131415161718package org.apache.hadoop.hive.contrib.udf.example;import org.apache.commons.lang.StringUtils;import org.apache.hadoop.hive.ql.exec.UDF;import java.util.Arrays;public class ReturnMin extends UDF&#123; public static Integer evaluate(String str, String separator) &#123; int result = 100; if(StringUtils.isEmpty(str))&#123; return result; &#125; String[] strArray = str.split(separator); int[] intArray = new int[strArray.length]; for(int i = 0 ; i &lt; strArray.length ; i++) &#123; intArray[i] = Integer.parseInt(strArray[i]); &#125; Arrays.sort(intArray); return intArray[0]; &#125; (2) 然后将ReturnMin.class文件拷贝到hive-contrib-2.1.1.jar\org\apache\hadoop\hive\contrib\udf\example目录中，然后替换hive select在hive的cli中执行：CREATE FUNCTION ReturnMin AS ‘org.apache.hadoop.hive.contrib.udf.example.ReturnMin’;然后重新进入cli中，给这个函数传递一个数字的字符串和分隔符就能够将这个字符串最小的值返回。 hive删除一个自定义函数： drop function cutString;]]></content>
      <categories>
        <category>hive</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[hive中sql总结]]></title>
    <url>%2F2017%2F09%2F14%2Fhive%E4%B8%ADsql%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[hive中sql总结1、对指定的某一个列去重1insert into table allrecord partition (year=2017, month=08, day=25) select t.id, t.tssend, t.apmac, t.mac, t.rssi from (select id, tssend, apmac ,mac, rssi, row_number() over(distribute by mac sort by tssend )as rn from allrecords) t where t.rn=1; 上面试对字段mac去重，然后对tssend排序后插入allrecord这张表 2、删除分区1ALTER TABLE macinfo_2017 DROP IF EXISTS PARTITION (year=2017, month=03, day=22); 3、删除一张表，如果启动了回收站功能，那么这张表会保存在回收站里，也是能够恢复信息的。1drop table tablename; 若是表创建为外部表，则只会删除相应的原信息，对数据没有影响。 4、 创建一个partition_table002的表，使得表结构和partition_table001一样。1create table if not exists partition_table002 like partition_table001; 5、使用load加载数据到表中1load data local inpath 't_student1.txt' overwrite inte table t_student1; 6、创建外部表create EXTERNAL table IF NOT EXISTS macinfo_20170221( tssend string COMMENT 'Probe upload data time', apmac string COMMENT 'Probe MAC address', mac string COMMENT 'Mobile device MAC address', rssi string COMMENT 'WiFi signal strength of mobile devices') partitioned by (year int, month int, day int) ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe' WITH SERDEPROPERTIES ( "mac"="$.mac", "apmac"="$.apmac", "rssi"="$.rssi", "tssend"="$.tssend" ) STORED AS TEXTFILE LOCATION '/opt/local/software/a.txt'; 注意：这里创建外部表时会有坑，主要是数据存储格式的问题，我的数据存储为json格式，而hive并不直接支持读取json格式数据，需要添加相应的jar包才能解析数据。 7、hive外部表创建分区关联相应的目录（必须是目录）ALTER TABLE macinfo_2017 ADD PARTITION (year = 2017, month = 02, day = 21); 未完待续。。。。]]></content>
      <categories>
        <category>hive</category>
      </categories>
  </entry>
</search>
