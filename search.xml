<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[单例模式的线程安全问题]]></title>
    <url>%2F2018%2F09%2F07%2F%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%E7%9A%84%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[单例模式单例模式是最简单的设计模式，分为两种：懒汉模式、恶汉模式 恶汉模式在属性中直接创建对象，然后在方法中返回。 12345678910public class Singletom &#123; private static Singletom ourInstance = new Singletom(); public static Singletom getInstance() &#123; return ourInstance; &#125; private Singletom() &#123; &#125;&#125; 上面不存在线程安全的问题，因为加载类的时候就会创建Singletom的对象，然后所有线程来获取的时候都是返回这个对象。 懒汉模式刚开始并不直接创建对象，而是在需要的时候在创建，主要代码如下 1234567891011121314public class OtherSingleton &#123; private static OtherSingleton instance; private OtherSingleton() &#123;&#125; public static OtherSingleton getInstance()&#123; if (instance == null)&#123; ————1 instance = new OtherSingleton(); —————2 return instance; &#125; return instance; &#125;&#125; 这个时候是线程不安全的，假设两个线程一前一后来执行getInstance()方法，这个时候线程A正在执行2这个位置，由于new对象并非一个原子操作，分为以下三步： 1) 在内存中分配空间。 2) 调用构造函数初始化对象。 3）将instance引用指向堆内存空间的起始地址（此时instance才为非null）。 若将上边三步用伪代码来表示就是下边这样： 123memory = allocate(); // 1、分配对象的内存空间ctorInstance(memory); // 2、初始化对象instance = memory； //3、设置instance指向刚分配的内存地址 实际上在上边的伪代码中2和3之间，可能会被重排序（在一些JIT编译器上，这种重排序也是真实发生的） ，根据《java语言规范》，所有线程在执行java程序时必须遵守intra-thread semantics,intra-thread semantics保证排序不会改变单线程的执行结果。换句话说，intra-thread semantics允许那些在单线程中不改变执行结果的重排序，这样会提高程序的执行性能。 这个时候A线程执行刚刚才执行到创建对象的第二步，线程B就执行到代码1处，此时判断出instance为null，然后进入对象在创建一个对象！ 这个时候就出现问题了，单例模式此时创建了两个对象。违背了单例模式的初衷。 双重锁检查123456789101112131415161718public class OtherSingleton &#123; private static OtherSingleton instance; private OtherSingleton() &#123;&#125; public static OtherSingleton getInstance()&#123; if (instance == null)&#123; ————1 synchronized (OtherSingleton.class)&#123; if (instance == null)&#123; ————2 instance = new OtherSingleton(); ————3 return instance; &#125; &#125; &#125; return instance; &#125;&#125; 这里加双重检查锁就是在同步代码前和同步代码后边都检查实例是否为空。如果仅仅在同步块外边检查，而在同步块内不检查，那么可能存在多个线程同时进入了同步块外的代码，则可能生成多个实例。 上边代码看起来还不错，但是他还是有问题的。问题出现在代码3处，这个地方并非是原子操作，如上边的描述分为三步。由于cpu为了提高执行效率可能会将这个三步分为1-2-3或者1-3-2。 如果线程A执行到代码3处的1-3步，此时instance已经不为null了，但是创建对象的第二部却并没有执行。而此时线程B进入到方法的1处，判断instance是否为空，此时已经不会空了，但是返回的对象却并没有初始化，顺理成章的报错。。 这里的关键是instance对象还没有初始化执行第二步，就执行的第三步，导致线程B拿到一个空的对象。那么我们只需要让cpu在执行创建对象时禁止重排序就能够解决这个问题了。明显需要使用volatile关键字来禁止重排序。 123456789101112131415161718public class OtherSingleton &#123; private volatile static OtherSingleton instance; private OtherSingleton() &#123;&#125; public static OtherSingleton getInstance()&#123; if (instance == null)&#123; synchronized (OtherSingleton.class)&#123; if (instance == null)&#123; instance = new OtherSingleton(); return instance; &#125; &#125; &#125; return instance; &#125;&#125; 这里利用的是volatile的禁止重排序的性质，在volatile变量的赋值操作后边会加上一个内存屏障，读操作不会出现在内存屏障前边，所以取操作*必须是1-2-3或者1-3-2之后，不存在1-3**之后就读取到。从happen-before原则来看，就是对于一个 volatile 变量的写操作都先行发生于后面对这个变量的读操作（这里的“后面”是时间上的先后顺序）。 参考 如何正确地写出单例模式 《java并发编程的艺术》]]></content>
      <categories>
        <category>java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[分布式调度中心]]></title>
    <url>%2F2018%2F09%2F06%2F%E5%88%86%E5%B8%83%E5%BC%8F%E8%B0%83%E5%BA%A6%E4%B8%AD%E5%BF%83%2F</url>
    <content type="text"><![CDATA[最近公司的服务已经从传统tomcat服务拆分成了微服务，使得各个原本臃肿的业务变得轻量、易维护，但是也带了一些问题，比如生产、测试环境中的定时任务管理就成了问题，由于服务器变得更多，定时任务变得分散，各个环境容易变得混杂、维护成本升高且容易出错，同时无法知道这个定时任务执行是否成功，只有当服务出现了问题才能反推定时任务出现了问题。 使用系统级定时crontab存在的主要问题 任务的时间粒度不够小（只能是分钟级别以上） 当任务没有执行的时候没有通知，不能及时发现定时任务是否执行，只有当出现异常的时候才能发现定时的执行出现了问题。 任务执行过程没有日志记录，出现的问题不容易定位 没有失败处理策略（失败重试、失败告警） 没有阻塞策略 依赖运维人员，低效 那么上边这些问题要怎样解决呢？ 我们可以使用分布式调度系统来彻底解决这些痛点。 分布式调度系统 TBSchedule：阿里早期开源的分布式任务调度系统。代码略陈旧，使用timer而非线程池执行任务调度。众所周知，timer在处理异常状况时是有缺陷的。而且TBSchedule作业类型较为单一，只能是获取/处理数据一种模式。还有就是文档缺失比较严重 Saturn：是唯品会自主研发的分布式的定时任务的调度平台，基于当当的elastic-job 版本1开发，并且可以很好的部署到docker容器上。 elastic-job：当当开发的弹性分布式任务调度系统，功能丰富强大，采用zookeeper实现分布式协调，实现任务高可用以及分片，目前是版本2.15，并且可以支持云开发 xxl-job: 是大众点评员工徐雪里于2015年发布的分布式任务调度平台，是一个轻量级分布式任务调度框架，其核心设计目标是开发迅速、学习简单、轻量级、易扩展 在上边这几种解决方案中，以xxl-job和elastic-job用的更多、文档相对完善。 共同点： E-Job和X-job都有广泛的用户基础和完整的技术文档，都能满足定时任务的基本功能需求。 不同点：X-Job 侧重的业务实现的简单和管理的方便，学习成本简单，失败策略和路由策略丰富。推荐使用在“用户基数相对少，服务器数量在一定范围内”的情景下使用E-Job 关注的是数据，增加了弹性扩容和数据分片的思路，以便于更大限度的利用分布式服务器的资源。但是学习成本相对高些，推荐在“数据量庞大，且部署服务器数量较多”时使用 根据我们业务的数据量、用户数量可以采用xxl-job快速实现分布式任务调度中心。 xxl-job下载、运行首先我们先去github中下载xxl-job最新的代码 1&gt; git clone https://github.com/xuxueli/xxl-job.git 然后在idea中加载这个模块 注意上边的三个黄色框圈起来的部分，从上往下： 1、tables_xxl_job.sql这个是我们在库里边创建的库-表，执行之后会有16张表被创建 1&gt; mysql -uroot -p &lt; tables_xxl_job.sql 2、xxl-job-admin就是调度中心，我们需要修改resources里边的xxl-job-admin.properties文件一些字段 123456789101112131415161718192021 ### xxl-job db (use &amp;amp; replace &amp; in xml)xxl.job.db.driverClass=com.mysql.jdbc.Driverxxl.job.db.url=jdbc:mysql://localhost:3306/xxl-job?useUnicode=true&amp;characterEncoding=UTF-8xxl.job.db.user=rootxxl.job.db.password=123456### xxl-job emailxxl.job.mail.host=smtp.163.comxxl.job.mail.port=25xxl.job.mail.username=ovono802302@163.comxxl.job.mail.password=asdfzxcvxxl.job.mail.sendNick=《任务调度平台XXL-JOB》### xxl-job loginxxl.job.login.username=adminxxl.job.login.password=123456### xxl-job, access tokenxxl.job.accessToken=### xxl-job, i18n (default empty as chinese, "en" as english)xxl.job.i18n= 配置完成之后就可以启动了。 3、 第三个是xxl-job-executor-sample-springboot这个是xxl-job提供的执行器中的一种，另外还支持spring等等，修改配置文件，将相应参数修改成和自己环境匹配的参数。 123456789101112131415161718192021# web portserver.port=8081# log configlogging.config=classpath:logback.xml### xxl-job admin address list, such as "http://address" or "http://address01,http://address02"xxl.job.admin.addresses=http://127.0.0.1:8080/xxl-job-admin### xxl-job executor addressxxl.job.executor.appname=xxl-job-executor-samplexxl.job.executor.ip=xxl.job.executor.port=9999### xxl-job, access tokenxxl.job.accessToken=### xxl-job log pathxxl.job.executor.logpath=/Users/leon/work/logs/xxl-job/jobhandler### xxl-job log retention daysxxl.job.executor.logretentiondays=-1 上边执行器是具体执行任务的模块。下边就来具体讲讲xxl-job整体使用架构图。 上图中虚线包围起来的两个部分就是xxl-job最重要的部分，一个叫调度中心，一个叫执行器（我更愿意叫他执行器群）。其中调度中心负责所有执行器群的管理，各个执行器群的任务分发管理，日志的查询等等功能，而执行器负责具体执行任务。而中间的通讯组件则是自研的RPC组件，生产业务执行器就相当于是生产环境的执行器群，测试业务执行器相当于测试环境的执行器群。这样就能满足将各个环境通过执行器群来区分，然后统一管理起来在一个web后台就能处理所有环境的定时任务了。 调度中心相关 要使用xxl-job来管理所有环境的定时任务，首先需要根据环境创建执行器群，然后在知情器群的基础上创建任务。 创建执行器群点击web页面左侧的执行器管理 =&gt; 新增执行器 上边的AppName很重要，是区分各个执行器群的标志，如果选择下边的手动录入的话那么就需要将机器ip:port添加到机器地址中。把执行器群创建好了之后就可以创建定时任务了。 创建定时任务 点击左侧的任务管理 =&gt; 新增任务 里边的具体选项的含义大家可以去查xxl-job的官方文档，也记录的很详细。这里不在重新复述。 这里还想多提一点的是他的报警机制，只支持邮件报警，但是相对来说大家打开邮件的频率比较低，平时也许不太关注邮件，所以可能导致一些任务执行失败了我们也没有及时发现。基于上面的原因，就在作者原来的代码基础上增加钉钉报警（在github issues中也有人提出过），实现起来也并不复杂。修改之后创建任务的时候多了两个参数，这两个参数也需要在数据库中添加两个字段。 若出现任务异常，那么会直接将自定义的信息通过http post的方式发送给配置的钉钉群，下面是我测试的效果。 思维导图大致画了一下xxl-job的思维导图，还不够完备。 参考 美团点评许雪里：分布式任务调度平台 XXL-JOB xxl-job官方文档 分布式定时任务调度系统技术选型]]></content>
      <categories>
        <category>java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ELK+KAFKA详解(二)]]></title>
    <url>%2F2018%2F08%2F25%2FELK%2BKAFKA%E8%AF%A6%E8%A7%A3(%E4%BA%8C)%2F</url>
    <content type="text"><![CDATA[上一篇文章我们已经将filebeat、kafka、zookeeper、logstash安装配置完毕了。接下来主要是索引数据、展示数据阶段。 安装Elasticsearch首先到官网下载5.3.3版本的安装包 1&gt; axel -n 5 https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.3.3.tar.gz 然后在ES的根目录下创建两个目录data、logs，用来保存索引数据、日志等等 12&gt; mkdir logs&gt; mkdir data 进入config目录编辑elasticsearch.yml文件，这种以.yml类型结尾的文件要注意缩进。 12345678910# Path to directory where to store the data (separate multiple locations by comma):path.data: /opt/soft/elasticsearch-5.5.3/data# Path to log files:path.logs: /opt/soft/elasticsearch-5.5.3/logs# 绑定一个指定的ip# network.host: 0.0.0.0http.port: 9200# 后边使用插件的时候需要使用到下边两个配置。http.cors.enabled: truehttp.cors.allow-origin: "*" 上边是一些主要的配置，然后可以直接启动ES了 1&gt; ./bin/elasticsearch &amp; 上边我们是已经启动了，但是我们还不知道数据是否通过logstash写入到ES中，所以我们还需要两个步骤来确保我们安装配置是没有问题的： 创建一个索引（ES中的索引类似于数据库中的一个数据库实例） 。 安装head插件，查看ES情况 索引在这之前我们需要了解ES中的三个基本概念： index：ES管理数据的最顶层单位，类似于单个数据库的概念。 document ：ES的index中单条数据被称为document type：document可以分组，分组就叫type 通过指定一个文件的方式创建一个索引： 1&gt; curl -XPUT 'http://localhost:9200/nginx_log_index/test_log/1?pretty' -H 'Content-Type: application/json' -d '/opt/soft/logstash-5.5.3/output/es_template.json' 删除索引： 1&gt; curl -X DELETE 'localhost:9200/nginx_log_index' 安装head插件head插件运行需要node环境，这里就不讨论如何安装node了，网上资料比较多。 使用git下载head源码： 1&gt; git clone git://github.com/mobz/elasticsearch-head.git 然后进入head的目录执行指令 1&gt; npm install 启动 1&gt; npm run start 若是ES使用的都是默认端口，那么head插件就不需要修改配置。 在浏览器中访问http://ip:9100/就可以看到ES的情况 ES补充Elasticsearch是一个搜索引擎，我们可以通过它提供的查询语句实现类似sql的查询功能，为我们展示数据提供便利，下面列举一个常用的查询。 1、指定一个index，搜索其下所有type的数据 123456&gt; curl -X GET 'http://localhost:9200/nginx-log/_search?pretty' -H 'Content-Type: application/json' -d ' &#123; "query": &#123; "match_all": &#123;&#125; &#125;&#125;' 上边的nginx-log就是我们想要查询的index 2、查看每个index所包含的type 1&gt; curl 'localhost:9200/_mapping?pretty=true' 3、查看当前节点的所有Index 1&gt; curl -X GET 'http://localhost:9200/_cat/indices?v' 4、根据某个字段进行分组后统计，实现类似于sql里边的功能 1select count(*) from table_name group by field; 下面的ES查询语句功能和上边的sql是类似的 1234567891011&gt; curl -X POST 'http://localhost:9200/nginx-log/nginxlog/_search?pretty' -H 'Content-Type: application/json' -d'&#123; "size": 0, "aggs": &#123; "group_by_service_name": &#123; "terms": &#123; "field": "service_name.keyword" &#125; &#125; &#125;&#125;' 安装kibana下载： 1&gt; axel -n 5 https://artifacts.elastic.co/downloads/kibana/kibana-5.5.3-linux-x86_64.tar.gz kibana的安装配置较为简单，只需要在config目录中的kibana.yml文件配置两个字段 1234# Kibana is served by a back end server. This setting specifies the port to use.server.port: 5601# The URL of the Elasticsearch instance to use for all your queries.elasticsearch.url: "http://localhost:9200" 然后启动 1&gt; ./bin/kibana &amp; 在第一次启动kibana的时候，需要手动配置一个index，然后kibana会通过这个index去ES中查询并展示。 我们将刚才创建的index加入到里边，然后点击Discover选择刚才配置的index，在选择时间，就能够看到数据已经被展示出来了。 上图中的左侧可以选择在右侧展示哪些字段。 使用kibana绘图&emsp;&emsp;在kibana中也是可以完成一些绘图的，但是由于kibana没有相应权限控制，并且kibana的颜值是没有Grafana高，所以我们还是用Grafana来展示图表，kibana查询日志。下面就来探讨一下Grafana相关内容 Grafana相关我们先进入Grafana的官网，里边有介绍如何安装，也很简单。 12&gt; wget https://s3-us-west-2.amazonaws.com/grafana-releases/release/grafana_5.2.2_amd64.deb &gt; sudo dpkg -i grafana_5.2.2_amd64.deb 然后启动 1&gt; sudo /bin/systemctl start grafana-server 访问http://localhost:3000然后使用admin登录进去，密码默认也是admin。进去后需要先配置数据源。 上边有几个地方需要注意： type：由于我们是使用ES作为数据源，所以这里一定要选择elasticsearch url : 连接elasticsearch服务的地址 Access：需要选择具体某种连接方式，两种方式是不同的，具体不同这里不详细描述 Auth：ES若是安装了相应密码权限，这里需要正确配置之后才能访问。 Index name ： 这里是填在ES中创建的index，但是要注意，若是index name包含了日志，那么后边的pattern就不要选择任何日期了。 Version ： 需要选择和安装的ES匹配的版本 然后保存，然后就可以创建DashBoard、panel。grafana绘图、查询相关内容较多，后边若有时间还会探讨一下grafana绘图相关。 参考： elasticsearch 创建索引，以及检索一条数据 grafana官网 groups.io]]></content>
      <categories>
        <category>ELK</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ELK+KAFKA详解（一）]]></title>
    <url>%2F2018%2F08%2F22%2FELK%2BKAFKA%E8%AF%A6%E8%A7%A3%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;日志，对于任何系统来说都是及其重要的组成部分。在计算机系统里面，更是如此。但是由于现在的计算机系统大多比较复杂，很多系统都不是在一个地方，甚至都是跨国界的；即使是在一个地方的系统，也有不同的来源，比如，操作系统，应用服务，业务逻辑等等。他们都在不停产生各种各样的日志数据。 &emsp;&emsp;根据不完全统计，我们全球每天大约要产生 2EB（1018）的数据。面对如此海量的数据，又是分布在各个不同地方，如果我们需要去查找一些重要的信息，难道还是使用传统的方法，去登陆到一台台机器上查看？看来传统的工具和方法已经显得非常笨拙和低效了。于是，一些聪明人就提出了建立一套集中式的方法，把不同来源的数据集中整合到一个地方。 一个完整的集中式日志系统，是离不开以下几个主要特点的。 收集－能够采集多种来源的日志数据 传输－能够稳定的把日志数据传输到中央系统 存储－如何存储日志数据 分析－可以支持 UI 分析 警告－能够提供错误报告，监控机制 本文采用ELK + FILEBEAT + KAFKA的架构搭建日志收集系统。架构图如下： 环境、版本 JDK 1.8 OS : ubuntu18.04 filebeat、logstash、elasticsearch、kibana（5.3.3） kafka_2.11-0.10.0.1 Grafana ：Latest version filebeat安装 &emsp;&emsp;一个轻量级开源日志文件数据搜集器，基于Logstash-Forwarder 源代码开发，是对它的替代。在需要采集日志数据的 server 上安装 Filebeat，并指定日志目录或日志文件后，Filebeat 就能读取数据，迅速发送到指定的MQ中，亦或直接发送到logstash进行过滤转发。现下载到本地 123&gt; axel -n 5 https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-5.3.3-linux-x86_64.tar.gz&gt; tar -zxvf filebeat-5.5.3-linux-x86_64.tar.gz&gt; mv filebeat-5.5.3-linux-x86_64 filebeat-5.5.3 然后进入目录中复制一份cp filebeat.yml filebeat-kafka.yml，修改文件中相应的地方。 1234567891011#指定输入类型- input_type: log # 需要监控的文件目录，也可以使用通配符对整个目录监控 paths: - /opt/soft/logs/platform.pro.access.log#定义kafka为输出目的地output.kafka: # Array of hosts to connect to. hosts: ["ip:9092"] #kafka的主题 topic: nginx_log 上边需要注意一定需要将output输出目的从默认的ElasticSearch修改成kafka，以上配置就算安装完成了，接着启动. 1&gt; ./filebeat -e -c filebeat-kafka.yml kafka安装配置 kafka的运行依赖zookeeper，所以安装kafka之前需要先安装zookeeper，这里不用kafka自身带有的zookeeper。 zookeeper配置12&gt; axel -n 5 https://archive.apache.org/dist/zookeeper/zookeeper-3.4.6/zookeeper-3.4.6.tar.gz&gt; tar -zxvf zookeeper-3.4.6.tar.gz 然后进入zookeeper目录创建两个文件夹 12&gt; mkdir logs&gt; mkdir data 在/root/elk/zookeeper-3.4.6/conf目录下复制zoo_sample.cfg命名为zoo.cfg 1&gt; cp zoo_sample.cfg zoo.cfg 打开zoo.cfg，修改需要配置的地方 123456tickTime=2000initLimit=10syncLimit=5dataDir=/opt/soft/zookeeper-3.4.6/datadataLogDir=/opt/soft/zookeeper-3.4.6/logsclientPort=2181 在/root/elk/zookeeper-3.4.6/data目录下创建一个myid文件。由于我们这里只是单点环境下，所以只需要在文件里边写入server.1。 最后将zk的路径添加到相关的环境变量中，并添加PATH,然后启动 1&gt; zkServer.sh start 最后查看zk是否启动成功 1&gt; zkServer.sh status kafka配置安装kafka的时候需要注意和logstash之间版本匹配的问题，logstash5.x以上必须使用kakfa 0.10以上版本，否则无法正常运行。 1&gt; axel -n 5 https://archive.apache.org/dist/kafka/0.10.0.1/kafka_2.11-0.10.0.1.tgz 下载完成之后解压、进入kafka根目录创建logs目录,然后打开config目录下的server.properties修改一些参数 123log.dirs=/opt/soft/kafka_2.11-0.10.0.1/logszookeeper.connect=ip:2181其他项根据需要自行修改.... 然后可以启动kafka 1&gt; bin/kafka-server-start.sh config/server.properties &amp; 创建一个名叫nginx_log的kafka主题 1&gt; bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic nginx_log 然后自己可以尝试打开一个终端进入生产者模式，发送几条数据 1&gt; bin/kafka-console-producer.sh --broker-list localhost:9092 --topic nginx_log 在另外一个终端中开启消费者模式，查看生产者发送的数据是否能够看到，若能正常看到发送的数据，说明kafka安装没有问题。 1&gt; bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic nginx_log --from-beginning logstash安装配置 &emsp;&emsp;&ensp;记住文章开始的那张整体架构图，kafka是从filebeat接收数据，然后发送给logstash。所以这里logstash的input就是kafka了。 logstash是整个流程中配置、修改最多的一环，过程中也有一些坑。 1&gt; axel -n 5 https://artifacts.elastic.co/downloads/logstash/logstash-5.3.3.tar.gz 老规矩，先解压，进入config目录创建一个logstash.conf的文件，然后在这个文件里边添加input、filter、output三个模块。 input模块先来讲讲input模块，由于在logstash5.x中已经默认支持kafka作为input,所以可以直接使用。 1234567input&#123; kafka&#123; topics =&gt; ["nginx_log"] type =&gt; "nginx-access" bootstrap_servers =&gt; "ip:9092" &#125;&#125; topics :指定消费kafka的topic type ： 若是logstash需要处理多个数据源，可以使用type来做后边过滤的区分。 bootstrap_servers ：kakfa服务的ip和端口号 filter模块&emsp;&emsp;&ensp; filter模块是具体处理数据的一个环节，当kafka的数据被input模块接收过来之后，接下来就会交给filter模块处理。这里我们是为了处理nginx的access日志，所以首先需要根据nginx的log_format来定制自己的grok正则表达式。生产环境的log_format是下面这样👇 1234log_format '$remote_addr [$time_local] $host "$request" ' '$status $body_bytes_sent "$http_referer" ' '"$http_x_forwarded_for" "$http_user_agent" $request_time $upstream_response_time'; 实际的日志长成这样 1100.116.224.17 [14/Aug/2018:07:45:49 +0800] platform.blingabc.com "POST /homeworkRecord/v1/insert HTTP/1.1" 200 517 "https://i.blingabc.com/home/read-book?stuID=11936" "27.187.252.91" "Mozilla/5.0 (iPhone; CPU iPhone OS 11_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E216 MicroMessenger/6.7.1 NetType/WIFI Language/zh_CN" 0.456 0.456 &emsp;&emsp;&ensp;当我们在做nginx日志解析的时候，尽量复用grok自身带有的。少数不能解析的根据现有修改，基本上都满足解析nginx日志。在filter模块中加入下面的配置 123456grok &#123; patterns_dir =&gt; "/opt/soft/logstash-5.5.3/patterns/nginx_pattern" match =&gt; &#123; "message" =&gt; "%&#123;NGINXACCESS&#125;" &#125; &#125; &emsp;&emsp;&ensp;需要注意上边的patterns_dir，这个路径中patterns这个目录必须创建在logstash的根目录中且只能叫这个名字，之后的nginx_pattern可以随意取。nginx_pattern文件的内容： 1234STATUS ([0-9.]&#123;0,3&#125;)REQUEST_TIME ([0-9.]&#123;0,5&#125;)FORWORD (?:%&#123;IPV4&#125;[,]?[ ]?)NGINXACCESS %&#123;IPV4:remote_addr&#125; \[%&#123;HTTPDATE:time_local&#125;\] (%&#123;HOSTNAME:host&#125;) (%&#123;QUOTEDSTRING:request&#125;) %&#123;STATUS:http_status&#125; %&#123;BASE10NUM:body_bytes_sent&#125; \"(?:%&#123;DATA:http_referer&#125;|-)\" \"(%&#123;FORWORD:http_x_forwarded_for&#125;|-)\" \"(%&#123;GREEDYDATA:user_agent&#125;|-)\" (%&#123;REQUEST_TIME:request_time&#125;|-) (%&#123;REQUEST_TIME:upstream_response_time&#125;|-) &emsp;&emsp;&ensp;其中前面三条是自己的定义的正则表达式，后边NGINXACCESS是整体解析nginx日志的表达式。在自己写解析日志的grok表达式的时候在Grok Debugger中测试。 最终解析出来的数据如下 【部分省略】 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657&#123; "remote_addr": [ [ "100.116.224.72" ] ], "time_local": [ [ "14/Aug/2018:00:20:14 +0800" ] ], "host": [ [ "platform.blingabc.com" ] ], "request": [ [ ""OPTIONS /foreign/teacher/v1/login HTTP/1.1"" ] ], "http_status": [ [ "200" ] ], "body_bytes_sent": [ [ "0" ] ], "http_referer": [ [ "-" ] ], "http_x_forwarded_for": [ [ "181.143.78.26" ] ], "user_agent": [ [ "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36" ] ], "request_time": [ [ "0.001" ] ], "upstream_response_time": [ [ "0.001" ] ]&#125; 此时我们有一个需求，需要通过下面的request解析得到/foreign/teacher/v1/login和foreign，然后通过Grafana统计做top N展示，那么数据就还需要进一步拆分。 12345"request": [ [ ""OPTIONS /foreign/teacher/v1/login HTTP/1.1"" ] ], filter模块中支持很多插件给我们使用，例如grok、kv、ruby、mutate等等… 下面就用ruby和mutate来实现我们想达到的目标。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546 filter &#123; grok &#123; patterns_dir =&gt; "/opt/soft/logstash-5.5.3/patterns/nginx_pattern" match =&gt; &#123; "message" =&gt; "%&#123;NGINXACCESS&#125;" &#125; &#125; if [request] &#123; ruby &#123; init =&gt; "@kname = ['method','uri']" code =&gt; " new_event = LogStash::Event.new(Hash[@kname.zip(event.get('request').split(' '))]) new_event.remove('@timestamp') event.append(new_event) " &#125; if [uri] &#123; ruby &#123; init =&gt; "@kname = ['url_path','url_args']" code =&gt; " new_event = LogStash::Event.new(Hash[@kname.zip(event.get('uri').split('?'))]) new_event.remove('@timestamp') event.append(new_event) " &#125; &#125; if [url_path] &#123; ruby &#123; init =&gt; "@kname = ['service_name', 'url_path']" code =&gt; " new_event = LogStash::Event.new(Hash[@kname.zip(event.get('uri').split('/'))]) new_event.remove('@timestamp') event.append(new_event) " &#125; &#125; &#125; mutate &#123; rename =&gt; ["url_path[1]", "service_name"] rename =&gt; ["url_path[0]", "uri"] &#125; if [service_name] == "monitor" or [service_name] == "weixin" or [method] == "OPTIONS" &#123; drop &#123;&#125; &#125;&#125; 上边的执行时流式执行的，就是说上边执行得到的结果下边可以直接使用。然后将数组里边的结果重名名，提供给下面ES使用。 output最后就是output模块了，这里比较简单，主要是讲过滤后的数据交给ES。 123456789output &#123; elasticsearch&#123; hosts =&gt; ["ip:9200"] index =&gt; "nginx-log" template_overwrite =&gt; true template =&gt; "/opt/soft/logstash-5.5.3/output/es_template.json" &#125; stdout&#123;codec =&gt; rubydebug&#125;&#125; 但是需要注意上边的template指定的文件，在es_template.json这个文件中定义了filter模块解析出来的字段名称、类型。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293&#123; "template":"nginx-log", "settings":&#123; "index.refresh_interval":"5s", "index.number_of_replicas":"1" &#125;, "analysis":&#123; "analyzer":&#123; "default":&#123; "type":"standard", "stopwords":"_none_" &#125; &#125; &#125;, "mappings":&#123; "nginx_log":&#123; "_all":&#123; "enabled":false &#125;, "properties":&#123; "remote_addr":&#123; "type":"ip", "fielddata": true &#125;, "time_local":&#123; "type":"date", "format":"dd/MMM/yyyy:HH:mm:ss Z" &#125;, "host":&#123; "type":"string", "fielddata": true &#125;, "uri":&#123; "type":"string", "fielddata": true &#125;, "request":&#123; "type":"string" &#125;, "http_status":&#123; "type":"integer", "fielddata": true &#125;, "body_bytes_sent":&#123; "type":"integer" &#125;, "http_referer":&#123; "type":"string" &#125;, "http_x_forwarded_for":&#123; "type":"ip" &#125;, "user_agent":&#123; "type":"string" &#125;, "request_time":&#123; "type":"float", "fielddata": true &#125;, "upstream_response_time":&#123; "type":"float" &#125;, "service_name":&#123; "type":"string", "fielddata": true &#125;, "request_url":&#123; "type":"string", "fielddata": true &#125;, "geoip":&#123; "properties":&#123; "city_name":&#123; "type":"keyword" &#125;, "country_name":&#123; "type":"keyword" &#125;, "latitude":&#123; "type":"float" &#125;, "location":&#123; "type":"geo_point" &#125;, "longitude":&#123; "type":"float" &#125; &#125; &#125; &#125; &#125; &#125;&#125; 最终形成的配置文件就是这样的 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364input&#123; file &#123; path =&gt; "/opt/soft/logs/platform.pro.access.log" type =&gt; "nginxlog" start_position =&gt; "beginning" &#125;&#125;filter &#123; grok &#123; patterns_dir =&gt; "/opt/soft/logstash-5.5.3/patterns/nginx_pattern" match =&gt; &#123; "message" =&gt; "%&#123;NGINXACCESS&#125;" &#125; &#125; if [request] &#123; ruby &#123; init =&gt; "@kname = ['method','uri']" code =&gt; " new_event = LogStash::Event.new(Hash[@kname.zip(event.get('request').split(' '))]) new_event.remove('@timestamp') event.append(new_event) " &#125; if [uri] &#123; ruby &#123; init =&gt; "@kname = ['url_path','url_args']" code =&gt; " new_event = LogStash::Event.new(Hash[@kname.zip(event.get('uri').split('?'))]) new_event.remove('@timestamp') event.append(new_event) " &#125; &#125; if [url_path] &#123; ruby &#123; init =&gt; "@kname = ['service_name', 'url_path']" code =&gt; " new_event = LogStash::Event.new(Hash[@kname.zip(event.get('uri').split('/'))]) new_event.remove('@timestamp') event.append(new_event) " &#125; &#125; &#125; mutate &#123; rename =&gt; ["url_path[1]", "service_name"] rename =&gt; ["url_path[0]", "uri"] &#125; if [service_name] == "monitor" or [service_name] == "weixin" or [method] == "OPTIONS" &#123; drop &#123;&#125; &#125;&#125;output &#123; elasticsearch&#123; hosts =&gt; ["localhost:9200"] index =&gt; "nginx-log" template_overwrite =&gt; true template =&gt; "/opt/soft/logstash-5.5.3/output/es_template.json" &#125; stdout&#123;codec =&gt; rubydebug&#125;&#125; 上边已经整个环境搭建完成了一大半了，接下来就是安装Elasticsearch和kibana、Grafana。 参考： ELK初体验-Nginx日志实时分析 搭建ELK日志分析平台（上） elk搭建实战 ELK(ElasticSearch, Logstash, Kibana)搭建实时日志分析平台 ……]]></content>
      <categories>
        <category>ELK</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[java 工程师成神之路(转载)]]></title>
    <url>%2F2018%2F06%2F28%2Fjava%20%E5%B7%A5%E7%A8%8B%E5%B8%88%E6%88%90%E7%A5%9E%E4%B9%8B%E8%B7%AF(%E8%BD%AC%E8%BD%BD)%2F</url>
    <content type="text"><![CDATA[今天在论坛上看到的一个帖子，上边提到的内容极为宽泛，可能在短时间内不可能完全做到提到的所有内容，但也不妨作为一个学习的目标。原文地址 基础篇JVMJVM内存结构堆、栈、方法区、直接内存、堆和栈区别 Java 内存模型内存可见性、重排序、顺序一致性、volatile、锁、final 垃圾回收内存分配策略、垃圾收集器（ G1 ）、GC 算法、GC 参数、对象存活的判定 JVM 参数及调优Java 对象模型oop-klass、对象头 HotSpot即时编译器、编译优化 类加载机制classLoader、类加载过程、双亲委派（破坏双亲委派）、模块化（ jboss modules、osgi、jigsaw ） 虚拟机性能监控与故障处理工具 jps, jstack, jmap、jstat, jconsole, jinfo, jhat, javap, btrace、TProfiler 编译与反编译 javac、javap、jad、CRF Java 基础知识阅读源代码String、Integer、Long、Enum、BigDecimal、ThreadLocal、ClassLoader &amp; URLClassLoader、ArrayList &amp; LinkedList、HashMap &amp; LinkedHashMap &amp; TreeMap &amp; CouncurrentHashMap、HashSet &amp; LinkedHashSet &amp; TreeSet Java中各种变量类型熟悉 Java String 的使用，熟悉 String 的各种函数JDK 6 和 JDK 7 中 substring 的原理及区别、 replaceFirst、replaceAll、replace 区别、 String 对“+”的重载、 String.valueOf 和 Integer.toString 的区别、 字符串的不可变性 自动拆装箱 Integer 的缓存机制 熟悉 Java 中各种关键字transient、instanceof、volatile、synchronized、final、static、const 原理及用法。 集合类常用集合类的使用、ArrayList 和 LinkedList 和 Vector 的区别 、SynchronizedList 和 Vector 的区别、HashMap、HashTable、ConcurrentHashMap 区别、Java 8 中 stream 相关用法、apache 集合处理工具类的使用、不同版本的 JDK 中 HashMap 的实现的区别以及原因 枚举枚举的用法、枚举与单例、Enum 类 Java IO&amp;Java NIO，并学会使用bio、nio 和 aio 的区别、三种 IO 的用法与原理、netty Java 反射与 javassist反射与工厂模式、 java.lang.reflect.* Java 序列化什么是序列化与反序列化、为什么序列化、序列化底层原理、序列化与单例模式、protobuf、为什么说序列化并不安全 注解元注解、自定义注解、Java 中常用注解使用、注解与反射的结合 JMS什么是 Java 消息服务、JMS 消息传送模型 JMXjava.lang.management.、 javax.management. 泛型泛型与继承、类型擦除、泛型中 K T V E ？ object 等的含义、泛型各种用法 单元测试junit、mock、mockito、内存数据库（ h2 ） 正则表达式java.lang.util.regex.* 常用的 Java 工具库commons.lang, commons.*… guava-libraries netty 什么是 API&amp;SPI异常异常类型、正确处理异常、自定义异常 时间处理时区、时令、Java 中时间 API 编码方式解决乱码问题、常用编码方式 语法糖Java 中语法糖原理、解语法糖 Java 并发编程什么是线程，与进程的区别阅读源代码，并学会使用Thread、Runnable、Callable、ReentrantLock、ReentrantReadWriteLock、Atomic*、Semaphore、CountDownLatch、、ConcurrentHashMap、Executors 线程池自己设计线程池、submit() 和 execute() 线程安全死锁、死锁如何排查、Java 线程调度、线程安全和内存模型的关系 锁CAS、乐观锁与悲观锁、数据库相关锁机制、分布式锁、偏向锁、轻量级锁、重量级锁、monitor、锁优化、锁消除、锁粗化、自旋锁、可重入锁、阻塞锁、死锁 死锁volatilehappens-before、编译器指令重排和 CPU 指令重synchronizedsynchronized 是如何实现的？ synchronized 和 lock 之间关系、不使用 synchronized 如何实现一个线程安全的单例 sleep 和 waitwait 和 notifynotify 和 notifyAllThreadLocal写一个死锁的程序写代码来解决生产者消费者问题守护线程守护线程和非守护线程的区别以及用法进阶篇Java 底层知识字节码、class 文件格式CPU 缓存，L1，L2，L3 和伪共享尾递归位运算用位运算实现加、减、乘、除、取余设计模式了解 23 种设计模式会使用常用设计模式单例、策略、工厂、适配器、责任链。 实现 AOP实现 IOC不用 synchronized 和 lock，实现线程安全的单例模式nio 和 reactor 设计模式网络编程知识tcp、udp、http、https 等常用协议三次握手与四次关闭、流量控制和拥塞控制、OSI 七层模型、tcp 粘包与拆包 http/1.0 http/1.1 http/2 之前的区别Java RMI，Socket，HttpClientcookie 与 sessioncookie 被禁用，如何实现 session 用 Java 写一个简单的静态文件的 HTTP 服务器实现客户端缓存功能，支持返回 304 实现可并发下载一个文件 使用线程池处理客户端请求 使用 nio 处理客户端请求 支持简单的 rewrite 规则 上述功能在实现的时候需要满足“开闭原则” 了解 nginx 和 apache 服务器的特性并搭建一个对应的服务器用 Java 实现 FTP、SMTP 协议进程间通讯的方式什么是 CDN ？如果实现？什么是 DNS ？反向代理框架知识Servlet 线程安全问题Servlet 中的 filter 和 listenerHibernate 的缓存机制Hiberate 的懒加载Spring Bean 的初始化Spring 的 AOP 原理自己实现 Spring 的 IOCSpring MVCSpring Boot2.0Spring Boot 的 starter 原理，自己实现一个 starterSpring Security应用服务器知识 JBosstomcatjettyWeblogic工具git &amp; svnmaven &amp; gradleidea高级篇新技术java8 lambda 表达式、Stream API、 Java 9Jigsaw、Jshell、Reactive Streams Java 10局部变量类型推断、G1 的并行 Full GC、ThreadLocal 握手机制 Spring 5响应式编程 Spring Boot 2.0性能优化使用单例、使用 Future 模式、使用线程池、选择就绪、减少上下文切换、减少锁粒度、数据压缩、结果缓存 线上问题分析dump 获取线程 Dump、内存 Dump、gc 情况 dump 分析分析死锁、分析内存泄露 自己编写各种 outofmemory，stackoverflow 程序HeapOutOfMemory、Young OutOfMemory、MethodArea OutOfMemory、ConstantPool OutOfMemory、DirectMemory OutOfMemory、Stack OutOfMemory Stack OverFlow 常见问题解决思路内存溢出、线程死锁、类加载冲突 使用工具尝试解决以下问题，并写下总结当一个 Java 程序响应很慢时如何查找问题、 当一个 Java 程序频繁 FullGC 时如何解决问题、 如何查看垃圾回收日志、 当一个 Java 应用发生 OutOfMemory 时该如何解决、 如何判断是否出现死锁、 如何判断是否存在内存泄露 编译原理知识编译与反编译Java 代码的编译与反编译Java 的反编译工具词法分析，语法分析（ LL 算法，递归下降算法，LR 算法），语义分析，运行时环境，中间代码，代码生成，代码优化操作系统知识Linux 的常用命令进程同步缓冲区溢出分段和分页虚拟内存与主存数据库知识MySql 执行引擎MySQL 执行计划如何查看执行计划，如何根据执行计划进行 SQL 优化SQL 优化事务事务的隔离级别、事务能不能实现锁的功能 数据库锁行锁、表锁、使用数据库锁实现乐观锁、 数据库主备搭建binlog内存数据库h2 常用的 nosql 数据库redis、memcached 分别使用数据库锁、NoSql 实现分布式锁性能调优数据结构与算法知识简单的数据结构栈、队列、链表、数组、哈希表、树二叉树、字典树、平衡树、排序树、B 树、B+树、R 树、多路树、红黑树 排序算法各种排序算法和时间复杂度 深度优先和广度优先搜索 全排列、贪心算法、KMP 算法、hash 算法、海量数据处理 大数据知识Zookeeper基本概念、常见用法 Solr，Lucene，ElasticSearch在 linux 上部署 solr，solrcloud，，新增、删除、查询索引 Storm，流式计算，了解 Spark，S4在 linux 上部署 storm，用 zookeeper 做协调，运行 storm hello world，local 和 remote 模式运行调试 storm topology。 Hadoop，离线计算HDFS、MapReduce 分布式日志收集 flume，kafka，logstash数据挖掘，mahout网络安全知识什么是 XSSXSS 的防御 什么是 CSRF什么是注入攻击SQL 注入、XML 注入、CRLF 注入 什么是文件上传漏洞加密与解密MD5，SHA1、DES、AES、RSA、DSA 什么是 DOS 攻击和 DDOS 攻击memcached 为什么可以导致 DDos 攻击、什么是反射型 DDoS SSL、TLS，HTTPS如何通过 Hash 碰撞进行 DOS 攻击用 openssl 签一个证书部署到 apache 或 nginx四、架构篇分布式数据一致性、服务治理、服务降级 分布式事务2PC、3PC、CAP、BASE、 可靠消息最终一致性、最大努力通知、TCC Dubbo服务注册、服务发现，服务治理 分布式数据库怎样打造一个分布式数据库、什么时候需要分布式数据库、mycat、otter、HBase 分布式文件系统mfs、fastdfs 分布式缓存缓存一致性、缓存命中率、缓存冗余 微服务SOA、康威定律 ServiceMeshDocker &amp; KubernetsSpring BootSpring Cloud高并发分库分表分库分表CDN 技术消息队列ActiveMQ、RabbitMQ、Kafka 监控监控什么CPU、内存、磁盘 I/O、网络 I/O 等 监控手段进程监控、语义监控、机器资源监控、数据波动 监控数据采集日志、埋点 Dapper负载均衡tomcat 负载均衡、Nginx 负载均衡 DNSDNS 原理、DNS 的设计 CDN数据一致性五、 扩展篇云计算IaaS、SaaS、PaaS、虚拟化技术、openstack、Serverlsess 搜索引擎Solr、Lucene、Nutch、Elasticsearch 权限管理Shiro 区块链哈希算法、Merkle 树、公钥密码算法、共识算法、Raft 协议、Paxos 算法与 Raft 算法、拜占庭问题与算法、消息认证码与数字签名 比特币挖矿、共识机制、闪电网络、侧链、热点问题、分叉 以太坊超级账本人工智能数学基础、机器学习、人工神经网络、深度学习、应用场景。 常用框架TensorFlow、DeepLearning4J 其他语言Groovy、Python、Go、NodeJs、Swift、Rust 六、 推荐书籍《深入理解 Java 虚拟机》 《 Effective Java 》 《深入分析 Java Web 技术内幕》 《大型网站技术架构》 《代码整洁之道》 《 Head First 设计模式》 《 maven 实战》 《区块链原理、设计与应用》 《 Java 并发编程实战》 《鸟哥的 Linux 私房菜》 《从 Paxos 到 Zookeeper 》 《架构即未来》]]></content>
      <categories>
        <category>java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Guava Cache初探]]></title>
    <url>%2F2018%2F06%2F01%2FGuava%E5%88%9D%E6%8E%A2%2F</url>
    <content type="text"><![CDATA[缓存的类别、应用场景 &emsp;&emsp;高速缓存简称缓存，原始意义是指访问速度比一般随机存取存储器（RAM）快的一种RAM，通常它不像系统主存那样使用DRAM技术，而使用昂贵但较快速的SRAM技术。当CPU处理数据时，它会先到Cache中去寻找，如果数据因之前的操作已经读取而被暂存其中，就不需要再从随机存取存储器（Main memory）中读取数据——由于CPU的运行速度一般比主内存的读取速度快，主存储器周期（访问主存储器所需要的时间）为数个时钟周期。因此若要访问主内存的话，就必须等待数个CPU周期从而造成浪费。 &emsp;&emsp; 现在的缓存概念不仅仅是CPU和主存之间有Cache，而且在内存和硬盘之间也有缓存（磁盘缓存），乃至在硬盘与网络之间也有某种意义上的Cache──称为Internet临时文件夹或网络内容缓存等。凡是位于速度相差较大的两种硬件之间，用于协调两者数据传输速度差异的结构，均可称之为Cache。 摘录自维基百科 &emsp;&emsp;在这里我们讨论的是磁盘缓存，当我们在实际开发中遇到查询远远比更新、插入操作更多的时候就可以考虑使用缓存，用空间来换取时间的策略。 JVM缓存我们平常使用的Map、List都属于JVM缓存，使用JVM缓存的优势在于简单，但是也存在一些问题 只能显式的写入、清理数据 不能按照一定的规则淘汰数据，如（LRU）、先进先出算法（FIFO）、最近最少使用算法（LFU）、非最近使用算法（NMRU） 而本文提到的Guava Cache有如下的优势： 当缓存的数据已经超过预先设置的最大值时，使用LRU算法移除一些数据;由于GuavaCahce是基于JVM的缓存方式，所以它缓存的大小对于我们自己应用的本身有很大的影响，所以在必要的时候需要对它进行部分控制和淘汰，在Guava中采用LRU算法。 具备根据entry节点上次被访问或者写入的时间来计算过期机制；例如在初始化loadingCache对象的时候我们可以指定写入的节点经过多长时间没有更新操作，就会被淘汰（expireAfterWrite） 缓存的key被封装在WeakReference引用内;同样为了让缓存不会影响到自己的应用，Guava引入了软、弱引用；当缓存太大，或者缓存在GC的时候无法被回收掉，这个时候可以直接将这部分数据回收掉。 移除entry节点，可以触发监听器通知事件；一般在Guava Cache中删除entry对于我们是透明的，但是Guava为我们提供了removeLinstener，这样我们在删除entry的时候对我们可见。 统计缓存使用过程中命中率/异常率/未命中率等数据。 &emsp;&emsp;&ensp;同时，Guava Cache和ConcurrentHashMap 的核心数据结构一致，实际上Guava的核心类LocalCache实现了ConcurrentMap，所以Guava Cache Cache和ConcurrentHashMap是有“一样的血统”，但是在具体功能上还有有一些区别，ConcurrentHashMap 会一致保存所有添加的元素，直到显式的移除，而Guava Cache则在创建对象的时候就需要设置自动回收元素的方式。 分布式缓存上面的几种放缓方式都只能是在单节点中使用，而对于在分布式的环境下则需要使用Redis、Memcached这样的缓存中间件了。 如何使用Guava Cache首先我们需要在pom文件加入相关的依赖 12345&lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;21.0&lt;/version&gt; &lt;/dependency&gt; 然后我们可以初始化一个LoadingCache对象 123456789&lt;div&gt; &lt;ul&gt; &lt;li class="item-0"&gt;&lt;a href="link1.html"&gt;first item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-inactive"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; 12345678910111213141516171819202122232425262728private LoadingCache&lt;String, String&gt; loadingCache ;private void init() throws InterruptedException &#123; loadingCache = CacheBuilder.newBuilder() // 若在2秒以内没有被更新就会expire .expireAfterWrite(2, TimeUnit.SECONDS) //当entries接近maximumSize的时候就会驱赶出一个最近最少用的entry .maximumSize(1000) .build(new CacheLoader&lt;String, String&gt;() &#123; @Override public String load(String key) &#123; return "kobe"; &#125; &#125;); for (int i = 10; i &lt; 15; i++) &#123; QUEUE.put("jordan" + i); &#125; &#125;private void testCache() &#123; try &#123; TimeUnit.SECONDS.sleep(3); logger.info("当前缓存值 : &#123;&#125;,缓存大小 : &#123;&#125;", loadingCache.get(KEY), loadingCache.size()); loadingCache.get(key); &#125; catch (ExecutionException e ) &#123; logger.error("Exception", e); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;&#125; 上边初始化是给定2秒钟内若没有被更新就失效，而我们调用testCache方法中先sleep了3秒，所以此时缓存中的值一定是失效都清除掉了。下面我们可以通过debug一步一步看看loadingCache.get(key)这个方法到底是怎样获取到对应的值的。 1234V get(K key, CacheLoader&lt;? super K, V&gt; loader) throws ExecutionException &#123; int hash = hash(checkNotNull(key)); return segmentFor(hash).get(key, hash, loader); &#125; 首先可以看到将key哈希一次，通过hash和segmentShift、segmentMask做位运算得到对应的下标。在这一点和ConcurrentHashMap 是一致的。 123456789101112131415161718192021222324V get(K key, int hash, CacheLoader&lt;? super K, V&gt; loader) throws ExecutionException &#123; checkNotNull(key); checkNotNull(loader); try &#123; if (count != 0) &#123; // read-volatile // don't call getLiveEntry, which would ignore loading values ReferenceEntry&lt;K, V&gt; e = getEntry(key, hash); if (e != null) &#123; long now = map.ticker.read(); // 获取没有失效、过期的entry V value = getLiveValue(e, now); if (value != null) &#123; recordRead(e, now); statsCounter.recordHits(1); return scheduleRefresh(e, key, hash, value, now, loader); &#125; ValueReference&lt;K, V&gt; valueReference = e.getValueReference(); if (valueReference.isLoading()) &#123; return waitForLoadingValue(e, key, valueReference); &#125; &#125; &#125; ... &#125; 在上面方法里边比较关键的是V value = getLiveValue(e, now);这行，去获取仍然有效的value，继续追踪 12345678910111213141516V getLiveValue(ReferenceEntry&lt;K, V&gt; entry, long now) &#123; if (entry.getKey() == null) &#123; tryDrainReferenceQueues(); return null; &#125; V value = entry.getValueReference().get(); if (value == null) &#123; tryDrainReferenceQueues(); return null; &#125; if (map.isExpired(entry, now)) &#123; tryExpireEntries(now); return null; &#125; return value; &#125; 从if (map.isExpired(entry, now))这行可以看到是否过期是通过时间来判断的 12345678910boolean isExpired(ReferenceEntry&lt;K, V&gt; entry, long now) &#123; checkNotNull(entry); if (expiresAfterAccess() &amp;&amp; (now - entry.getAccessTime() &gt;= expireAfterAccessNanos)) &#123; return true; &#125; if (expiresAfterWrite() &amp;&amp; (now - entry.getWriteTime() &gt;= expireAfterWriteNanos)) &#123; return true; &#125; return false; &#125; 建造者模式上边我们在初始化loadingCache对象时，自主选择性的初始化了自己的参数。采用建造者模式赋予的灵活性对于初始化参数较多、自定义程度高的对象时一个很好的选择。 guava cache为我们提供了下面这些初始化方法。 1234567public CacheBuilder&lt;K, V&gt; maximumSize(long size)public CacheBuilder&lt;K, V&gt; initialCapacity(int initialCapacity)public CacheBuilder&lt;K, V&gt; maximumWeight(long weight)public CacheBuilder&lt;K, V&gt; refreshAfterWrite(long duration, TimeUnit unit)CacheBuilder&lt;K, V&gt; setValueStrength(Strength strength)public CacheBuilder&lt;K, V&gt; softValues() .... 参考： Guava 源码分析（Cache 原理） guava Cache源码分析（二）]]></content>
      <categories>
        <category>java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[如何在ubuntu16.04中搭建redis集群]]></title>
    <url>%2F2018%2F04%2F01%2F%E5%A6%82%E4%BD%95%E5%9C%A8ubuntu16.04%E4%B8%AD%E6%90%AD%E5%BB%BAredis%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[如何在ubuntu16.04中搭建redis集群 要想搭建一个最简单的Redis集群，那么至少需要6个节点：3个Master和3个Slave。 在ubuntu16.04中安装redis 12345678910111213//下载axel -n 5 http://download.redis.io/releases/redis-4.0.9.tar.gz//解压tar -zxvf redis-4.0.9.tar.gz编译make//为了方便启动redis，在profile中添加PATH#SET REDIS_HOMEexport REDIS_HOME=/opt/local/software/redis-4.0.9export PATH=$PATH:$REDIS_HOME/src 然后尝试启动redis检查是否安装成功，若出现下面的内容说明安装成功。 配置redis集群由于电脑配置有限，不大可能安装那么多虚拟机。可以在一台虚拟机中开启多个redis实例，将这多个实例组建成一个redis集群。首先创建六个文件夹，将redis-conf文件copy到每一个文件夹中。 123456cp ../redis-4.0.9/redis.conf ./8001cp ../redis-4.0.9/redis.conf ./8002cp ../redis-4.0.9/redis.conf ./8003cp ../redis-4.0.9/redis.conf ./8004cp ../redis-4.0.9/redis.conf ./8005cp ../redis-4.0.9/redis.conf ./8006 redis配置文件123456789101112131415161718# redis 提供服务时绑定的本机网络地址，可以是多个满足多块网卡分别对外网内网的访问, 按需要改bind 127.0.0.1# 提供服务的端口, 供客户端连接port 8001# 启动redis服务时,进程pid存储位置pidfile /opt/local/software/redis-node/8001/redis.pid# redis 日志文件所在logfile &quot;/opt/local/software/redis-node/8001/log.txt&quot;# 持久化文件存储目录dir /opt/local/software/redis-node/8001# 开启集群模式cluster-enabled yes# 集群模式下的节点配置信息cluster-config-file nodes.conf# 集群中各节点间连接超时时间cluster-node-timeout 5000# 允许数据持久化追加appendonly yes 然后根据上面的配置文件将每一个redis实例的端口号、目录、日志文件等等按照具体的情况设置。 启动redis123456redis-server 8001/redis.confredis-server 8002/redis.confredis-server 8003/redis.confredis-server 8004/redis.confredis-server 8005/redis.confredis-server 8006/redis.conf 启动完成之后先查看一下各个节点是否启动成功 12345678910leon at leon_ubuntu in /opt/local/software/redis-node $ ps aux | grep redisleon 15499 0.0 0.1 50908 5580 pts/19 Sl+ 14:22 0:02 redis-server 127.0.0.1:8001 [cluster]leon 15506 0.0 0.1 50908 5580 pts/4 Sl+ 14:22 0:02 redis-server 127.0.0.1:8002 [cluster]leon 15511 0.0 0.1 50908 5536 pts/18 Sl+ 14:23 0:02 redis-server 127.0.0.1:8003 [cluster]leon 15516 0.0 0.1 50908 5320 pts/21 Sl+ 14:23 0:02 redis-server 127.0.0.1:8004 [cluster]leon 15521 0.0 0.1 50908 5436 pts/20 Sl+ 14:23 0:02 redis-server 127.0.0.1:8005 [cluster]leon 15526 0.0 0.1 50908 5464 pts/22 Sl+ 14:23 0:02 redis-server 127.0.0.1:8006 [cluster]leon 15536 0.0 0.0 14352 2624 pts/23 S+ 14:23 0:00 redis-cli -c -p 8001leon 15886 0.0 0.0 14224 936 pts/25 S+ 15:05 0:00 grep --color=auto --exclude-dir=.bzr --exclude-dir=CVS --exclude-dir=.git --exclude-dir=.hg --exclude-dir=.svn redis 上面的几个节点都已经启动成功了，但是并没有组建成为集群，要想组建一个集群还需要使用redis-trib.rb工具将上面六个节点连接起来。但是它需要在ruby的环境下运行并且在这个环境下安装对应的redis插件。1sudo gem install redis 安装完成之后执行下面的指令将redis实例组建成一个集群1redis-trib.rb create --replicas 1 127.0.0.1:8001 127.0.0.1:8002 127.0.0.1:8003 127.0.0.1:8004 127.0.0.1:8005 127.0.0.1:8006 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253leon at leon_ubuntu in /opt/local/software/redis-4.0.9/src $ redis-trib.rb create --replicas 1 127.0.0.1:8001 127.0.0.1:8002 127.0.0.1:8003 127.0.0.1:8004 127.0.0.1:8005 127.0.0.1:8006&gt;&gt;&gt; Creating cluster&gt;&gt;&gt; Performing hash slots allocation on 6 nodes...Using 3 masters:127.0.0.1:8001127.0.0.1:8002127.0.0.1:8003Adding replica 127.0.0.1:8005 to 127.0.0.1:8001Adding replica 127.0.0.1:8006 to 127.0.0.1:8002Adding replica 127.0.0.1:8004 to 127.0.0.1:8003&gt;&gt;&gt; Trying to optimize slaves allocation for anti-affinity[WARNING] Some slaves are in the same host as their masterM: 1bc512b4d4b5d0c73a72d97e7d70ea307db7ae96 127.0.0.1:8001 slots:0-5460 (5461 slots) masterM: b9b6ff12c2a788a708a5fc754ee6e9add83ef089 127.0.0.1:8002 slots:5461-10922 (5462 slots) masterM: 01742676bd6bb86313c44365df28328a383a007f 127.0.0.1:8003 slots:10923-16383 (5461 slots) masterS: 5d3d629269a52017f0e1fb5d043f5797319aa306 127.0.0.1:8004 replicates 1bc512b4d4b5d0c73a72d97e7d70ea307db7ae96S: ab65c3dd627b885d60e7f01d69acc7fbd1db50b0 127.0.0.1:8005 replicates b9b6ff12c2a788a708a5fc754ee6e9add83ef089S: 2487d2d01333914e7d407a7fe89ddfa2f12884a9 127.0.0.1:8006 replicates 01742676bd6bb86313c44365df28328a383a007fCan I set the above configuration? (type &apos;yes&apos; to accept): yes&gt;&gt;&gt; Nodes configuration updated&gt;&gt;&gt; Assign a different config epoch to each node&gt;&gt;&gt; Sending CLUSTER MEET messages to join the clusterWaiting for the cluster to join..&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:8001)M: 1bc512b4d4b5d0c73a72d97e7d70ea307db7ae96 127.0.0.1:8001 slots:0-5460 (5461 slots) master 1 additional replica(s)M: b9b6ff12c2a788a708a5fc754ee6e9add83ef089 127.0.0.1:8002 slots:5461-10922 (5462 slots) master 1 additional replica(s)S: 2487d2d01333914e7d407a7fe89ddfa2f12884a9 127.0.0.1:8006 slots: (0 slots) slave replicates 01742676bd6bb86313c44365df28328a383a007fS: 5d3d629269a52017f0e1fb5d043f5797319aa306 127.0.0.1:8004 slots: (0 slots) slave replicates 1bc512b4d4b5d0c73a72d97e7d70ea307db7ae96M: 01742676bd6bb86313c44365df28328a383a007f 127.0.0.1:8003 slots:10923-16383 (5461 slots) master 1 additional replica(s)S: ab65c3dd627b885d60e7f01d69acc7fbd1db50b0 127.0.0.1:8005 slots: (0 slots) slave replicates b9b6ff12c2a788a708a5fc754ee6e9add83ef089[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 上面的日志打印看出已经成功将6个节点全部组建成一个redis集群了，同时将[8001,8002,8003]作为master，[8004，8005，8006]为前面三个的复制节点。 至此redis集群搭建完成。 参考： ubuntu安装redis集群 如何在Ubuntu 14.04配置的Redis集群 Linux下Redis集群安装部署及使用详解]]></content>
      <categories>
        <category>中间件</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[浅析java语言中锁机制]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%B5%85%E6%9E%90java%E8%AF%AD%E8%A8%80%E4%B8%AD%E9%94%81%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[浅析java语言中锁机制 &emsp;&emsp;多任务并发已经是计算机中不可缺少的一部分了，很多时候让计算机同一时刻做几件事，不仅仅是因为运算能力提高了，还有一个重要原因就是cpu的运算速度和存储、通信子系统速度差异太大。所以不得不“压榨”运算能力。 而压榨运算能力就不得不用到多线程，在单线程的情况下运算环境较为简单，而到了多线程情况下则会产生很多意想不到的结果。要保证在多线程的情况下各个线程“相安无事”的运行，则必须用到锁机制。 java内存模型在讲锁机制之前，我们必须先了解jvm的内存模型。 模型图如下 &emsp;&emsp;java线程读写数据是直接读写工作内存中的数据，工作内存可能并非是物理内存，为了性能很大可能是高速缓存和寄存器。线程间通信必须通过内内存来实现。假设初始值x = 0， 然后线程A将x值修改为1， 那么B要知道此时A修改过后的值就必须是A线程先将值从工作内存中写入到主内存，然后线程B再从主内存中读取x的值，如此完成线程间的通信–共享内存的方式。 JVM内存模型围绕三个点进行： 原子性、可见性、有序性。 原子性：不可分割，java中的基本类型的读写基本可以认为是原子操作，如果要在更大范围内保证原子性，可以是用synchronized来保证原子性。 可见性：当一个线程修改共享变量的值，其他线程能够立即得知这个变化。volatile、synchronized、final都能够保证内存可见性。volatile能够保证一个被修改之后立即同步到主内存中，然后其他线程使用到被volatile修饰的变量时，必须先从主内存中同步到工作内存中。synchronized保证可见性是一个线程unlock时，必须将修改的值同步到主内存中，下一个获取到lock的线程必须先同步主内存数据，从而保证可见性。 有序性：如果在本线程内观察，所有操作都是有序的；如果在另外一个线程中观察另一个线程，所有操作都是无序的。 前面半句意思是在同一个线程内表现为串行的语义，后半句是“指令重排序”现象和“工作内存与主内存之间延迟”现象。 先行发生：指的是如果A先行发生于B，那么操作A的影响能够被操作B观察到，“影响”包括修改了共享内存变量的值、发送的消息、调用方法等。由于优化重排序的原因，操作之间的时间上先后执行和“先行发生”并没有关系。 volatile 需要注意的是volatile并不是锁，它是jvm提供的最轻量级的同步机制。大多数人对volatile的原理和使用并不清楚。volatile主要具备两个特性： 保证内存可见性 禁止指令重排序 内存可见性&emsp;&emsp;被volatile修饰的变量，被线程修改之后会立即从工作内存中同步到主内存中，线程若要读被volatile修饰的变量则必须先从主内存中同步最新的值，这样就保证了线程之间变量的同步。但是线程之间变量的值并非立即同步的，各个线程之间的值可能存在不一致的情况，但是由于每一次读的时候都从主内存中更新，因此也不认为存有不一致的情况。 但是由于volatile只能保证可见性，并不保证原子性，所以在不符合下面的两种情况还是需要通过加锁来保证原子性。 运算结果并不依赖当前值 例如： a++ 、 b += 3 变量不需要参加其他状态变量共同参与不变约束。 例如： low &lt; up 禁止指令重排序 &emsp;&emsp;现代cpu为了提高执行效率，一个指令的执行被分成：取指、译码、访存、执行、写回、等若干个阶段。然后，多条指令可以同时存在于流水线中，同时被执行。但是无论怎样重排序必须要满足as-if-serial语义，这也是为什么我们在单线程的情况下编程并不需要考虑代码执行顺序的问题，因为无论怎样排序，最后执行的结果总是和顺序执行的结果一致。 &emsp;&emsp;&ensp; 禁止重排序时候通过内存屏障来实现的，即是说重排序时不能讲后面的指令重排序到内存屏障之前的位置。若只有一个cpu访问内存时并不需要内存屏障，若多个cpu访问内存时，并且一个正在观察另外一个时，就需要内存屏障来保证一致性。 &emsp;&emsp;&ensp; 下面摘自《深入理解Java虚拟机》： “观察加入volatile关键字和没有加入volatile关键字时所生成的汇编代码发现，加入volatile关键字时，会多出一个lock前缀指令”lock前缀指令实际上相当于一个内存屏障（也成内存栅栏），内存屏障会提供3个功能： 1、它确保指令重排序时不会把其后面的指令排到内存屏障之前的位置，也不会把前面的指令排到内存屏障的后面；即在执行到内存屏障这句指令时，在它前面的操作已经全部完成； 2）它会强制将对缓存的修改操作立即写入主存； 3）如果是写操作，它会导致其他CPU中对应的缓存行无效。 synchronized synchronized是我们最常使用的加锁方式，也是官方推荐的方式。 &emsp;&emsp;synchronized最常用的互斥有段，synchronized经过编译之后，会在同步块前后分别形成monitorenter和monitorexit两个字节码指令。这两个指令都需要明确指定一个引用类型的对象参数来确定需要锁定和解锁的对象。 &emsp;&emsp;&ensp; 根据虚拟机的规范要求，在执行monitorenter指令时，首先要尝试获取对象的锁，如果对象没有被锁定，或者当前线程已经拥有那个对象的锁，把锁的计数器加1，相应的，如果执行monitorexit指令就会将锁计数器减1，当计数器为0时，锁就被释放。如果获取对象锁失败，那么当前线程就要一直阻塞等待，直到对象锁被另外一个线程释放为止。同时Synchronized锁是可重入的，假设对象person有两个被Synchronized修饰的同步方法M、N，当线程执行完M之后还可以接着执行N，所以这在一定程度上避免了死锁， 如何保证可见性？ &emsp;&emsp;&ensp; 当一个线程释放锁的时候，将自己修改的数据从工作内存更新到主内存中，获取锁那个线程也会先将主内存中的数据同步到主机的工作内存中，如此就保证了内存可见性。 ReentrantLock ReentrantLock 类实现了Lock，它拥有与 synchronized相同的并发性和内存语义，但是添加了类似公平锁、轮询锁、定时锁等候和可中断锁等候的一些特性。此外，它还提供了在激烈争用情况下更佳的性能。 &emsp;&emsp;&ensp; 但相比synchronized锁来说，Lock锁使用需要手动获取、手动释放。我们在创建Lock实例的时候可以指定使用公平锁还是非公平锁，非公平锁性能会更好一下(公平锁需要维护公平所需要的记账和同步)，所以我们通常时候时都是用非公平锁。 123public ReentrantLock(boolean fair) &#123; sync = fair ? new FairSync() : new NonfairSync(); &#125; 这里FairSync和NonfairSync都继承了Sync。而公平锁和非公平锁的区别在于获取锁的方式。非公平锁： 1234567final void lock() &#123; // 先判断此时锁是否有其他线程持有，没有的话通过CAS更新锁状态 if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); else acquire(1); &#125; 自旋锁 Java的线程是映射到操作系统的原生线程之上的，如果要阻塞或唤醒一个线程，都需要操作系统来帮忙完成，这就需要从用户态转换到核心态中，因此状态装换需要耗费很多的处理器时间，对于代码简单的同步块状态转换消耗的时间有可能比用户代码执行的时间还要长。 &emsp;&emsp;&ensp; 那么就可以采用自旋锁的方式，假设这个时候有A线程持有锁，B线程等待锁，若是A线程持有锁的时间并不长，那么这个时候B线程不会被阻塞而是采用“忙等”的方式等待锁被释放。从而避免了操作系统切换用户态、内核态的消耗。 带来的问题：1、可能占用太多cpu资源： &emsp;&emsp;&ensp; 若是A线程在短时间内并不释放锁，那么这种方式可能占用太多cpu时间，导致cpu资源浪费。因此jvm有一个默认自旋次数限制（默认是10次，可以使用-XX:PreBlockSpin来更改），若还是没有获取到锁，那就应该使用传统的方式去挂起线程了。 2、死锁问题 &emsp;&emsp;&ensp; 假设有有一个线程运行在一个处理器上， 而另外一个线程想获取这个处理器的锁， 那么他将一直持有这个cpu进行自旋操作，你的线程代码则永远无法获得机会释放锁，于是陷入死锁。 自适应自旋&emsp;&emsp;&ensp; 在jdk1.6中引入了自适应的自旋锁。自适应意味着自旋的时间不在固定了，而是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来实现。如果在同一个锁对象刚刚成功获取到了锁，并且持有锁的线程正在运行中，那么jvm久认为这次也能够获取成功，那么久可能多尝试几次来获取，比方说50次。若是对于某一个锁，自适应锁很少成功过，那么jvm可能就忽略掉自旋的过程而直接挂起。 锁消除 锁消除是Java虚拟机在JIT编译是，通过对运行上下文的扫描，去除不可能存在共享资源竞争的锁，通过锁消除，可以节省毫无意义的请求锁时间。 &emsp;&emsp;&ensp; 在动态编译同步块的时候，JIT编译器可以借助一种被称为逃逸分析（Escape Analysis）的技术来判断同步块所使用的锁对象是否只能够被一个线程访问而没有被发布到其他线程。如果同步块所使用的锁对象通过这种分析被证实只能够被一个线程访问，那么JIT编译器在编译这个同步块的时候并不生成synchronized所表示的锁的申请与释放对应的机器码，而仅生成原临界区代码对应的机器码，这就造成了被动态编译的字节码就像是不包含monitorenter（申请锁）和monitorexit（释放锁）这两个字节码指令一样，即消除了锁的使用。这种编译器优化就被称为锁消除（Lock Elision），它使得特定情况下我们可以完全消除锁的开销。 示意图： 锁粗化 锁粗化（Lock Coarsening/Lock Merging）是JIT编译器对内部锁的具体实现所做的一种优化。 &emsp;&emsp;一般来说，我们加锁会尽量锁住更小的代码块， 这样使得需要同步的操作数量尽可能小，如果存在竞争那么也能尽快拿到锁。在通常情况下这都是正确的，但是如果反复对同一个对象加锁，甚至加锁操作出现在了循环体中，频繁的进行互斥操作也会导致不必要的性能损耗。如果虚拟机探测到有这样的情况的话，会把加锁同步的范围扩展到整个操作序列的外部，这样的一个锁范围扩展的操作就称之为锁粗化。 偏向锁 它的目的是消除数据在无竞争情况下的同步原语，进一步提高程序性能。如果说轻量级锁是在无竞争的情况下使用CAS操作去消除同步使用的互斥量，那偏向锁就是在无竞争的情况下把整个同步都消除掉，连CAS都不做。 &emsp;&emsp;偏向锁会偏向于第一个获得它的线程，如果在接下来的执行过程中，该锁没有被其他的线程获取，则持有偏向锁的线程将永远不需要同步。大多数情况下，锁不仅不存在多线程竞争，而且总是由同一线程多次获得，为了让线程获得锁的代价更低而引入了偏向锁。 &emsp;&emsp;当锁对象第一次被线程获取的时候，线程使用CAS操作把这个锁的线程ID记录再对象Mark Word之中，同时置偏向标志位1。以后该线程在进入和退出同步块时不需要进行CAS操作来加锁和解锁，只需要简单地测试一下对象头的Mark Word里是否存储着指向当前线程的偏向锁。如果测试成功，表示线程已经获得了锁 参考： -《深入理解java虚拟机》 http://blog.csdn.net/u013256816/article/details/51008443 https://segmentfault.com/a/1190000009828216 https://www.jianshu.com/p/dfbe0ebfec95 http://www.importnew.com/19472.html http://traxexer.iteye.com/blog/1846344 http://www.broadview.com.cn/article/789]]></content>
      <categories>
        <category>java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[如何将github pages映射到域名上]]></title>
    <url>%2F2018%2F03%2F17%2F%E5%A6%82%E4%BD%95%E5%B0%86github%20pages%E6%98%A0%E5%B0%84%E5%88%B0%E8%87%AA%E5%B7%B1%E7%9A%84%E5%9F%9F%E5%90%8D%2F</url>
    <content type="text"><![CDATA[使用github pages和hexo可以轻松搭建一个属于自己的博客，但是这个博客的域名是一个github.io结尾的。看着不爽，下面讲一下具体的步骤。关于如何使用github pages和hexo搭建博客，网上有很多，这里就不介绍了。 将username.github.io解析到自己的域名上，首先需要将域名解析到github pages的ip上，然后在github的settings中添加一个CNAME文件。具体步骤如下 注册域名可以去腾讯云上边买一个域名，便宜点的就行。 解析域名域名其实就是一个ip的重命名而已，所以解析就相当于将域名绑定到一个ip，那么以后我们就可以直接用域名来访问那个ip对应的服务器的服务。 这里直接使用腾讯提供的快速解析方法,将github提供的两个ip中的一个加入就行 192.30.252.153 192.30.252.154 创建CNAME文件 我们打开在github中的博客目录中的settings，填入自己的域名，然后save。就可以在项目的根目录中看到一个CNAME的文件。这个文件就已经填入了我们的域名，但是当我们重新发布之后这个CNAME文件会被覆盖，所以需要source目录下创建同样的一个CNAME的文件。 最后访问自己的域名，就能直接访问到博客了。]]></content>
      <categories>
        <category>博客</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[从输入URL到页面加载完成的过程中都发生了什么事情？]]></title>
    <url>%2F2018%2F01%2F14%2F%E5%AE%8C%E6%95%B4%E6%8F%8F%E8%BF%B0%E4%B8%80%E6%AC%A1URL%E8%AE%BF%E9%97%AE%E5%88%B0%E9%A1%B5%E9%9D%A2%E5%8A%A0%E8%BD%BD%E5%88%B0%E5%BA%95%E9%83%BD%E7%BB%8F%E5%8E%86%E4%BA%86%E4%BA%9B%E4%BB%80%E4%B9%88%2F</url>
    <content type="text"><![CDATA[从输入URL到页面加载完成的过程中都发生了什么事情？ &emsp;&emsp;平时我们总是习惯了输入一个网址，然后就静静的等待着各种漂亮的网页展示在我们的面前，但是可能大多数人都没有关注这其中都经历了哪些什么。下面大致做一个总结。 一、首先简单介绍一下OSI七层模型： &emsp;&emsp;开放式系统互联通信参考模型（英语：Open System Interconnection Reference Model，缩写为 OSI），简称为OSI模型（OSI model），一种概念模型，由国际标准化组织（ISO）提出，一个试图使各种计算机在世界范围内互连为网络的标准框架。 其中各层的详细内容大家可以参考维基百科 OSI是一个理想的分层模型，很少有完全遵守这样的分层，只会使用其中的几层而已。 二、从访问google说起这里就不讨论我们伟大的防火长城了，作为一个程序员，这是基本技能 当你单击回车键，就意味着你需要向google的服务器发送一次网页请求的数据包。下面从涉及到的协议或者服务说起。 1、DNS协议&emsp;&emsp;当我们要访问https://www.google.co.jp/时，必须要知道ip地址，但这个时候我们并不知道ip是多少。DNS服务就可以帮我们通过域名找到ip地址。它是一个存储着ip和域名映射的分布式数据库。GFW其中就有通过dns污染和dns劫持的方式使得我们无法访问到国外的某些“fd”网站。 这里需要注意的是，ip和域名并非一对一的关系。而是多对多，也就是说一个ip可以绑定多个域名，一个域名也可以绑定多个ip，但同一时刻只能是一对一的。 ok，现在我们有了ip地址。 2、 子网掩码 &emsp;&emsp;全世界这么多计算机，当然不可能都在同一个子网里。那这个时候就需要确认google服务器和我们本机是否在在同一个子网里，这里用到了子网掩码。 &emsp;&emsp;子网掩码和ip地址一样，都是32位的二进制数，通常用4段式表示，网络位全为1，主机位全为0。将我们的目标地址和本机地址分别与子网掩码按位做and操作。如果最终结果相同那么在同一子网中。不需要将数据包转发到网关。否则，将数据包转发到网关处理。 例如： DNS: 255.255.255.0 主机ip：10.20.0.121 google：216.239.34.10 明显不在同一个子网中 3、http/https协议浏览器访问网页使用的是http协议，在OSI协议中属于应用层协议。 http请求的报文格式： 头部（request line + header） + 数据（data） 其中的请求行（request line）包括请求方法字段、URL字段和HTTP协议的版本三个字段组成 GET/index.html HTTP/1.1 目前大多数HTTP请求使用的是长连接（HTTP/1.1默认keep-alive为true），而长连接意味着，一个TCP的socket在当前请求结束后，如果没有新的请求到来，socket不会立马释放，而是等timeout后再释放。 请求方法字段有：GET、POST、HEAD、PUT、DELETE、TRACE, 这些字段在REST ful风格中能够被充分利用，而不在使用URL来表达目的动作。 类似下面的内容： 123456789101112131415alt-svc:hq=":443"; ma=2592000; quic=51303431; quic=51303339; quic=51303338; quic=51303337; quic=51303335,quic=":443"; ma=2592000; v="41,39,38,37,35"cache-control:private, max-age=0content-encoding:brcontent-type:text/html; charset=UTF-8date:Thu, 18 Jan 2018 10:37:50 GMTexpires:-1p3p:CP="This is not a P3P policy! See g.co/p3phelp for more info."server:gwsset-cookie:1P_JAR=2018-01-18-10; expires=Sat, 17-Feb-2018 10:37:50 GMT; path=/; domain=.google.co.jpset-cookie:NID=*, 20-Jul-2018 10:37:50 GMT; path=/; domain=.google.co.jp; HttpOnlystatus:200strict-transport-security:max-age=3600x-frame-options:SAMEORIGINx-xss-protection:1; mode=blockRequest Headers 4、tcp、udp协议 tcp协议是输入传输层的协议，主要作用是建立端口到端口的通信，同时保证数据的正确传输和差错处理。 同时和tcp同出于传输层协议的还有udp（用户数据报协议），这个协议比较简单，不保证数据的可靠性，数据一旦发出无法知道对方是否收到。但是优点是过程简单，节省资源。 无论是tcp还是udp协议，http的数据包都是加入到他们的数据段中。 三次握手问题：所谓三次握手（Three-Way Handshake）即建立TCP连接，就是指建立一个TCP连接时，需要客户端和服务端总共发送3个包以确认连接的建立。在socket编程中，这一过程由客户端执行connect来触发。 （1）第一次握手：Client将标志位SYN置为1，随机产生一个值seq=J，并将该数据包发送给Server，Client进入SYN_SENT状态，等待Server确认。 （2）第二次握手：Server收到数据包后由标志位SYN=1知道Client请求建立连接，Server将标志位SYN和ACK都置为1，ack=J+1，随机产生一个值seq=K，并将该数据包发送给Client以确认连接请求，Server进入SYN_RCVD状态。 （3）第三次握手：Client收到确认后，检查ack是否为J+1，ACK是否为1，如果正确则将标志位ACK置为1，ack=K+1，并将该数据包发送给Server，Server检查ack是否为K+1，ACK是否为1，如果正确则连接建立成功，Client和Server进入ESTABLISHED状态，完成三次握手，随后Client与Server之间可以开始传输数据了。 白话一点就是： 主机： 你好，我想认识你 服务器：好的，我也想认识你 主机：很高兴认识你 那为什么一定要三次握手而不是两次呢？ 我们假设有下面的这种情况： &emsp;&emsp;当客户端发送一个请求，但是由于此时网络拥塞这个包暂时没有到达服务器， 客户端等待超时之后又重新发送一个请求，此时服务器端接收到了，并正确应答，双方开始通信，通信结束后释放socket连接。但这个时候，那个被拥塞的连接此时到达了服务器，如何tcp是采用两次握手的方式，那么此时服务器以为客户端又发起了一个连接请求，然后返回确认请求，此时连接就已经建立好了。服务器状态为ESTABLISHED，等待着客户端发送数据，但由于此时客户端进入了close状态，将导致服务器等待下去，浪费资源。 四次挥手： 首先需要知道的是tcp是全双工通信的，那么关闭就只能一个一个关闭。 当客户端和服务器需要关闭socket时（socket翻译成套接字并不恰当，把它当做“插座”也许更容易理解），具体的流程如下图所示： 假设这个时候client发起请求中断的请求，也就是发送Fin报文，告诉server：“我没有数据发给你了，但如果你还有数据要发给我，那就先不用关闭socket，发送完毕在告诉我”，此时client处于FIN_WAIT1。 当server接收到了之后发送ack给client：“你的请求我收到了，但是我还没有准备好，请继续等我消息”此时client进入FIN_WAIT2状态。 当server端确认数据发送完毕了，就告诉client：“好了，我这边数据发完了，准备好关闭连接了”。 但是这个时候client并不相信网络，怕server端不知道要关闭，所以进入TIME_WAIT,如果server端没有收到ack则可以重传，server端收到ack后就知道可以断开了，client等待2MSL(最大报文段生存时间)之后还没有收到回复，那么久认为server端正常关闭了，client也可以关闭了。 &emsp;&emsp;在这里需要注意最后client等待2MSL(最大报文段生存时间)，虽然按道理，四个报文都发送完毕，我们可以直接进入CLOSE状态了，但是我们必须假象网络是不可靠的，有可以最后一个ACK丢失。所以TIME_WAIT状态就是用来重发可能丢失的ACK报文。 5、IP协议，网络层 在七层网络分层中，网络层是最复杂的。负责主机到主机间的通信，其中ip协议是最重要的。 计算机若在同一个子网里边，完全可以只需要mac地址就可以通信，主机A想和主机B通信，那么只需要将请求广播到整个子网，然后各台计算机拿出目的mac地址和自己的mac地址相对比，如果相同，两台主机进行通信。但是计算机遍布全世界，不可能都在一个子网里边，若在其他子网里边怎么办呢？ &emsp;&emsp;这个时候就必须用到ip地址了。由于ip地址分为两部分，一部分是网络段，一部分是主机段。网络段决定处于哪一个子网，主机段决定是子网的那一台机器。当我们要访问google的服务器ip是216.239.34.10时，由于不在同一个子网中，需要将数据报转发给网关，然后再通过复杂的路由，最终到达服务器。 ip报文将tcp的报文嵌入在数据块中 6、ARP协议、以太网协议 最后，ip数据包嵌入到以太网数据报，以太网数据包需要设置通信双方的mac地址，发送方为本机的mac地址，接收方网卡216.239.34.1的mac地址需要用ARP协议得到。 &emsp;&emsp;我们需要一种机制，能够从IP地址得到MAC地址。这里又可以分成两种情况。第一种情况，如果两台主机不在同一个子网络，那么事实上没有办法得到对方的MAC地址，只能把数据包传送到两个子网络连接处的”网关”（gateway），让网关去处理。 &emsp;&emsp;第二种情况，如果两台主机在同一个子网络，那么我们可以用ARP协议，得到对方的MAC地址。ARP协议也是发出一个数据包（包含在以太网数据包中），其中包含它所要查询主机的IP地址，在对方的MAC地址这一栏，填的是FF:FF:FF:FF:FF:FF，表示这是一个”广播”地址。它所在子网络的每一台主机，都会收到这个数据包，从中取出IP地址，与自身的IP地址进行比较。如果两者相同，都做出回复，向对方报告自己的MAC地址，否则就丢弃这个包。 总之，有了ARP协议之后，我们就可以得到同一个子网络内的主机MAC地址，可以把数据包发送到任意一台主机之上了。 7、服务器响应 经过多个网关的转发，Google的服务器216.239.34.10，收到数据包之后通过解析，取出其中的请求request line，做出响应，封装成response返回给浏览器。 8、浏览器解析渲染页面浏览器在收到HTML,CSS,JS文件后，它是如何把页面呈现到屏幕上的？下图对应的就是WebKit渲染的过程。 由于涉及到很多的内容，难免有偏差的地方，还请见谅。 参考： https://zh.wikipedia.org/wiki/OSI%E6%A8%A1%E5%9E%8B http://blog.csdn.net/whuslei/article/details/6667471 http://www.ruanyifeng.com/blog/2012/06/internet_protocol_suite_part_ii.html http://www.ruanyifeng.com/blog/2012/05/internet_protocol_suite_part_i.html]]></content>
      <categories>
        <category>网络</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[类unix平台中几个大杀器指令]]></title>
    <url>%2F2017%2F12%2F14%2F%E7%B1%BBunix%E5%B9%B3%E5%8F%B0%E4%B8%AD%E5%87%A0%E4%B8%AA%E5%A4%A7%E6%9D%80%E5%99%A8%E6%8C%87%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[类unix平台中几个大杀器指令 下边介绍几个“很强势”的命令行指令，能大大提高我们的工作生产效率。使用macOS Sierra作为演示环境。 1、autojump 通常我们在类unix操作系统中切换目录都是使用cd，使用这个指令如果要切换到很深的目录，是很烦恼的。那么这个时候autojump就是大杀器。 1、 在mac中安装autojump比较简单： 1brew install autojump 但是需要注意安装完并不能直接使用。在我们安装的过程中会有如下的提示： 我们只需要按照步骤操作就好：（1） 先在~/.zshrc文件或者~/.bash_profile中添加 1[ -f /usr/local/etc/profile.d/autojump.sh ] &amp;&amp; . /usr/local/etc/profile.d/autojump.sh (2) 然后source相应的文件，重新打开一个窗口。就可以使用了。 简单介绍一下autojump的简单使用可以使用下面的命令来手动添加一个目录 1&gt; autojump -a [目录] 如果你突然想要把当前目录变成你的最爱和使用最频繁的文件夹，你可以在该目录通过命令的参数 i 来手工增加它的权重 1&gt; autojump -i [权重] 这将使得该目录更可能被选择跳转。相反的例子是在该目录使用参数 d 来减少权重： 1&gt; autojump -d [权重] 要跟踪所有这些改变，输入： 1&gt; autojump -s 2、mycli &emsp;&emsp; 对于程序员来说，数据库再熟悉不过了，很多时候我们更喜欢用命令行来访问数据库，但是默认自带的mysql对操作并不是十分友好，不能自动补全，没有语法高亮。这个时候使用mycli就再合适不过了。 1、 先安装，依旧使用brew来安装管理1brew install mycli 然后使用mycli -u username进入数据库，需要注意的是这里不需要-p参数。 如下所示： 3、screen 当我们在通过ssh连接远端机器执行任务时，如果任务执行时间相对比较短还好，如果是执行hadoop的任务，很多时候回超过几个小时，那么这个时候如果ssh连接中断了，那么这个任务也会失败。所以这个时候screen就派上用场了。 1&gt; brew install screen 进入后台执行： 1&gt; screen 当开始执行任务之后使用如下指令退出： 1&gt; control + A + D 将正在执行的任务列举出来 1&gt; screen -ls 查看任务执行进度： 1&gt; screen -r 进程号 还包括axel、fasd、m-cli等等有兴趣的小伙伴可以自己研究。]]></content>
      <categories>
        <category>macOS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[记录spark一次踩坑]]></title>
    <url>%2F2017%2F11%2F27%2F%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1spark%E8%B8%A9%E5%9D%91%2F</url>
    <content type="text"><![CDATA[Application has been killed. Reason: Master removed our application: FAILED spark从五月份开始跑到11月份一直正常，但是在最近两天报如下的错误。 查看worker运行的log，发现下面的问题。 然后进入到/opt/soft/spark/work目录发现当前有31999个文件， 而在linux文件系统中一个文件夹最多能够32000个子文件，包括一个当前目录、父级目录，那么操作这个数就无法在这个目录中写入文件。在linux 内核中有下面的定义： 12include/linux/ext2_fs.h:#define EXT2_LINK_MAX 32000include/linux/ext3_fs.h:#define EXT3_LINK_MAX 32000 所以，只能将生成的多余临时文件删除了。动手ing…. 删除之后，重新提交spark任务，没有问题。]]></content>
      <categories>
        <category>spark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[如何解决“非WeChat官方网页，继续访问将转换成.....”的问题]]></title>
    <url>%2F2017%2F10%2F18%2F%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E2%80%9C%E9%9D%9EWeChat%E5%AE%98%E6%96%B9%E7%BD%91%E9%A1%B5%EF%BC%8C%E7%BB%A7%E7%BB%AD%E8%AE%BF%E9%97%AE%E5%B0%86%E8%BD%AC%E6%8D%A2%E6%88%90......%E2%80%9D%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[我们在开发微信公众号时，使用网页工具生成自定义菜单，关联到某一个网站，当我们访问这个网页的时候就会出现如下情况： 要解决上述问题，可以采用如下方式： 1、登录微信公众号，进入公众号设置 –&gt; 功能设置，进入到下面的页面中，然后点击设置。 然后最终进入到设置的页面。有两个地方需要特别注意：（1）首先要确保设置的域名必须是备过案的。（2）同时还需要下载MP_verify_zvHGyljL5rHLDOB1.txt文件到能够访问的目录。 假设是java web的项目，备案的url可以具体到访问的首页目录（WebContent目录下），那么也就可以将MP_verify_zvHGyljL5rHLDOB1.txt文件放置到WebContent目录下。 通过以上的设置，过一段时间生效之后，就不会在出现安全提示了。]]></content>
      <categories>
        <category>java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring MVC访问静态资源文件]]></title>
    <url>%2F2017%2F09%2F27%2FSpringMVC%E8%AE%BF%E9%97%AE%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[Spring MVC访问静态资源文件 我们在使用spring MVC开发时，需要在web.xml文件中配置DispatcherServlet，指定映射条件，如下所示： 1234567891011121314&lt;servlet&gt; &lt;servlet-name&gt;Dispatcher&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;!--加载配置文件--&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:/conf/spring-mvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;Dispatcher&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; 通常都需要将DispatcherServlet配置为拦截所有请求，所以将url-pattern配置为”/“。但这样就会出现一个问题，通常在页面中需要使用到的js、css、图片等等静态文件就无法请求了。 一般有用两种方式来处理这种情况1、使用 mvc:resources的方式，将配置添加到appContext-**.xml文件中例如： 12 &lt;!-- 对静态资源文件的访问 --&gt;&lt;mvc:resources mapping="/**" location="/" /&gt; mapping是url映射处理，location是相对于WebContent的路径。 2、 配置DispatcherServlet值拦截.do的请求。 1234567891011121314&lt;servlet&gt; &lt;servlet-name&gt;Dispatcher&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet &lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:configure/spring-mvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;Dispatcher&lt;/servlet-name&gt; &lt;url-pattern&gt;*.do&lt;/url-pattern&gt;&lt;/servlet-mapping&gt;]]></content>
      <categories>
        <category>java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[爬取猫眼电影数据（一）]]></title>
    <url>%2F2017%2F09%2F24%2F%E7%88%AC%E5%8F%96%E7%8C%AB%E7%9C%BC%E7%94%B5%E5%BD%B1%E6%95%B0%E6%8D%AE-1%2F</url>
    <content type="text"><![CDATA[爬取猫眼电影数据(一) 概述：近期由于业务需要，需要将爬取猫眼网站中的部分数据作为公众号数据源，猫眼的电影数据相当之全备，大概收录了几十万部电影。当然这些数据我们也没那么容易就能爬取到的。遂将整个爬取过程记录如下。 1、使用chrome进入开发者模式，观察html结构、元素，找到我们所需数据，第一步我们需要现获取每一个电影的详情页面url。 注意，在爬取猫眼的电影数据时并不需要登录和携带cookie去请求html 2、 这里我们结合BeautifulSoup和xpath来实现对数据的过滤。实现代码如下： 12345678910111213def get_detail_url(self, targetUrl): """从概览页面中去获取剧透影片的详情页面""" html = self.getHtml(targetUrl) print html soup = BeautifulSoup(html, 'lxml') urls = soup.find_all('a', attrs=&#123;'data-act':'movie-click'&#125;) dd = soup.find_all('dd') print dd for url in urls: detailPath = etree.HTML(str(url)) detail_url = detailPath.xpath(r'//a/@href') for durl in detail_url: self.getContents('https://maoyan.com' + durl, proxys) 然后获取到对应每一个电影的详情url，进入详情页，分析所需数据对应html元素。在这里我们只需要三个字段、电影名称、电影海报url、电影上映时间。 提示:使用xpath时，最简单的方式是在chrome中点击选中具体某一个元素，然后右键——&gt; copy –&gt; copy xpath就能获取到对应的xpath路径 实现代码： 12345678910def getContents(self, url, proxys): """从详情页面中获取所需data""" html = self.getHtml(url,proxys) detailPath = etree.HTML(html) #电影名称 film_name = detailPath.xpath(r'/html/body/div[3]/div/div[2]/div[1]/h3/text()') #图片url pic_url = detailPath.xpath(r'/html/body/div[3]/div/div[1]/div/img/@src') #上映时间 release_time = detailPath.xpath(r'/html/body/div[3]/div/div[2]/div[1]/ul/li[3]/text()') 3、 这个时候获取到数据了，就该存库了。嗯，就存到mysql中吧。没啥说的，上代码。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748def insertData(self, my_dict): try: self.db.set_character_set('utf8') cols = ', '.join(my_dict.keys()) print cols values = '","'.join(my_dict.values()) print values sql = "INSERT INTO maoyan (%s) VALUES (%s)" % (cols, '"' + values + '"') try: result = self.cur.execute(sql) insert_id = self.db.insert_id() self.db.commit() # 判断是否执行成功 if result: return insert_id else: return 0 except MySQLdb.Error, e: # 发生错误时回滚 self.db.rollback() # 主键唯一，无法插入 if "key 'PRIMARY'" in e.args[1]: print self.getCurrentTime(), "数据已存在，未插入数据" else: print self.getCurrentTime(), "插入数据失败，原因 %d: %s" % (e.args[0], e.args[1]) except MySQLdb.Error, e: print self.getCurrentTime(), "数据库错误，原因%d: %s" % (e.args[0], e.args[1]) def dealData(self, my_dict): """先从数据库中查询这个影片名的记录，如果没有直接插入； 若是数据库中已经存在数据，那么什么都不做""" self.db.set_character_set('utf8') filmTitile = my_dict.get('film_title') result = self.findFilmByfilmTitle(filmTitile) if len(result) == 0: self.insertData(my_dict) return else: pass def findFilmByfilmTitle(self, filmTitile): """通过影片名称去数据库中查询""" self.db.set_character_set('utf8') sql = "select * from maoyan where film_title='%s'" % (filmTitile) self.cur.execute(sql) results = self.cur.fetchall() self.db.commit() return results 4、 万事俱备只欠启动了，跑啊跑啊。想必大家都想到了，猫眼肯定是不可能随便让人这样爬取他们的数据，而且短时间大量的请求对服务器的负载也是相当大的，不出意料爬取了不到300条数据，就被停止了。(ಥ _ ಥ)处理这种情况一般有两种方式，一种通过延缓请求，一种是通过代理ip的方式。第一种我尝试延缓5、10s，但并没有用。因而尝试用第二种方式，代理ip池的方式。代理ip如何使用在下一篇文章中记录。 下面是本次的完成代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133#!/usr/bin/env python# coding: utf-8import urllib2import urllibimport requestsfrom bs4 import BeautifulSoupfrom lxml import etreeimport MySQLdbimport timeimport randomimport sysreload(sys)sys.setdefaultencoding('utf8')"""猫眼不需要http请求的头部信息，添加上反而会出错"""class Maoyan(): # 获取当前时间 def getCurrentTime(self): return time.strftime('[%Y-%m-%d %H:%M:%S]', time.localtime(time.time())) def __init__(self): self.time = time self.base_url = "https://maoyan.com/films?showType=1&amp;sortId=2" try: self.db = MySQLdb.connect('localhost', 'flyer_user', 'Tu4a0X9hOPKz6jS!e', 'flyer_db',charset='utf8') self.cur = self.db.cursor() except MySQLdb.Error, e: print self.getCurrentTime(), "连接数据库错误，原因%d: %s" % (e.args[0], e.args[1]) # 插入数据 def insertData(self, my_dict): try: self.db.set_character_set('utf8') cols = ', '.join(my_dict.keys()) print cols values = '","'.join(my_dict.values()) print values sql = "INSERT INTO maoyan (%s) VALUES (%s)" % (cols, '"' + values + '"') try: result = self.cur.execute(sql) insert_id = self.db.insert_id() self.db.commit() # 判断是否执行成功 if result: return insert_id else: return 0 except MySQLdb.Error, e: # 发生错误时回滚 self.db.rollback() # 主键唯一，无法插入 if "key 'PRIMARY'" in e.args[1]: print self.getCurrentTime(), "数据已存在，未插入数据" else: print self.getCurrentTime(), "插入数据失败，原因 %d: %s" % (e.args[0], e.args[1]) except MySQLdb.Error, e: print self.getCurrentTime(), "数据库错误，原因%d: %s" % (e.args[0], e.args[1]) def dealData(self, my_dict): """先从数据库中查询这个影片名的记录，如果没有直接插入； 若是数据库中已经存在数据，那么什么都不做""" self.db.set_character_set('utf8') filmTitile = my_dict.get('film_title') result = self.findFilmByfilmTitle(filmTitile) if len(result) == 0: self.insertData(my_dict) return else: pass def findFilmByfilmTitle(self, filmTitile): """通过影片名称去数据库中查询""" self.db.set_character_set('utf8') sql = "select * from maoyan where film_title='%s'" % (filmTitile) self.cur.execute(sql) results = self.cur.fetchall() self.db.commit() return results def getHtml(self,url,proxys): """获取html""" req = urllib2.Request(url) html = urllib2.urlopen(req).read().decode('utf-8') return html def get_detail_url(self, targetUrl, proxys): """从概览页面中去获取剧透影片的详情页面""" html = self.getHtml(targetUrl, proxys) print html soup = BeautifulSoup(html, 'lxml') urls = soup.find_all('a', attrs=&#123;'data-act':'movie-click'&#125;) dd = soup.find_all('dd') print dd for url in urls: detailPath = etree.HTML(str(url)) detail_url = detailPath.xpath(r'//a/@href') for durl in detail_url: self.getContents('https://maoyan.com' + durl, proxys) def getContents(self, url, proxys): """从详情页面中获取所需data""" tmp1 = [] tmp2 = [] html = self.getHtml(url,proxys) detailPath = etree.HTML(html) film_name = detailPath.xpath(r'/html/body/div[3]/div/div[2]/div[1]/h3/text()') for name in film_name: tmp1.append("film_title") tmp2.append(name) pic_url = detailPath.xpath(r'/html/body/div[3]/div/div[1]/div/img/@src') for url in pic_url: strs = url.split('@') tmp1.append("pic_url") tmp2.append(strs[0]) release_time = detailPath.xpath(r'/html/body/div[3]/div/div[2]/div[1]/ul/li[3]/text()') for time in release_time: tmp1.append("release_time") tmp2.append(time[:10]) #将两个列表转换为字典，方便插入数据库 result_dict = dict(zip(tmp1, tmp2)) self.dealData(result_dict) def main(self): self.get_detail_url(self.base_url, proxys) for i in range(50): targetUrl = self.base_url + '&amp;offset=' + str((i+1)*30) self.get_detail_url(targetUrl, proxys)if __name__ == '__main__': maoyan = Maoyan() maoyan.main()]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python爬虫如何获取到只需要的数据]]></title>
    <url>%2F2017%2F09%2F18%2Fpython%E7%88%AC%E8%99%AB%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96%E5%88%B0%E5%8F%AA%E9%9C%80%E8%A6%81%E7%9A%84%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[python爬虫如何获取到只需要的数据 概述：在爬虫的学习过程中，最麻烦的也许就是如何从一大堆html标签中过滤出我们所需要的，在这里有三种方法：re正则表达式、BeautifulSoup、使用lxml中的xpath。 1 、先介绍如何使用lxml中的xpath(1) 我们需要安装相应的依赖，然后在项目中导入相应的模块 12from lxml import etreeimport lxml (2) 之后再代码中转化成xpath12detailPath = etree.HTML(html) questions_titile = detailPath.xpath('//ul[@class="list-group"]/li/div/div[@class="question-title"]/a/text()') 注意上边的两个单引号之间的一串，前面两个’//‘是表示从这里开始，然后一级一级的去寻找所需要的元素，如果想获取属性值，例如是图片的url，最后可以使用@href，如果想获取里边的文本值，那就使用text()。 (3) 特殊用法12345678910from lxml import etreehtml=&quot;&quot;&quot; &lt;body&gt; &lt;div id=&quot;aa&quot;&gt;aa&lt;/div&gt; &lt;div id=&quot;ab&quot;&gt;ab&lt;/div&gt; &lt;div id=&quot;ac&quot;&gt;ac&lt;/div&gt; &lt;/body&gt; &quot;&quot;&quot;selector=etree.HTML(html)content=selector.xpath(&apos;//div[starts-with(@id,&quot;a&quot;)]/text()&apos;) #这里使用starts-with方法提取div的id标签属性值开头为a的div标签 这篇文章有简单应用：http://blog.csdn.net/winterto1990/article/details/47903653介绍 最后一个大绝招：直接在chrome中开发者模式，右键找到你想寻找的元素标签，copy xpath就能将这个元素对应的xpath找到！ 1 、python中提供的re模块 正则表达式是一种更为强大的字符串匹配、字符串查找、字符串替换等操作工具。上篇讲解了正则表达式的基本概念和语法以及re模块的基本使用方式，这节来详细说说 re 模块作为 Python 正则表达式引擎提供了哪些便利性操作。正则表达式的所有操作都是围绕着匹配对象(Match)进行的，只有表达式与字符串匹配才有可能进行后续操作。判断匹配与否有两个方法，分别是 re.match() 和 re.search()，两者有什么区别呢？ 区别：match 方法从字符串的起始位置开始检查，如果刚好有一个子字符串与正则表达式相匹配，则返回一个Match对象，只要起始位置不匹配则退出，不再往后检查了，返回 None 1234&gt;&gt;&gt; re.match(r&quot;b.r&quot;, &quot;foobar&quot;) # 不匹配&gt;&gt;&gt; re.match(r&quot;b.r&quot;, &quot;barfoo&quot;) # 匹配&lt;_sre.SRE_Match object at 0x102f05b28&gt;&gt;&gt;&gt; search 方法虽然也是从起始位置开始检查，但是它在起始位置不匹配的时候会一直尝试往后检查，直到匹配为止，如果到字符串的末尾还没有匹配，则返回 None123&gt;&gt;&gt; re.search(r&quot;b.r&quot;, &quot;foobar&quot;) # 匹配&lt;_sre.SRE_Match object at 0x000000000254D578&gt;&gt;&gt;&gt; re.match(r&quot;b.r&quot;, &quot;foobr&quot;) # 不匹配 但是二者都是匹配到就停止匹配了，哪怕还有更多能匹配的也不管。 参考：https://foofish.net/re-tutorial.html 、https://foofish.net/crawler-re-second.html]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[上传本地jar包到nexus仓库中]]></title>
    <url>%2F2017%2F09%2F16%2F%E4%B8%8A%E4%BC%A0%E6%9C%AC%E5%9C%B0jar%E5%8C%85%E5%88%B0nexus%E4%BB%93%E5%BA%93%E4%B8%AD%2F</url>
    <content type="text"><![CDATA[#上传本地jar包到nexus仓库中 概述：有时在我们的nexus仓库中没有项目中所需的jar包，这个时候就需要从网络中下载，然后上传到nexus仓库中，然后我们就可以嗨森的敲代码啦。 1、首先需要是对应的repository有上传的权限 2、我们这里讲jar包上传到3rd party仓库中。先点击artifact upload然后选择GAV definition，手动将group id、artifact、version填入，然后packaging选择jar。 3、然后点击select artifact（s） to upload将本地的jar加入，点击add artifact，然后点击upload artifact。 4、最后刷新就能看到刚才加入的jar包。 此外需要注意的是，将入到3rd仓库之后可能对应的坐标会发生变化。如果前后坐标没有对应上，那么在pom文件中还是找不到相应的依赖。]]></content>
      <categories>
        <category>java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[hive如何实现自定义函数]]></title>
    <url>%2F2017%2F09%2F14%2Fhive%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[hive如何实现自定义函数 概述：我们在使用hive的时候，很多时候执行复杂的sql时，hive提供的函数无法满足我们的需求。这个时候就需要我们自己去定义一个函数。幸好，hive为我们允许使用java来子线自定义函数。 1、实现自定义UDF函数必须满足两个条件。 （1） 必须是org.apache.Hadoop.hive.ql.exec.UDF的子类 （2） 必须实现evaluate函数。 2、实现方法具体如下： （1）将$HIVE_HOME/lib中的两个jar包，分别是hive-contrib-2.1.1.jar和hive-exec-2.1.1.jar，把两个jar包放置到java的目录下，然后新建一个类，继承UDF。 123456789101112131415161718package org.apache.hadoop.hive.contrib.udf.example;import org.apache.commons.lang.StringUtils;import org.apache.hadoop.hive.ql.exec.UDF;import java.util.Arrays;public class ReturnMin extends UDF&#123; public static Integer evaluate(String str, String separator) &#123; int result = 100; if(StringUtils.isEmpty(str))&#123; return result; &#125; String[] strArray = str.split(separator); int[] intArray = new int[strArray.length]; for(int i = 0 ; i &lt; strArray.length ; i++) &#123; intArray[i] = Integer.parseInt(strArray[i]); &#125; Arrays.sort(intArray); return intArray[0]; &#125; (2) 然后将ReturnMin.class文件拷贝到hive-contrib-2.1.1.jar\org\apache\hadoop\hive\contrib\udf\example目录中，然后替换hive select在hive的cli中执行：CREATE FUNCTION ReturnMin AS ‘org.apache.hadoop.hive.contrib.udf.example.ReturnMin’;然后重新进入cli中，给这个函数传递一个数字的字符串和分隔符就能够将这个字符串最小的值返回。 hive删除一个自定义函数： drop function cutString;]]></content>
      <categories>
        <category>hive</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[hive中sql总结]]></title>
    <url>%2F2017%2F09%2F14%2Fhive%E4%B8%ADsql%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[hive中sql总结1、对指定的某一个列去重1insert into table allrecord partition (year=2017, month=08, day=25) select t.id, t.tssend, t.apmac, t.mac, t.rssi from (select id, tssend, apmac ,mac, rssi, row_number() over(distribute by mac sort by tssend )as rn from allrecords) t where t.rn=1; 上面试对字段mac去重，然后对tssend排序后插入allrecord这张表 2、删除分区1ALTER TABLE macinfo_2017 DROP IF EXISTS PARTITION (year=2017, month=03, day=22); 3、删除一张表，如果启动了回收站功能，那么这张表会保存在回收站里，也是能够恢复信息的。1drop table tablename; 若是表创建为外部表，则只会删除相应的原信息，对数据没有影响。 4、 创建一个partition_table002的表，使得表结构和partition_table001一样。1create table if not exists partition_table002 like partition_table001; 5、使用load加载数据到表中1load data local inpath 't_student1.txt' overwrite inte table t_student1; 6、创建外部表create EXTERNAL table IF NOT EXISTS macinfo_20170221( tssend string COMMENT 'Probe upload data time', apmac string COMMENT 'Probe MAC address', mac string COMMENT 'Mobile device MAC address', rssi string COMMENT 'WiFi signal strength of mobile devices') partitioned by (year int, month int, day int) ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe' WITH SERDEPROPERTIES ( "mac"="$.mac", "apmac"="$.apmac", "rssi"="$.rssi", "tssend"="$.tssend" ) STORED AS TEXTFILE LOCATION '/opt/local/software/a.txt'; 注意：这里创建外部表时会有坑，主要是数据存储格式的问题，我的数据存储为json格式，而hive并不直接支持读取json格式数据，需要添加相应的jar包才能解析数据。 7、hive外部表创建分区关联相应的目录（必须是目录）ALTER TABLE macinfo_2017 ADD PARTITION (year = 2017, month = 02, day = 21); 未完待续。。。。]]></content>
      <categories>
        <category>hive</category>
      </categories>
  </entry>
</search>
