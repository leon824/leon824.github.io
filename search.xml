<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[如何在ubuntu16.04中搭建redis集群]]></title>
    <url>%2F2018%2F04%2F01%2F%E5%A6%82%E4%BD%95%E5%9C%A8ubuntu16.04%E4%B8%AD%E6%90%AD%E5%BB%BAredis%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[如何在ubuntu16.04中搭建redis集群 要想搭建一个最简单的Redis集群，那么至少需要6个节点：3个Master和3个Slave。 在ubuntu16.04中安装redis 12345678910111213//下载axel -n 5 http://download.redis.io/releases/redis-4.0.9.tar.gz//解压tar -zxvf redis-4.0.9.tar.gz编译make//为了方便启动redis，在profile中添加PATH#SET REDIS_HOMEexport REDIS_HOME=/opt/local/software/redis-4.0.9export PATH=$PATH:$REDIS_HOME/src 然后尝试启动redis检查是否安装成功，若出现下面的内容说明安装成功。 配置redis集群由于电脑配置有限，不大可能安装那么多虚拟机。可以在一台虚拟机中开启多个redis实例，将这多个实例组建成一个redis集群。首先创建六个文件夹，将redis-conf文件copy到每一个文件夹中。 123456cp ../redis-4.0.9/redis.conf ./8001cp ../redis-4.0.9/redis.conf ./8002cp ../redis-4.0.9/redis.conf ./8003cp ../redis-4.0.9/redis.conf ./8004cp ../redis-4.0.9/redis.conf ./8005cp ../redis-4.0.9/redis.conf ./8006 redis配置文件123456789101112131415161718# redis 提供服务时绑定的本机网络地址，可以是多个满足多块网卡分别对外网内网的访问, 按需要改bind 127.0.0.1# 提供服务的端口, 供客户端连接port 8001# 启动redis服务时,进程pid存储位置pidfile /opt/local/software/redis-node/8001/redis.pid# redis 日志文件所在logfile &quot;/opt/local/software/redis-node/8001/log.txt&quot;# 持久化文件存储目录dir /opt/local/software/redis-node/8001# 开启集群模式cluster-enabled yes# 集群模式下的节点配置信息cluster-config-file nodes.conf# 集群中各节点间连接超时时间cluster-node-timeout 5000# 允许数据持久化追加appendonly yes 然后根据上面的配置文件将每一个redis实例的端口号、目录、日志文件等等按照具体的情况设置。 启动redis123456redis-server 8001/redis.confredis-server 8002/redis.confredis-server 8003/redis.confredis-server 8004/redis.confredis-server 8005/redis.confredis-server 8006/redis.conf 启动完成之后先查看一下各个节点是否启动成功 12345678910leon at leon_ubuntu in /opt/local/software/redis-node $ ps aux | grep redisleon 15499 0.0 0.1 50908 5580 pts/19 Sl+ 14:22 0:02 redis-server 127.0.0.1:8001 [cluster]leon 15506 0.0 0.1 50908 5580 pts/4 Sl+ 14:22 0:02 redis-server 127.0.0.1:8002 [cluster]leon 15511 0.0 0.1 50908 5536 pts/18 Sl+ 14:23 0:02 redis-server 127.0.0.1:8003 [cluster]leon 15516 0.0 0.1 50908 5320 pts/21 Sl+ 14:23 0:02 redis-server 127.0.0.1:8004 [cluster]leon 15521 0.0 0.1 50908 5436 pts/20 Sl+ 14:23 0:02 redis-server 127.0.0.1:8005 [cluster]leon 15526 0.0 0.1 50908 5464 pts/22 Sl+ 14:23 0:02 redis-server 127.0.0.1:8006 [cluster]leon 15536 0.0 0.0 14352 2624 pts/23 S+ 14:23 0:00 redis-cli -c -p 8001leon 15886 0.0 0.0 14224 936 pts/25 S+ 15:05 0:00 grep --color=auto --exclude-dir=.bzr --exclude-dir=CVS --exclude-dir=.git --exclude-dir=.hg --exclude-dir=.svn redis 上面的几个节点都已经启动成功了，但是并没有组建成为集群，要想组建一个集群还需要使用redis-trib.rb工具将上面六个节点连接起来。但是它需要在ruby的环境下运行并且在这个环境下安装对应的redis插件。1sudo gem install redis 安装完成之后执行下面的指令将redis实例组建成一个集群1redis-trib.rb create --replicas 1 127.0.0.1:8001 127.0.0.1:8002 127.0.0.1:8003 127.0.0.1:8004 127.0.0.1:8005 127.0.0.1:8006 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253leon at leon_ubuntu in /opt/local/software/redis-4.0.9/src $ redis-trib.rb create --replicas 1 127.0.0.1:8001 127.0.0.1:8002 127.0.0.1:8003 127.0.0.1:8004 127.0.0.1:8005 127.0.0.1:8006&gt;&gt;&gt; Creating cluster&gt;&gt;&gt; Performing hash slots allocation on 6 nodes...Using 3 masters:127.0.0.1:8001127.0.0.1:8002127.0.0.1:8003Adding replica 127.0.0.1:8005 to 127.0.0.1:8001Adding replica 127.0.0.1:8006 to 127.0.0.1:8002Adding replica 127.0.0.1:8004 to 127.0.0.1:8003&gt;&gt;&gt; Trying to optimize slaves allocation for anti-affinity[WARNING] Some slaves are in the same host as their masterM: 1bc512b4d4b5d0c73a72d97e7d70ea307db7ae96 127.0.0.1:8001 slots:0-5460 (5461 slots) masterM: b9b6ff12c2a788a708a5fc754ee6e9add83ef089 127.0.0.1:8002 slots:5461-10922 (5462 slots) masterM: 01742676bd6bb86313c44365df28328a383a007f 127.0.0.1:8003 slots:10923-16383 (5461 slots) masterS: 5d3d629269a52017f0e1fb5d043f5797319aa306 127.0.0.1:8004 replicates 1bc512b4d4b5d0c73a72d97e7d70ea307db7ae96S: ab65c3dd627b885d60e7f01d69acc7fbd1db50b0 127.0.0.1:8005 replicates b9b6ff12c2a788a708a5fc754ee6e9add83ef089S: 2487d2d01333914e7d407a7fe89ddfa2f12884a9 127.0.0.1:8006 replicates 01742676bd6bb86313c44365df28328a383a007fCan I set the above configuration? (type &apos;yes&apos; to accept): yes&gt;&gt;&gt; Nodes configuration updated&gt;&gt;&gt; Assign a different config epoch to each node&gt;&gt;&gt; Sending CLUSTER MEET messages to join the clusterWaiting for the cluster to join..&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:8001)M: 1bc512b4d4b5d0c73a72d97e7d70ea307db7ae96 127.0.0.1:8001 slots:0-5460 (5461 slots) master 1 additional replica(s)M: b9b6ff12c2a788a708a5fc754ee6e9add83ef089 127.0.0.1:8002 slots:5461-10922 (5462 slots) master 1 additional replica(s)S: 2487d2d01333914e7d407a7fe89ddfa2f12884a9 127.0.0.1:8006 slots: (0 slots) slave replicates 01742676bd6bb86313c44365df28328a383a007fS: 5d3d629269a52017f0e1fb5d043f5797319aa306 127.0.0.1:8004 slots: (0 slots) slave replicates 1bc512b4d4b5d0c73a72d97e7d70ea307db7ae96M: 01742676bd6bb86313c44365df28328a383a007f 127.0.0.1:8003 slots:10923-16383 (5461 slots) master 1 additional replica(s)S: ab65c3dd627b885d60e7f01d69acc7fbd1db50b0 127.0.0.1:8005 slots: (0 slots) slave replicates b9b6ff12c2a788a708a5fc754ee6e9add83ef089[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 上面的日志打印看出已经成功将6个节点全部组建成一个redis集群了，同时将[8001,8002,8003]作为master，[8004，8005，8006]为前面三个的复制节点。 至此redis集群搭建完成。 参考： ubuntu安装redis集群 如何在Ubuntu 14.04配置的Redis集群 Linux下Redis集群安装部署及使用详解]]></content>
      <categories>
        <category>中间件</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[浅析java语言中锁机制]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%B5%85%E6%9E%90java%E8%AF%AD%E8%A8%80%E4%B8%AD%E9%94%81%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[浅析java语言中锁机制 多任务并发已经是计算机中不可缺少的一部分了，很多时候让计算机同一时刻做几件事，不仅仅是因为运算能力提高了，还有一个重要原因就是cpu的运算速度和存储、通信子系统速度差异太大。所以不得不“压榨”运算能力。 而压榨运算能力就不得不用到多线程，在单线程的情况下运算环境较为简单，而到了多线程情况下则会产生很多意想不到的结果。要保证在多线程的情况下各个线程“相安无事”的运行，则必须用到锁机制。 java内存模型在讲锁机制之前，我们必须先了解jvm的内存模型。 模型图如下 java线程读写数据是直接读写工作内存中的数据，工作内存可能并非是物理内存，为了性能很大可能是高速缓存和寄存器。线程间通信必须通过共享内存来实现。假设初始值x = 0， 然后线程A将x值修改为1， 那么B要知道此时A修改过后的值就必须是A线程先将值从工作内存中写入到主内存，然后线程B再从主内存中读取x的值，如此完成线程间的通信–共享内存的方式。 JVM内存模型围绕三个点进行： 原子性、可见性、有序性。 原子性：不可分割，java中的基本类型的读写基本可以认为是原子操作，如果要在更大范围内保证原子性，可以是用synchronized来保证原子性。 可见性：当一个线程修改共享变量的值，其他线程能够立即得知这个变化。volatile、synchronized、final都能够保证内存可见性。volatile能够保证一个被修改之后立即同步到主内存中，然后其他线程使用到被volatile修饰的变量时，必须先从主内存中同步到工作内存中。synchronized保证可见性是一个线程unlock时，必须将修改的值同步到主内存中，下一个获取到lock的线程必须先同步主内存数据，从而保证可见性。 有序性：如果在本线程内观察，所有操作都是有序的；如果在另外一个线程中观察另一个线程，所有操作都是无序的。 前面半句意思是在同一个线程内表现为串行的语义，后半句是“指令重排序”现象和“工作内存与主内存之间延迟”现象。 先行发生：指的是如果A先行发生于B，那么操作A的影响能够被操作B观察到，“影响”包括修改了共享内存变量的值、发送的消息、调用方法等。由于优化重排序的原因，操作之间的时间上先后执行和“先行发生”并没有关系。 volatile 需要注意的是volatile并不是锁，它是jvm提供的最轻量级的同步机制。大多数人对volatile的原理和使用并不清楚。volatile主要具备两个特性： 保证内存可见性 禁止指令重排序 内存可见性被volatile修饰的变量，被线程修改之后会立即从工作内存中同步到主内存中，线程若要读被volatile修饰的变量则必须先从主内存中同步最新的值，这样就保证了线程之间变量的同步。但是线程之间变量的值并非立即同步的，各个线程之间的值可能存在不一致的情况，但是由于每一次读的时候都从主内存中更新，因此也不认为存有不一致的情况。 但是由于volatile只能保证可见性，并不保证原子性，所以在不符合下面的两种情况还是需要通过加锁来保证原子性。 运算结果并不依赖当前值 例如： a++ 、 b += 3 变量不需要参加其他状态变量共同参与不变约束。 例如： low &lt; up 禁止指令重排序 现代cpu为了提高执行效率，一个指令的执行被分成：取指、译码、访存、执行、写回、等若干个阶段。然后，多条指令可以同时存在于流水线中，同时被执行。但是无论怎样重排序必须要满足as-if-serial语义，这也是为什么我们在单线程的情况下编程并不需要考虑代码执行顺序的问题，因为无论怎样排序，最后执行的结果总是和顺序执行的结果一致。 禁止重排序时候通过内存屏障来实现的，即是说重排序时不能讲后面的指令重排序到内存屏障之前的位置。若只有一个cpu访问内存时并不需要内存屏障，若多个cpu访问内存时，并且一个正在观察另外一个时，就需要内存屏障来保证一致性。 下面摘自《深入理解Java虚拟机》： “观察加入volatile关键字和没有加入volatile关键字时所生成的汇编代码发现，加入volatile关键字时，会多出一个lock前缀指令”lock前缀指令实际上相当于一个内存屏障（也成内存栅栏），内存屏障会提供3个功能： 1、它确保指令重排序时不会把其后面的指令排到内存屏障之前的位置，也不会把前面的指令排到内存屏障的后面；即在执行到内存屏障这句指令时，在它前面的操作已经全部完成； 2）它会强制将对缓存的修改操作立即写入主存； 3）如果是写操作，它会导致其他CPU中对应的缓存行无效。 synchronized synchronized是我们最常使用的加锁方式，也是官方推荐的方式。 synchronized是最常用的互斥手段，synchronized经过编译之后，会在同步块前后分别形成monitorenter和monitorexit两个字节码指令。这两个指令都需要明确指定一个引用类型的对象参数来确定需要锁定和解锁的对象。 根据虚拟机的规范要求，在执行monitorenter指令时，首先要尝试获取对象的锁，如果对象没有被锁定，或者当前线程已经拥有那个对象的锁，把锁的计数器加1，相应的，如果执行monitorexit指令就会将锁计数器减1，当计数器为0时，锁就被释放。如果获取对象锁失败，那么当前线程就要一直阻塞等待，直到对象锁被另外一个线程释放为止。同时Synchronized锁是可重入的，假设对象person有两个被Synchronized修饰的同步方法M、N，当线程执行完M之后还可以接着执行N，所以这在一定程度上避免了死锁， 如何保证可见性？ 当一个线程释放锁的时候，将自己修改的数据从工作内存更新到主内存中，获取锁那个线程也会先将主内存中的数据同步到自己的工作内存中，如此就保证了内存可见性。 自旋锁 Java的线程是映射到操作系统的原生线程之上的，如果要阻塞或唤醒一个线程，都需要操作系统来帮忙完成，这就需要从用户态转换到核心态中，因此状态装换需要耗费很多的处理器时间，对于代码简单的同步块状态转换消耗的时间有可能比用户代码执行的时间还要长。 那么就可以采用自旋锁的方式，假设这个时候有A线程持有锁，B线程等待锁，若是A线程持有锁的时间并不长，那么这个时候B线程不会被阻塞而是采用“忙等”的方式等待锁被释放。从而避免了操作系统切换用户态、内核态的消耗。 带来的问题：1、可能占用太多cpu资源： 若是A线程在短时间内并不释放锁，那么这种方式可能占用太多cpu时间，导致cpu资源浪费。因此jvm有一个默认自旋次数限制（默认是10次，可以使用-XX:PreBlockSpin来更改），若还是没有获取到锁，那就应该使用传统的方式去挂起线程了。 2、死锁问题 假设有有一个线程运行在一个处理器上， 而另外一个线程想获取这个处理器的锁， 那么他将一直持有这个cpu进行自旋操作，你的线程代码则永远无法获得机会释放锁，于是陷入死锁。 自适应自旋 在jdk1.6中引入了自适应的自旋锁。自适应意味着自旋的时间不在固定了，而是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来实现。如果在同一个锁对象刚刚成功获取到了锁，并且持有锁的线程正在运行中，那么jvm久认为这次也能够获取成功，那么久可能多尝试几次来获取，比方说50次。若是对于某一个锁，自适应锁很少成功过，那么jvm可能就忽略掉自旋的过程而直接挂起。 锁消除 锁消除是Java虚拟机在JIT编译是，通过对运行上下文的扫描，去除不可能存在共享资源竞争的锁，通过锁消除，可以节省毫无意义的请求锁时间。 在动态编译同步块的时候，JIT编译器可以借助一种被称为逃逸分析（Escape Analysis）的技术来判断同步块所使用的锁对象是否只能够被一个线程访问而没有被发布到其他线程。如果同步块所使用的锁对象通过这种分析被证实只能够被一个线程访问，那么JIT编译器在编译这个同步块的时候并不生成synchronized所表示的锁的申请与释放对应的机器码，而仅生成原临界区代码对应的机器码，这就造成了被动态编译的字节码就像是不包含monitorenter（申请锁）和monitorexit（释放锁）这两个字节码指令一样，即消除了锁的使用。这种编译器优化就被称为锁消除（Lock Elision），它使得特定情况下我们可以完全消除锁的开销。 示意图： 锁粗化 锁粗化（Lock Coarsening/Lock Merging）是JIT编译器对内部锁的具体实现所做的一种优化。 一般来说，我们加锁会尽量锁住更小的代码块， 这样使得需要同步的操作数量尽可能小，如果存在竞争那么也能尽快拿到锁。在通常情况下这都是正确的，但是如果反复对同一个对象加锁，甚至加锁操作出现在了循环体中，频繁的进行互斥操作也会导致不必要的性能损耗。如果虚拟机探测到有这样的情况的话，会把加锁同步的范围扩展到整个操作序列的外部，这样的一个锁范围扩展的操作就称之为锁粗化。 偏向锁 它的目的是消除数据在无竞争情况下的同步原语，进一步提高程序性能。如果说轻量级锁是在无竞争的情况下使用CAS操作去消除同步使用的互斥量，那偏向锁就是在无竞争的情况下把整个同步都消除掉，连CAS都不做。 偏向锁会偏向于第一个获得它的线程，如果在接下来的执行过程中，该锁没有被其他的线程获取，则持有偏向锁的线程将永远不需要同步。大多数情况下，锁不仅不存在多线程竞争，而且总是由同一线程多次获得，为了让线程获得锁的代价更低而引入了偏向锁。 当锁对象第一次被线程获取的时候，线程使用CAS操作把这个锁的线程ID记录再对象Mark Word之中，同时置偏向标志位1。以后该线程在进入和退出同步块时不需要进行CAS操作来加锁和解锁，只需要简单地测试一下对象头的Mark Word里是否存储着指向当前线程的偏向锁。如果测试成功，表示线程已经获得了锁 参考： -《深入理解java虚拟机》 http://blog.csdn.net/u013256816/article/details/51008443 https://segmentfault.com/a/1190000009828216 https://www.jianshu.com/p/dfbe0ebfec95 http://www.importnew.com/19472.html http://traxexer.iteye.com/blog/1846344 http://www.broadview.com.cn/article/789]]></content>
      <categories>
        <category>java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[如何将github pages映射到域名上]]></title>
    <url>%2F2018%2F03%2F17%2F%E5%A6%82%E4%BD%95%E5%B0%86github%20pages%E6%98%A0%E5%B0%84%E5%88%B0%E8%87%AA%E5%B7%B1%E7%9A%84%E5%9F%9F%E5%90%8D%2F</url>
    <content type="text"><![CDATA[使用github pages和hexo可以轻松搭建一个属于自己的博客，但是这个博客的域名是一个github.io结尾的。看着不爽，下面讲一下具体的步骤。关于如何使用github pages和hexo搭建博客，网上有很多，这里就不介绍了。 将username.github.io解析到自己的域名上，首先需要将域名解析到github pages的ip上，然后在github的settings中添加一个CNAME文件。具体步骤如下 注册域名可以去腾讯云上边买一个域名，便宜点的就行。 解析域名域名其实就是一个ip的重命名而已，所以解析就相当于将域名绑定到一个ip，那么以后我们就可以直接用域名来访问那个ip对应的服务器的服务。 这里直接使用腾讯提供的快速解析方法,将github提供的两个ip中的一个加入就行 192.30.252.153 192.30.252.154 创建CNAME文件 我们打开在github中的博客目录中的settings，填入自己的域名，然后save。就可以在项目的根目录中看到一个CNAME的文件。这个文件就已经填入了我们的域名，但是当我们重新发布之后这个CNAME文件会被覆盖，所以需要source目录下创建同样的一个CNAME的文件。 最后访问自己的域名，就能直接访问到博客了。]]></content>
      <categories>
        <category>博客</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[从输入URL到页面加载完成的过程中都发生了什么事情？]]></title>
    <url>%2F2018%2F01%2F14%2F%E5%AE%8C%E6%95%B4%E6%8F%8F%E8%BF%B0%E4%B8%80%E6%AC%A1URL%E8%AE%BF%E9%97%AE%E5%88%B0%E9%A1%B5%E9%9D%A2%E5%8A%A0%E8%BD%BD%E5%88%B0%E5%BA%95%E9%83%BD%E7%BB%8F%E5%8E%86%E4%BA%86%E4%BA%9B%E4%BB%80%E4%B9%88%2F</url>
    <content type="text"><![CDATA[从输入URL到页面加载完成的过程中都发生了什么事情？ 平时我们总是习惯了输入一个网址，然后就静静的等待着各种漂亮的网页展示在我们的面前，但是可能大多数人都没有关注这其中都经历了哪些什么。下面大致做一个总结。 一、首先简单介绍一下OSI七层模型： 开放式系统互联通信参考模型（英语：Open System Interconnection Reference Model，缩写为 OSI），简称为OSI模型（OSI model），一种概念模型，由国际标准化组织（ISO）提出，一个试图使各种计算机在世界范围内互连为网络的标准框架。 其中各层的详细内容大家可以参考维基百科 OSI是一个理想的分层模型，很少有完全遵守这样的分层，只会使用其中的几层而已。 二、从访问google说起这里就不讨论我们伟大的防火长城了，作为一个程序员，这是基本技能 当你单击回车键，就意味着你需要向google的服务器发送一次网页请求的数据包。下面从涉及到的协议或者服务说起。 1、DNS协议 当我们要访问https://www.google.co.jp/时，必须要知道ip地址，但这个时候我们并不知道ip是多少。DNS服务就可以帮我们通过域名找到ip地址。它是一个存储着ip和域名映射的分布式数据库。GFW其中就有通过dns污染和dns劫持的方式使得我们无法访问到国外的某些“fd”网站。 这里需要注意的是，ip和域名并非一对一的关系。而是多对多，也就是说一个ip可以绑定多个域名，一个域名也可以绑定多个ip，但同一时刻只能是一对一的。 ok，现在我们有了ip地址。 2、 子网掩码 全世界这么多计算机，当然不可能都在同一个子网里。那这个时候就需要确认google服务器和我们本机是否在在同一个子网里，这里用到了子网掩码。 子网掩码和ip地址一样，都是32位的二进制数，通常用4段式表示，网络位全为1，主机位全为0。将我们的目标地址和本机地址分别与子网掩码按位做and操作。如果最终结果相同那么在同一子网中。不需要将数据包转发到网关。否则，将数据包转发到网关处理。 例如： DNS: 255.255.255.0 主机ip：10.20.0.121 google：216.239.34.10 明显不在同一个子网中 3、http/https协议 浏览器访问网页使用的是http协议，在OSI协议中属于应用层协议。 http请求的报文格式： 头部（request line + header） + 数据（data） 其中的请求行（request line）包括请求方法字段、URL字段和HTTP协议的版本三个字段组成 GET/index.html HTTP/1.1 目前大多数HTTP请求使用的是长连接（HTTP/1.1默认keep-alive为true），而长连接意味着，一个TCP的socket在当前请求结束后，如果没有新的请求到来，socket不会立马释放，而是等timeout后再释放。 请求方法字段有：GET、POST、HEAD、PUT、DELETE、TRACE, 这些字段在REST ful风格中能够被充分利用，而不在使用URL来表达目的动作。 类似下面的内容： 123456789101112131415alt-svc:hq=":443"; ma=2592000; quic=51303431; quic=51303339; quic=51303338; quic=51303337; quic=51303335,quic=":443"; ma=2592000; v="41,39,38,37,35"cache-control:private, max-age=0content-encoding:brcontent-type:text/html; charset=UTF-8date:Thu, 18 Jan 2018 10:37:50 GMTexpires:-1p3p:CP="This is not a P3P policy! See g.co/p3phelp for more info."server:gwsset-cookie:1P_JAR=2018-01-18-10; expires=Sat, 17-Feb-2018 10:37:50 GMT; path=/; domain=.google.co.jpset-cookie:NID=*, 20-Jul-2018 10:37:50 GMT; path=/; domain=.google.co.jp; HttpOnlystatus:200strict-transport-security:max-age=3600x-frame-options:SAMEORIGINx-xss-protection:1; mode=blockRequest Headers 4、tcp、udp协议 tcp协议是输入传输层的协议，主要作用是建立端口到端口的通信，同时保证数据的正确传输和差错处理。 同时和tcp同出于传输层协议的还有udp（用户数据报协议），这个协议比较简单，不保证数据的可靠性，数据一旦发出无法知道对方是否收到。但是优点是过程简单，节省资源。 无论是tcp还是udp协议，http的数据包都是加入到他们的数据段中。 三次握手问题：所谓三次握手（Three-Way Handshake）即建立TCP连接，就是指建立一个TCP连接时，需要客户端和服务端总共发送3个包以确认连接的建立。在socket编程中，这一过程由客户端执行connect来触发。 （1）第一次握手：Client将标志位SYN置为1，随机产生一个值seq=J，并将该数据包发送给Server，Client进入SYN_SENT状态，等待Server确认。 （2）第二次握手：Server收到数据包后由标志位SYN=1知道Client请求建立连接，Server将标志位SYN和ACK都置为1，ack=J+1，随机产生一个值seq=K，并将该数据包发送给Client以确认连接请求，Server进入SYN_RCVD状态。 （3）第三次握手：Client收到确认后，检查ack是否为J+1，ACK是否为1，如果正确则将标志位ACK置为1，ack=K+1，并将该数据包发送给Server，Server检查ack是否为K+1，ACK是否为1，如果正确则连接建立成功，Client和Server进入ESTABLISHED状态，完成三次握手，随后Client与Server之间可以开始传输数据了。 白话一点就是： 主机： 你好，我想认识你 服务器：好的，我也想认识你 主机：很高兴认识你 那为什么一定要三次握手而不是两次呢？ 我们假设有下面的这种情况： 当客户端发送一个请求，但是由于此时网络拥塞这个包暂时没有到达服务器， 客户端等待超时之后又重新发送一个请求，此时服务器端接收到了，并正确应答，双方开始通信，通信结束后释放socket连接。但这个时候，那个被拥塞的连接此时到达了服务器，如何tcp是采用两次握手的方式，那么此时服务器以为客户端又发起了一个连接请求，然后返回确认请求，此时连接就已经建立好了。服务器状态为ESTABLISHED，等待着客户端发送数据，但由于此时客户端进入了close状态，将导致服务器等待下去，浪费资源。 四次挥手： 首先需要知道的是tcp是全双工通信的，那么关闭就只能一个一个关闭。 当客户端和服务器需要关闭socket时（socket翻译成套接字并不恰当，把它当做“插座”也许更容易理解），具体的流程如下图所示： 假设这个时候client发起请求中断的请求，也就是发送Fin报文，告诉server：“我没有数据发给你了，但如果你还有数据要发给我，那就先不用关闭socket，发送完毕在告诉我”，此时client处于FIN_WAIT1。 当server接收到了之后发送ack给client：“你的请求我收到了，但是我还没有准备好，请继续等我消息”此时client进入FIN_WAIT2状态。 当server端确认数据发送完毕了，就告诉client：“好了，我这边数据发完了，准备好关闭连接了”。 但是这个时候client并不相信网络，怕server端不知道要关闭，所以进入TIME_WAIT,如果server端没有收到ack则可以重传，server端收到ack后就知道可以断开了，client等待2MSL(最大报文段生存时间)之后还没有收到回复，那么久认为server端正常关闭了，client也可以关闭了。 在这里需要注意最后client等待2MSL(最大报文段生存时间)，虽然按道理，四个报文都发送完毕，我们可以直接进入CLOSE状态了，但是我们必须假象网络是不可靠的，有可以最后一个ACK丢失。所以TIME_WAIT状态就是用来重发可能丢失的ACK报文。 5、IP协议，网络层 在七层网络分层中，网络层是最复杂的。负责主机到主机间的通信，其中ip协议是最重要的。 计算机若在同一个子网里边，完全可以只需要mac地址就可以通信，主机A想和主机B通信，那么只需要将请求广播到整个子网，然后各台计算机拿出目的mac地址和自己的mac地址相对比，如果相同，两台主机进行通信。但是计算机遍布全世界，不可能都在一个子网里边，若在其他子网里边怎么办呢？ 这个时候就必须用到ip地址了。由于ip地址分为两部分，一部分是网络段，一部分是主机段。网络段决定处于哪一个子网，主机段决定是子网的那一台机器。当我们要访问google的服务器ip是216.239.34.10时，由于不在同一个子网中，需要将数据报转发给网关，然后再通过复杂的路由，最终到达服务器。 ip报文将tcp的报文嵌入在数据块中 6、ARP协议、以太网协议 最后，ip数据包嵌入到以太网数据报，以太网数据包需要设置通信双方的mac地址，发送方为本机的mac地址，接收方网卡216.239.34.1的mac地址需要用ARP协议得到。 我们需要一种机制，能够从IP地址得到MAC地址。这里又可以分成两种情况。第一种情况，如果两台主机不在同一个子网络，那么事实上没有办法得到对方的MAC地址，只能把数据包传送到两个子网络连接处的”网关”（gateway），让网关去处理。 第二种情况，如果两台主机在同一个子网络，那么我们可以用ARP协议，得到对方的MAC地址。ARP协议也是发出一个数据包（包含在以太网数据包中），其中包含它所要查询主机的IP地址，在对方的MAC地址这一栏，填的是FF:FF:FF:FF:FF:FF，表示这是一个”广播”地址。它所在子网络的每一台主机，都会收到这个数据包，从中取出IP地址，与自身的IP地址进行比较。如果两者相同，都做出回复，向对方报告自己的MAC地址，否则就丢弃这个包。 总之，有了ARP协议之后，我们就可以得到同一个子网络内的主机MAC地址，可以把数据包发送到任意一台主机之上了。 7、服务器响应 经过多个网关的转发，Google的服务器216.239.34.10，收到数据包之后通过解析，取出其中的请求request line，做出响应，封装成response返回给浏览器。 8、浏览器解析渲染页面浏览器在收到HTML,CSS,JS文件后，它是如何把页面呈现到屏幕上的？下图对应的就是WebKit渲染的过程。 由于涉及到很多的内容，难免有偏差的地方，还请见谅。 参考： https://zh.wikipedia.org/wiki/OSI%E6%A8%A1%E5%9E%8B http://blog.csdn.net/whuslei/article/details/6667471 http://www.ruanyifeng.com/blog/2012/06/internet_protocol_suite_part_ii.html http://www.ruanyifeng.com/blog/2012/05/internet_protocol_suite_part_i.html]]></content>
      <categories>
        <category>网络</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[类unix平台中几个大杀器指令]]></title>
    <url>%2F2017%2F12%2F14%2F%E7%B1%BBunix%E5%B9%B3%E5%8F%B0%E4%B8%AD%E5%87%A0%E4%B8%AA%E5%A4%A7%E6%9D%80%E5%99%A8%E6%8C%87%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[类unix平台中几个大杀器指令 下边介绍几个“很强势”的命令行指令，能大大提高我们的工作生产效率。使用macOS Sierra作为演示环境。 1、autojump 通常我们在类unix操作系统中切换目录都是使用cd，使用这个指令如果要切换到很深的目录，是很烦恼的。那么这个时候autojump就是大杀器。 1、 在mac中安装autojump比较简单： 1brew install autojump 但是需要注意安装完并不能直接使用。在我们安装的过程中会有如下的提示： 我们只需要按照步骤操作就好：（1） 先在~/.zshrc文件或者~/.bash_profile中添加 1[ -f /usr/local/etc/profile.d/autojump.sh ] &amp;&amp; . /usr/local/etc/profile.d/autojump.sh (2) 然后source相应的文件，重新打开一个窗口。就可以使用了。 简单介绍一下autojump的简单使用可以使用下面的命令来手动添加一个目录 1$ autojump -a [目录] 如果你突然想要把当前目录变成你的最爱和使用最频繁的文件夹，你可以在该目录通过命令的参数 i 来手工增加它的权重 1$ autojump -i [权重] 这将使得该目录更可能被选择跳转。相反的例子是在该目录使用参数 d 来减少权重： 1autojump -d [权重] 要跟踪所有这些改变，输入： 1$ autojump -s 2、mycli 对于程序员来说，数据库再熟悉不过了，很多时候我们更喜欢用命令行来访问数据库，但是默认自带的mysql对操作并不是十分友好，不能自动补全，没有语法高亮。这个时候使用mycli就再合适不过了。 1、 先安装，依旧使用brew来安装管理1brew install mycli 然后使用mycli -u username进入数据库，需要注意的是这里不需要-p参数。 如下所示： 3、screen 当我们在通过ssh连接远端机器执行任务时，如果任务执行时间相对比较短还好，如果是执行hadoop的任务，很多时候回超过几个小时，那么这个时候如果ssh连接中断了，那么这个任务也会失败。所以这个时候screen就派上用场了。 1brew install screen 进入后台执行： 1screen 当开始执行任务之后使用如下指令退出： 1control + A + D 将正在执行的任务列举出来 1screen -ls 查看任务执行进度： 1screen -r 进程号 还包括axel、fasd、m-cli等等有兴趣的小伙伴可以自己研究。]]></content>
      <categories>
        <category>macOS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[记录spark一次踩坑]]></title>
    <url>%2F2017%2F11%2F27%2F%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1spark%E8%B8%A9%E5%9D%91%2F</url>
    <content type="text"><![CDATA[Application has been killed. Reason: Master removed our application: FAILED spark从五月份开始跑到11月份一直正常，但是在最近两天报如下的错误。 查看worker运行的log，发现下面的问题。 然后进入到/opt/soft/spark/work目录发现当前有31999个文件， 而在linux文件系统中一个文件夹最多能够32000个子文件，包括一个当前目录、父级目录，那么操作这个数就无法在这个目录中写入文件。在linux 内核中有下面的定义： 12include/linux/ext2_fs.h:#define EXT2_LINK_MAX 32000include/linux/ext3_fs.h:#define EXT3_LINK_MAX 32000 所以，只能将生成的多余临时文件删除了。动手ing…. 删除之后，重新提交spark任务，没有问题。]]></content>
      <categories>
        <category>spark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[如何解决“非WeChat官方网页，继续访问将转换成.....”的问题]]></title>
    <url>%2F2017%2F10%2F18%2F%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E2%80%9C%E9%9D%9EWeChat%E5%AE%98%E6%96%B9%E7%BD%91%E9%A1%B5%EF%BC%8C%E7%BB%A7%E7%BB%AD%E8%AE%BF%E9%97%AE%E5%B0%86%E8%BD%AC%E6%8D%A2%E6%88%90......%E2%80%9D%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[我们在开发微信公众号时，使用网页工具生成自定义菜单，关联到某一个网站，当我们访问这个网页的时候就会出现如下情况： 要解决上述问题，可以采用如下方式： 1、登录微信公众号，进入公众号设置 –&gt; 功能设置，进入到下面的页面中，然后点击设置。 然后最终进入到设置的页面。有两个地方需要特别注意：（1）首先要确保设置的域名必须是备过案的。（2）同时还需要下载MP_verify_zvHGyljL5rHLDOB1.txt文件到能够访问的目录。 假设是java web的项目，备案的url可以具体到访问的首页目录（WebContent目录下），那么也就可以将MP_verify_zvHGyljL5rHLDOB1.txt文件放置到WebContent目录下。 通过以上的设置，过一段时间生效之后，就不会在出现安全提示了。]]></content>
      <categories>
        <category>java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring MVC访问静态资源文件]]></title>
    <url>%2F2017%2F09%2F27%2FSpringMVC%E8%AE%BF%E9%97%AE%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[Spring MVC访问静态资源文件 我们在使用spring MVC开发时，需要在web.xml文件中配置DispatcherServlet，指定映射条件，如下所示： 1234567891011121314&lt;servlet&gt; &lt;servlet-name&gt;Dispatcher&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;!--加载配置文件--&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:/conf/spring-mvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;Dispatcher&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; 通常都需要将DispatcherServlet配置为拦截所有请求，所以将url-pattern配置为”/“。但这样就会出现一个问题，通常在页面中需要使用到的js、css、图片等等静态文件就无法请求了。 一般有用两种方式来处理这种情况1、使用 mvc:resources的方式，将配置添加到appContext-**.xml文件中例如： 12 &lt;!-- 对静态资源文件的访问 --&gt;&lt;mvc:resources mapping="/**" location="/" /&gt; mapping是url映射处理，location是相对于WebContent的路径。 2、 配置DispatcherServlet值拦截.do的请求。 1234567891011121314&lt;servlet&gt; &lt;servlet-name&gt;Dispatcher&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet &lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:configure/spring-mvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;Dispatcher&lt;/servlet-name&gt; &lt;url-pattern&gt;*.do&lt;/url-pattern&gt;&lt;/servlet-mapping&gt;]]></content>
      <categories>
        <category>java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[爬取猫眼电影数据（一）]]></title>
    <url>%2F2017%2F09%2F24%2F%E7%88%AC%E5%8F%96%E7%8C%AB%E7%9C%BC%E7%94%B5%E5%BD%B1%E6%95%B0%E6%8D%AE-1%2F</url>
    <content type="text"><![CDATA[爬取猫眼电影数据(一) 概述：近期由于业务需要，需要将爬取猫眼网站中的部分数据作为公众号数据源，猫眼的电影数据相当之全备，大概收录了几十万部电影。当然这些数据我们也没那么容易就能爬取到的。遂将整个爬取过程记录如下。 1、使用chrome进入开发者模式，观察html结构、元素，找到我们所需数据，第一步我们需要现获取每一个电影的详情页面url。 注意，在爬取猫眼的电影数据时并不需要登录和携带cookie去请求html 2、 这里我们结合BeautifulSoup和xpath来实现对数据的过滤。实现代码如下： 12345678910111213def get_detail_url(self, targetUrl): """从概览页面中去获取剧透影片的详情页面""" html = self.getHtml(targetUrl) print html soup = BeautifulSoup(html, 'lxml') urls = soup.find_all('a', attrs=&#123;'data-act':'movie-click'&#125;) dd = soup.find_all('dd') print dd for url in urls: detailPath = etree.HTML(str(url)) detail_url = detailPath.xpath(r'//a/@href') for durl in detail_url: self.getContents('https://maoyan.com' + durl, proxys) 然后获取到对应每一个电影的详情url，进入详情页，分析所需数据对应html元素。在这里我们只需要三个字段、电影名称、电影海报url、电影上映时间。 提示:使用xpath时，最简单的方式是在chrome中点击选中具体某一个元素，然后右键——&gt; copy –&gt; copy xpath就能获取到对应的xpath路径 实现代码： 12345678910def getContents(self, url, proxys): """从详情页面中获取所需data""" html = self.getHtml(url,proxys) detailPath = etree.HTML(html) #电影名称 film_name = detailPath.xpath(r'/html/body/div[3]/div/div[2]/div[1]/h3/text()') #图片url pic_url = detailPath.xpath(r'/html/body/div[3]/div/div[1]/div/img/@src') #上映时间 release_time = detailPath.xpath(r'/html/body/div[3]/div/div[2]/div[1]/ul/li[3]/text()') 3、 这个时候获取到数据了，就该存库了。嗯，就存到mysql中吧。没啥说的，上代码。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748def insertData(self, my_dict): try: self.db.set_character_set('utf8') cols = ', '.join(my_dict.keys()) print cols values = '","'.join(my_dict.values()) print values sql = "INSERT INTO maoyan (%s) VALUES (%s)" % (cols, '"' + values + '"') try: result = self.cur.execute(sql) insert_id = self.db.insert_id() self.db.commit() # 判断是否执行成功 if result: return insert_id else: return 0 except MySQLdb.Error, e: # 发生错误时回滚 self.db.rollback() # 主键唯一，无法插入 if "key 'PRIMARY'" in e.args[1]: print self.getCurrentTime(), "数据已存在，未插入数据" else: print self.getCurrentTime(), "插入数据失败，原因 %d: %s" % (e.args[0], e.args[1]) except MySQLdb.Error, e: print self.getCurrentTime(), "数据库错误，原因%d: %s" % (e.args[0], e.args[1]) def dealData(self, my_dict): """先从数据库中查询这个影片名的记录，如果没有直接插入； 若是数据库中已经存在数据，那么什么都不做""" self.db.set_character_set('utf8') filmTitile = my_dict.get('film_title') result = self.findFilmByfilmTitle(filmTitile) if len(result) == 0: self.insertData(my_dict) return else: pass def findFilmByfilmTitle(self, filmTitile): """通过影片名称去数据库中查询""" self.db.set_character_set('utf8') sql = "select * from maoyan where film_title='%s'" % (filmTitile) self.cur.execute(sql) results = self.cur.fetchall() self.db.commit() return results 4、 万事俱备只欠启动了，跑啊跑啊。想必大家都想到了，猫眼肯定是不可能随便让人这样爬取他们的数据，而且短时间大量的请求对服务器的负载也是相当大的，不出意料爬取了不到300条数据，就被停止了。(ಥ _ ಥ)处理这种情况一般有两种方式，一种通过延缓请求，一种是通过代理ip的方式。第一种我尝试延缓5、10s，但并没有用。因而尝试用第二种方式，代理ip池的方式。代理ip如何使用在下一篇文章中记录。 下面是本次的完成代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133#!/usr/bin/env python# coding: utf-8import urllib2import urllibimport requestsfrom bs4 import BeautifulSoupfrom lxml import etreeimport MySQLdbimport timeimport randomimport sysreload(sys)sys.setdefaultencoding('utf8')"""猫眼不需要http请求的头部信息，添加上反而会出错"""class Maoyan(): # 获取当前时间 def getCurrentTime(self): return time.strftime('[%Y-%m-%d %H:%M:%S]', time.localtime(time.time())) def __init__(self): self.time = time self.base_url = "https://maoyan.com/films?showType=1&amp;sortId=2" try: self.db = MySQLdb.connect('localhost', 'flyer_user', 'Tu4a0X9hOPKz6jS!e', 'flyer_db',charset='utf8') self.cur = self.db.cursor() except MySQLdb.Error, e: print self.getCurrentTime(), "连接数据库错误，原因%d: %s" % (e.args[0], e.args[1]) # 插入数据 def insertData(self, my_dict): try: self.db.set_character_set('utf8') cols = ', '.join(my_dict.keys()) print cols values = '","'.join(my_dict.values()) print values sql = "INSERT INTO maoyan (%s) VALUES (%s)" % (cols, '"' + values + '"') try: result = self.cur.execute(sql) insert_id = self.db.insert_id() self.db.commit() # 判断是否执行成功 if result: return insert_id else: return 0 except MySQLdb.Error, e: # 发生错误时回滚 self.db.rollback() # 主键唯一，无法插入 if "key 'PRIMARY'" in e.args[1]: print self.getCurrentTime(), "数据已存在，未插入数据" else: print self.getCurrentTime(), "插入数据失败，原因 %d: %s" % (e.args[0], e.args[1]) except MySQLdb.Error, e: print self.getCurrentTime(), "数据库错误，原因%d: %s" % (e.args[0], e.args[1]) def dealData(self, my_dict): """先从数据库中查询这个影片名的记录，如果没有直接插入； 若是数据库中已经存在数据，那么什么都不做""" self.db.set_character_set('utf8') filmTitile = my_dict.get('film_title') result = self.findFilmByfilmTitle(filmTitile) if len(result) == 0: self.insertData(my_dict) return else: pass def findFilmByfilmTitle(self, filmTitile): """通过影片名称去数据库中查询""" self.db.set_character_set('utf8') sql = "select * from maoyan where film_title='%s'" % (filmTitile) self.cur.execute(sql) results = self.cur.fetchall() self.db.commit() return results def getHtml(self,url,proxys): """获取html""" req = urllib2.Request(url) html = urllib2.urlopen(req).read().decode('utf-8') return html def get_detail_url(self, targetUrl, proxys): """从概览页面中去获取剧透影片的详情页面""" html = self.getHtml(targetUrl, proxys) print html soup = BeautifulSoup(html, 'lxml') urls = soup.find_all('a', attrs=&#123;'data-act':'movie-click'&#125;) dd = soup.find_all('dd') print dd for url in urls: detailPath = etree.HTML(str(url)) detail_url = detailPath.xpath(r'//a/@href') for durl in detail_url: self.getContents('https://maoyan.com' + durl, proxys) def getContents(self, url, proxys): """从详情页面中获取所需data""" tmp1 = [] tmp2 = [] html = self.getHtml(url,proxys) detailPath = etree.HTML(html) film_name = detailPath.xpath(r'/html/body/div[3]/div/div[2]/div[1]/h3/text()') for name in film_name: tmp1.append("film_title") tmp2.append(name) pic_url = detailPath.xpath(r'/html/body/div[3]/div/div[1]/div/img/@src') for url in pic_url: strs = url.split('@') tmp1.append("pic_url") tmp2.append(strs[0]) release_time = detailPath.xpath(r'/html/body/div[3]/div/div[2]/div[1]/ul/li[3]/text()') for time in release_time: tmp1.append("release_time") tmp2.append(time[:10]) #将两个列表转换为字典，方便插入数据库 result_dict = dict(zip(tmp1, tmp2)) self.dealData(result_dict) def main(self): self.get_detail_url(self.base_url, proxys) for i in range(50): targetUrl = self.base_url + '&amp;offset=' + str((i+1)*30) self.get_detail_url(targetUrl, proxys)if __name__ == '__main__': maoyan = Maoyan() maoyan.main()]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python爬虫如何获取到只需要的数据]]></title>
    <url>%2F2017%2F09%2F18%2Fpython%E7%88%AC%E8%99%AB%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96%E5%88%B0%E5%8F%AA%E9%9C%80%E8%A6%81%E7%9A%84%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[python爬虫如何获取到只需要的数据 概述：在爬虫的学习过程中，最麻烦的也许就是如何从一大堆html标签中过滤出我们所需要的，在这里有三种方法：re正则表达式、BeautifulSoup、使用lxml中的xpath。 1 、先介绍如何使用lxml中的xpath(1) 我们需要安装相应的依赖，然后在项目中导入相应的模块 12from lxml import etreeimport lxml (2) 之后再代码中转化成xpath12detailPath = etree.HTML(html) questions_titile = detailPath.xpath('//ul[@class="list-group"]/li/div/div[@class="question-title"]/a/text()') 注意上边的两个单引号之间的一串，前面两个’//‘是表示从这里开始，然后一级一级的去寻找所需要的元素，如果想获取属性值，例如是图片的url，最后可以使用@href，如果想获取里边的文本值，那就使用text()。 (3) 特殊用法12345678910from lxml import etreehtml=&quot;&quot;&quot; &lt;body&gt; &lt;div id=&quot;aa&quot;&gt;aa&lt;/div&gt; &lt;div id=&quot;ab&quot;&gt;ab&lt;/div&gt; &lt;div id=&quot;ac&quot;&gt;ac&lt;/div&gt; &lt;/body&gt; &quot;&quot;&quot;selector=etree.HTML(html)content=selector.xpath(&apos;//div[starts-with(@id,&quot;a&quot;)]/text()&apos;) #这里使用starts-with方法提取div的id标签属性值开头为a的div标签 这篇文章有简单应用：http://blog.csdn.net/winterto1990/article/details/47903653介绍 最后一个大绝招：直接在chrome中开发者模式，右键找到你想寻找的元素标签，copy xpath就能将这个元素对应的xpath找到！ 1 、python中提供的re模块 正则表达式是一种更为强大的字符串匹配、字符串查找、字符串替换等操作工具。上篇讲解了正则表达式的基本概念和语法以及re模块的基本使用方式，这节来详细说说 re 模块作为 Python 正则表达式引擎提供了哪些便利性操作。正则表达式的所有操作都是围绕着匹配对象(Match)进行的，只有表达式与字符串匹配才有可能进行后续操作。判断匹配与否有两个方法，分别是 re.match() 和 re.search()，两者有什么区别呢？ 区别：match 方法从字符串的起始位置开始检查，如果刚好有一个子字符串与正则表达式相匹配，则返回一个Match对象，只要起始位置不匹配则退出，不再往后检查了，返回 None 1234&gt;&gt;&gt; re.match(r&quot;b.r&quot;, &quot;foobar&quot;) # 不匹配&gt;&gt;&gt; re.match(r&quot;b.r&quot;, &quot;barfoo&quot;) # 匹配&lt;_sre.SRE_Match object at 0x102f05b28&gt;&gt;&gt;&gt; search 方法虽然也是从起始位置开始检查，但是它在起始位置不匹配的时候会一直尝试往后检查，直到匹配为止，如果到字符串的末尾还没有匹配，则返回 None123&gt;&gt;&gt; re.search(r&quot;b.r&quot;, &quot;foobar&quot;) # 匹配&lt;_sre.SRE_Match object at 0x000000000254D578&gt;&gt;&gt;&gt; re.match(r&quot;b.r&quot;, &quot;foobr&quot;) # 不匹配 但是二者都是匹配到就停止匹配了，哪怕还有更多能匹配的也不管。 参考：https://foofish.net/re-tutorial.html 、https://foofish.net/crawler-re-second.html]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[上传本地jar包到nexus仓库中]]></title>
    <url>%2F2017%2F09%2F16%2F%E4%B8%8A%E4%BC%A0%E6%9C%AC%E5%9C%B0jar%E5%8C%85%E5%88%B0nexus%E4%BB%93%E5%BA%93%E4%B8%AD%2F</url>
    <content type="text"><![CDATA[#上传本地jar包到nexus仓库中 概述：有时在我们的nexus仓库中没有项目中所需的jar包，这个时候就需要从网络中下载，然后上传到nexus仓库中，然后我们就可以嗨森的敲代码啦。 1、首先需要是对应的repository有上传的权限 2、我们这里讲jar包上传到3rd party仓库中。先点击artifact upload然后选择GAV definition，手动将group id、artifact、version填入，然后packaging选择jar。 3、然后点击select artifact（s） to upload将本地的jar加入，点击add artifact，然后点击upload artifact。 4、最后刷新就能看到刚才加入的jar包。 此外需要注意的是，将入到3rd仓库之后可能对应的坐标会发生变化。如果前后坐标没有对应上，那么在pom文件中还是找不到相应的依赖。]]></content>
      <categories>
        <category>java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[hive如何实现自定义函数]]></title>
    <url>%2F2017%2F09%2F14%2Fhive%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[hive如何实现自定义函数 概述：我们在使用hive的时候，很多时候执行复杂的sql时，hive提供的函数无法满足我们的需求。这个时候就需要我们自己去定义一个函数。幸好，hive为我们允许使用java来子线自定义函数。 1、实现自定义UDF函数必须满足两个条件。 （1） 必须是org.apache.Hadoop.hive.ql.exec.UDF的子类 （2） 必须实现evaluate函数。 2、实现方法具体如下： （1）将$HIVE_HOME/lib中的两个jar包，分别是hive-contrib-2.1.1.jar和hive-exec-2.1.1.jar，把两个jar包放置到java的目录下，然后新建一个类，继承UDF。 123456789101112131415161718package org.apache.hadoop.hive.contrib.udf.example;import org.apache.commons.lang.StringUtils;import org.apache.hadoop.hive.ql.exec.UDF;import java.util.Arrays;public class ReturnMin extends UDF&#123; public static Integer evaluate(String str, String separator) &#123; int result = 100; if(StringUtils.isEmpty(str))&#123; return result; &#125; String[] strArray = str.split(separator); int[] intArray = new int[strArray.length]; for(int i = 0 ; i &lt; strArray.length ; i++) &#123; intArray[i] = Integer.parseInt(strArray[i]); &#125; Arrays.sort(intArray); return intArray[0]; &#125; (2) 然后将ReturnMin.class文件拷贝到hive-contrib-2.1.1.jar\org\apache\hadoop\hive\contrib\udf\example目录中，然后替换hive select在hive的cli中执行：CREATE FUNCTION ReturnMin AS ‘org.apache.hadoop.hive.contrib.udf.example.ReturnMin’;然后重新进入cli中，给这个函数传递一个数字的字符串和分隔符就能够将这个字符串最小的值返回。 hive删除一个自定义函数： drop function cutString;]]></content>
      <categories>
        <category>hive</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[hive中sql总结]]></title>
    <url>%2F2017%2F09%2F14%2Fhive%E4%B8%ADsql%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[hive中sql总结1、对指定的某一个列去重1insert into table allrecord partition (year=2017, month=08, day=25) select t.id, t.tssend, t.apmac, t.mac, t.rssi from (select id, tssend, apmac ,mac, rssi, row_number() over(distribute by mac sort by tssend )as rn from allrecords) t where t.rn=1; 上面试对字段mac去重，然后对tssend排序后插入allrecord这张表 2、删除分区1ALTER TABLE macinfo_2017 DROP IF EXISTS PARTITION (year=2017, month=03, day=22); 3、删除一张表，如果启动了回收站功能，那么这张表会保存在回收站里，也是能够恢复信息的。1drop table tablename; 若是表创建为外部表，则只会删除相应的原信息，对数据没有影响。 4、 创建一个partition_table002的表，使得表结构和partition_table001一样。1create table if not exists partition_table002 like partition_table001; 5、使用load加载数据到表中1load data local inpath 't_student1.txt' overwrite inte table t_student1; 6、创建外部表create EXTERNAL table IF NOT EXISTS macinfo_20170221( tssend string COMMENT 'Probe upload data time', apmac string COMMENT 'Probe MAC address', mac string COMMENT 'Mobile device MAC address', rssi string COMMENT 'WiFi signal strength of mobile devices') partitioned by (year int, month int, day int) ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe' WITH SERDEPROPERTIES ( "mac"="$.mac", "apmac"="$.apmac", "rssi"="$.rssi", "tssend"="$.tssend" ) STORED AS TEXTFILE LOCATION '/opt/local/software/a.txt'; 注意：这里创建外部表时会有坑，主要是数据存储格式的问题，我的数据存储为json格式，而hive并不直接支持读取json格式数据，需要添加相应的jar包才能解析数据。 7、hive外部表创建分区关联相应的目录（必须是目录）ALTER TABLE macinfo_2017 ADD PARTITION (year = 2017, month = 02, day = 21); 未完待续。。。。]]></content>
      <categories>
        <category>hive</category>
      </categories>
  </entry>
</search>
