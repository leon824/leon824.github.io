<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[从输入URL到页面加载完成的过程中都发生了什么事情？]]></title>
    <url>%2F2018%2F01%2F14%2F%E5%AE%8C%E6%95%B4%E6%8F%8F%E8%BF%B0%E4%B8%80%E6%AC%A1URL%E8%AE%BF%E9%97%AE%E5%88%B0%E9%A1%B5%E9%9D%A2%E5%8A%A0%E8%BD%BD%E5%88%B0%E5%BA%95%E9%83%BD%E7%BB%8F%E5%8E%86%E4%BA%86%E4%BA%9B%E4%BB%80%E4%B9%88%2F</url>
    <content type="text"><![CDATA[从输入URL到页面加载完成的过程中都发生了什么事情？ 平时我们总是习惯了输入一个网址，然后就静静的等待着各种漂亮的网页展示在我们的面前，但是可能大多数人都没有关注这其中都经历了哪些什么。下面大致做一个总结。 一、首先简单介绍一下OSI七层模型： 开放式系统互联通信参考模型（英语：Open System Interconnection Reference Model，缩写为 OSI），简称为OSI模型（OSI model），一种概念模型，由国际标准化组织（ISO）提出，一个试图使各种计算机在世界范围内互连为网络的标准框架。 其中各层的详细内容大家可以参考维基百科 OSI是一个理想的分层模型，很少有完全遵守这样的分层，只会使用其中的几层而已。 二、从访问google说起这里就不讨论我们伟大的防火长城了，作为一个程序员，这是基本技能 当你单击回车键，就意味着你需要向google的服务器发送一次网页请求的数据包。下面从涉及到的协议或者服务说起。 1、DNS协议 当我们要访问https://www.google.co.jp/时，必须要知道ip地址，但这个时候我们并不知道ip是多少。DNS服务就可以帮我们通过域名找到ip地址。它是一个存储着ip和域名映射的分布式数据库。GFW其中就有通过dns污染和dns劫持的方式使得我们无法访问到国外的某些“fd”网站。 这里需要注意的是，ip和域名并非一对一的关系。而是多对多，也就是说一个ip可以绑定多个域名，一个域名也可以绑定多个ip，但同一时刻只能是一对一的。 ok，现在我们有了ip地址。 2、 子网掩码 全世界这么多计算机，当然不可能都在同一个子网里。那这个时候就需要确认google服务器和我们本机是否在在同一个子网里，这里用到了子网掩码。 子网掩码和ip地址一样，都是32位的二进制数，通常用4段式表示，网络位全为1，主机位全为0。将我们的目标地址和本机地址分别与子网掩码按位做and操作。如果最终结果相同那么在同一子网中。不需要将数据包转发到网关。否则，将数据包转发到网关处理。 例如： DNS: 255.255.255.0 主机ip：10.20.0.121 google：216.239.34.10 明显不在同一个子网中 3、http/https协议 浏览器访问网页使用的是http协议，在OSI协议中属于应用层协议。 http请求的报文格式： 头部（request line + header） + 数据（data） 其中的请求行（request line）包括请求方法字段、URL字段和HTTP协议的版本三个字段组成 GET/index.html HTTP/1.1 目前大多数HTTP请求使用的是长连接（HTTP/1.1默认keep-alive为true），而长连接意味着，一个TCP的socket在当前请求结束后，如果没有新的请求到来，socket不会立马释放，而是等timeout后再释放。 请求方法字段有：GET、POST、HEAD、PUT、DELETE、TRACE, 这些字段在REST ful风格中能够被充分利用，而不在使用URL来表达目的动作。 类似下面的内容： 123456789101112131415alt-svc:hq=":443"; ma=2592000; quic=51303431; quic=51303339; quic=51303338; quic=51303337; quic=51303335,quic=":443"; ma=2592000; v="41,39,38,37,35"cache-control:private, max-age=0content-encoding:brcontent-type:text/html; charset=UTF-8date:Thu, 18 Jan 2018 10:37:50 GMTexpires:-1p3p:CP="This is not a P3P policy! See g.co/p3phelp for more info."server:gwsset-cookie:1P_JAR=2018-01-18-10; expires=Sat, 17-Feb-2018 10:37:50 GMT; path=/; domain=.google.co.jpset-cookie:NID=*, 20-Jul-2018 10:37:50 GMT; path=/; domain=.google.co.jp; HttpOnlystatus:200strict-transport-security:max-age=3600x-frame-options:SAMEORIGINx-xss-protection:1; mode=blockRequest Headers 4、tcp、udp协议 tcp协议是输入传输层的协议，主要作用是建立端口到端口的通信，同时保证数据的正确传输和差错处理。 同时和tcp同出于传输层协议的还有udp（用户数据报协议），这个协议比较简单，不保证数据的可靠性，数据一旦发出无法知道对方是否收到。但是优点是过程简单，节省资源。 无论是tcp还是udp协议，http的数据包都是加入到他们的数据段中。 三次握手问题：所谓三次握手（Three-Way Handshake）即建立TCP连接，就是指建立一个TCP连接时，需要客户端和服务端总共发送3个包以确认连接的建立。在socket编程中，这一过程由客户端执行connect来触发。 （1）第一次握手：Client将标志位SYN置为1，随机产生一个值seq=J，并将该数据包发送给Server，Client进入SYN_SENT状态，等待Server确认。 （2）第二次握手：Server收到数据包后由标志位SYN=1知道Client请求建立连接，Server将标志位SYN和ACK都置为1，ack=J+1，随机产生一个值seq=K，并将该数据包发送给Client以确认连接请求，Server进入SYN_RCVD状态。 （3）第三次握手：Client收到确认后，检查ack是否为J+1，ACK是否为1，如果正确则将标志位ACK置为1，ack=K+1，并将该数据包发送给Server，Server检查ack是否为K+1，ACK是否为1，如果正确则连接建立成功，Client和Server进入ESTABLISHED状态，完成三次握手，随后Client与Server之间可以开始传输数据了。 白话一点就是： 主机： 你好，我想认识你 服务器：好的，我也想认识你 主机：很高兴认识你 那为什么一定要三次握手而不是两次呢？ 我们假设有下面的这种情况： 当客户端发送一个请求，但是由于此时网络拥塞这个包暂时没有到达服务器， 客户端等待超时之后又重新发送一个请求，此时服务器端接收到了，并正确应答，双方开始通信，通信结束后释放socket连接。但这个时候，那个被拥塞的连接此时到达了服务器，如何tcp是采用两次握手的方式，那么此时服务器以为客户端又发起了一个连接请求，然后返回确认请求，此时连接就已经建立好了。服务器状态为ESTABLISHED，等待着客户端发送数据，但由于此时客户端进入了close状态，将导致服务器等待下去，浪费资源。 四次挥手： 首先需要知道的是tcp是全双工通信的，那么关闭就只能一个一个关闭。 当客户端和服务器需要关闭socket时（socket翻译成套接字并不恰当，把它当做“插座”也许更容易理解），具体的流程如下图所示： 假设这个时候client发起请求中断的请求，也就是发送Fin报文，告诉server：“我没有数据发给你了，但如果你还有数据要发给我，那就先不用关闭socket，发送完毕在告诉我”，此时client处于FIN_WAIT1。 当server接收到了之后发送ack给client：“你的请求我收到了，但是我还没有准备好，请继续等我消息”此时client进入FIN_WAIT2状态。 当server端确认数据发送完毕了，就告诉client：“好了，我这边数据发完了，准备好关闭连接了”。 但是这个时候client并不相信网络，怕server端不知道要关闭，所以进入TIME_WAIT,如果server端没有收到ack则可以重传，server端收到ack后就知道可以断开了，client等待2MSL(最大报文段生存时间)之后还没有收到回复，那么久认为server端正常关闭了，client也可以关闭了。 在这里需要注意最后client等待2MSL(最大报文段生存时间)，虽然按道理，四个报文都发送完毕，我们可以直接进入CLOSE状态了，但是我们必须假象网络是不可靠的，有可以最后一个ACK丢失。所以TIME_WAIT状态就是用来重发可能丢失的ACK报文。 5、IP协议，网络层 在七层网络分层中，网络层是最复杂的。负责主机到主机间的通信，其中ip协议是最重要的。 计算机若在同一个子网里边，完全可以只需要mac地址就可以通信，主机A想和主机B通信，那么只需要将请求广播到整个子网，然后各台计算机拿出目的mac地址和自己的mac地址相对比，如果相同，两台主机进行通信。但是计算机遍布全世界，不可能都在一个子网里边，若在其他子网里边怎么办呢？ 这个时候就必须用到ip地址了。由于ip地址分为两部分，一部分是网络段，一部分是主机段。网络段决定处于哪一个子网，主机段决定是子网的那一台机器。当我们要访问google的服务器ip是216.239.34.10时，由于不在同一个子网中，需要将数据报转发给网关，然后再通过复杂的路由，最终到达服务器。 ip报文将tcp的报文嵌入在数据块中 6、ARP协议、以太网协议 最后，ip数据包嵌入到以太网数据报，以太网数据包需要设置通信双方的mac地址，发送方为本机的mac地址，接收方网卡216.239.34.1的mac地址需要用ARP协议得到。 我们需要一种机制，能够从IP地址得到MAC地址。这里又可以分成两种情况。第一种情况，如果两台主机不在同一个子网络，那么事实上没有办法得到对方的MAC地址，只能把数据包传送到两个子网络连接处的”网关”（gateway），让网关去处理。 第二种情况，如果两台主机在同一个子网络，那么我们可以用ARP协议，得到对方的MAC地址。ARP协议也是发出一个数据包（包含在以太网数据包中），其中包含它所要查询主机的IP地址，在对方的MAC地址这一栏，填的是FF:FF:FF:FF:FF:FF，表示这是一个”广播”地址。它所在子网络的每一台主机，都会收到这个数据包，从中取出IP地址，与自身的IP地址进行比较。如果两者相同，都做出回复，向对方报告自己的MAC地址，否则就丢弃这个包。 总之，有了ARP协议之后，我们就可以得到同一个子网络内的主机MAC地址，可以把数据包发送到任意一台主机之上了。 7、服务器响应 经过多个网关的转发，Google的服务器216.239.34.10，收到数据包之后通过解析，取出其中的请求request line，做出响应，封装成response返回给浏览器。 8、浏览器解析渲染页面浏览器在收到HTML,CSS,JS文件后，它是如何把页面呈现到屏幕上的？下图对应的就是WebKit渲染的过程。 由于涉及到很多的内容，难免有偏差的地方，还请见谅。 参考： https://zh.wikipedia.org/wiki/OSI%E6%A8%A1%E5%9E%8B http://blog.csdn.net/whuslei/article/details/6667471 http://www.ruanyifeng.com/blog/2012/06/internet_protocol_suite_part_ii.html http://www.ruanyifeng.com/blog/2012/05/internet_protocol_suite_part_i.html]]></content>
      <categories>
        <category>网络</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[类unix平台中几个大杀器指令]]></title>
    <url>%2F2017%2F12%2F14%2F%E7%B1%BBunix%E5%B9%B3%E5%8F%B0%E4%B8%AD%E5%87%A0%E4%B8%AA%E5%A4%A7%E6%9D%80%E5%99%A8%E6%8C%87%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[类unix平台中几个大杀器指令 下边介绍几个“很强势”的命令行指令，能大大提高我们的工作生产效率。使用macOS Sierra作为演示环境。 1、autojump 通常我们在类unix操作系统中切换目录都是使用cd，使用这个指令如果要切换到很深的目录，是很烦恼的。那么这个时候autojump就是大杀器。 1、 在mac中安装autojump比较简单： 1brew install autojump 但是需要注意安装完并不能直接使用。在我们安装的过程中会有如下的提示： 我们只需要按照步骤操作就好：（1） 先在~/.zshrc文件或者~/.bash_profile中添加 1[ -f /usr/local/etc/profile.d/autojump.sh ] &amp;&amp; . /usr/local/etc/profile.d/autojump.sh (2) 然后source相应的文件，重新打开一个窗口。就可以使用了。 简单介绍一下autojump的简单使用可以使用下面的命令来手动添加一个目录 1$ autojump -a [目录] 如果你突然想要把当前目录变成你的最爱和使用最频繁的文件夹，你可以在该目录通过命令的参数 i 来手工增加它的权重 1$ autojump -i [权重] 这将使得该目录更可能被选择跳转。相反的例子是在该目录使用参数 d 来减少权重： 1autojump -d [权重] 要跟踪所有这些改变，输入： 1$ autojump -s 2、mycli 对于程序员来说，数据库再熟悉不过了，很多时候我们更喜欢用命令行来访问数据库，但是默认自带的mysql对操作并不是十分友好，不能自动补全，没有语法高亮。这个时候使用mycli就再合适不过了。 1、 先安装，依旧使用brew来安装管理1brew install mycli 然后使用mycli -u username进入数据库，需要注意的是这里不需要-p参数。 如下所示： 3、screen 当我们在通过ssh连接远端机器执行任务时，如果任务执行时间相对比较短还好，如果是执行hadoop的任务，很多时候回超过几个小时，那么这个时候如果ssh连接中断了，那么这个任务也会失败。所以这个时候screen就派上用场了。 1brew install screen 进入后台执行： 1screen 当开始执行任务之后使用如下指令退出： 1control + A + D 将正在执行的任务列举出来 1screen -ls 查看任务执行进度： 1screen -r 进程号 还包括axel、fasd、m-cli等等有兴趣的小伙伴可以自己研究。]]></content>
      <categories>
        <category>macOS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[记录spark一次踩坑]]></title>
    <url>%2F2017%2F11%2F27%2F%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1spark%E8%B8%A9%E5%9D%91%2F</url>
    <content type="text"><![CDATA[Application has been killed. Reason: Master removed our application: FAILED spark从五月份开始跑到11月份一直正常，但是在最近两天报如下的错误。 查看worker运行的log，发现下面的问题。 然后进入到/opt/soft/spark/work目录发现当前有31999个文件， 而在linux文件系统中一个文件夹最多能够32000个子文件，包括一个当前目录、父级目录，那么操作这个数就无法在这个目录中写入文件。在linux 内核中有下面的定义： 12include/linux/ext2_fs.h:#define EXT2_LINK_MAX 32000include/linux/ext3_fs.h:#define EXT3_LINK_MAX 32000 所以，只能将生成的多余临时文件删除了。动手ing…. 删除之后，重新提交spark任务，没有问题。]]></content>
      <categories>
        <category>spark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[如何解决“非WeChat官方网页，继续访问将转换成.....”的问题]]></title>
    <url>%2F2017%2F10%2F18%2F%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E2%80%9C%E9%9D%9EWeChat%E5%AE%98%E6%96%B9%E7%BD%91%E9%A1%B5%EF%BC%8C%E7%BB%A7%E7%BB%AD%E8%AE%BF%E9%97%AE%E5%B0%86%E8%BD%AC%E6%8D%A2%E6%88%90......%E2%80%9D%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[我们在开发微信公众号时，使用网页工具生成自定义菜单，关联到某一个网站，当我们访问这个网页的时候就会出现如下情况： 要解决上述问题，可以采用如下方式： 1、登录微信公众号，进入公众号设置 –&gt; 功能设置，进入到下面的页面中，然后点击设置。 然后最终进入到设置的页面。有两个地方需要特别注意：（1）首先要确保设置的域名必须是备过案的。（2）同时还需要下载MP_verify_zvHGyljL5rHLDOB1.txt文件到能够访问的目录。 假设是java web的项目，备案的url可以具体到访问的首页目录（WebContent目录下），那么也就可以将MP_verify_zvHGyljL5rHLDOB1.txt文件放置到WebContent目录下。 通过以上的设置，过一段时间生效之后，就不会在出现安全提示了。]]></content>
      <categories>
        <category>java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring MVC访问静态资源文件]]></title>
    <url>%2F2017%2F09%2F27%2FSpringMVC%E8%AE%BF%E9%97%AE%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[Spring MVC访问静态资源文件 我们在使用spring MVC开发时，需要在web.xml文件中配置DispatcherServlet，指定映射条件，如下所示： 1234567891011121314&lt;servlet&gt; &lt;servlet-name&gt;Dispatcher&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;!--加载配置文件--&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:/conf/spring-mvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;Dispatcher&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; 通常都需要将DispatcherServlet配置为拦截所有请求，所以将url-pattern配置为”/“。但这样就会出现一个问题，通常在页面中需要使用到的js、css、图片等等静态文件就无法请求了。 一般有用两种方式来处理这种情况1、使用 mvc:resources的方式，将配置添加到appContext-**.xml文件中例如： 12 &lt;!-- 对静态资源文件的访问 --&gt;&lt;mvc:resources mapping="/**" location="/" /&gt; mapping是url映射处理，location是相对于WebContent的路径。 2、 配置DispatcherServlet值拦截.do的请求。 1234567891011121314&lt;servlet&gt; &lt;servlet-name&gt;Dispatcher&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet &lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:configure/spring-mvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;Dispatcher&lt;/servlet-name&gt; &lt;url-pattern&gt;*.do&lt;/url-pattern&gt;&lt;/servlet-mapping&gt;]]></content>
      <categories>
        <category>java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[爬取猫眼电影数据（一）]]></title>
    <url>%2F2017%2F09%2F24%2F%E7%88%AC%E5%8F%96%E7%8C%AB%E7%9C%BC%E7%94%B5%E5%BD%B1%E6%95%B0%E6%8D%AE-1%2F</url>
    <content type="text"><![CDATA[爬取猫眼电影数据(一) 概述：近期由于业务需要，需要将爬取猫眼网站中的部分数据作为公众号数据源，猫眼的电影数据相当之全备，大概收录了几十万部电影。当然这些数据我们也没那么容易就能爬取到的。遂将整个爬取过程记录如下。 1、使用chrome进入开发者模式，观察html结构、元素，找到我们所需数据，第一步我们需要现获取每一个电影的详情页面url。 注意，在爬取猫眼的电影数据时并不需要登录和携带cookie去请求html 2、 这里我们结合BeautifulSoup和xpath来实现对数据的过滤。实现代码如下： 12345678910111213def get_detail_url(self, targetUrl): """从概览页面中去获取剧透影片的详情页面""" html = self.getHtml(targetUrl) print html soup = BeautifulSoup(html, 'lxml') urls = soup.find_all('a', attrs=&#123;'data-act':'movie-click'&#125;) dd = soup.find_all('dd') print dd for url in urls: detailPath = etree.HTML(str(url)) detail_url = detailPath.xpath(r'//a/@href') for durl in detail_url: self.getContents('https://maoyan.com' + durl, proxys) 然后获取到对应每一个电影的详情url，进入详情页，分析所需数据对应html元素。在这里我们只需要三个字段、电影名称、电影海报url、电影上映时间。 提示:使用xpath时，最简单的方式是在chrome中点击选中具体某一个元素，然后右键——&gt; copy –&gt; copy xpath就能获取到对应的xpath路径 实现代码： 12345678910def getContents(self, url, proxys): """从详情页面中获取所需data""" html = self.getHtml(url,proxys) detailPath = etree.HTML(html) #电影名称 film_name = detailPath.xpath(r'/html/body/div[3]/div/div[2]/div[1]/h3/text()') #图片url pic_url = detailPath.xpath(r'/html/body/div[3]/div/div[1]/div/img/@src') #上映时间 release_time = detailPath.xpath(r'/html/body/div[3]/div/div[2]/div[1]/ul/li[3]/text()') 3、 这个时候获取到数据了，就该存库了。嗯，就存到mysql中吧。没啥说的，上代码。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748def insertData(self, my_dict): try: self.db.set_character_set('utf8') cols = ', '.join(my_dict.keys()) print cols values = '","'.join(my_dict.values()) print values sql = "INSERT INTO maoyan (%s) VALUES (%s)" % (cols, '"' + values + '"') try: result = self.cur.execute(sql) insert_id = self.db.insert_id() self.db.commit() # 判断是否执行成功 if result: return insert_id else: return 0 except MySQLdb.Error, e: # 发生错误时回滚 self.db.rollback() # 主键唯一，无法插入 if "key 'PRIMARY'" in e.args[1]: print self.getCurrentTime(), "数据已存在，未插入数据" else: print self.getCurrentTime(), "插入数据失败，原因 %d: %s" % (e.args[0], e.args[1]) except MySQLdb.Error, e: print self.getCurrentTime(), "数据库错误，原因%d: %s" % (e.args[0], e.args[1]) def dealData(self, my_dict): """先从数据库中查询这个影片名的记录，如果没有直接插入； 若是数据库中已经存在数据，那么什么都不做""" self.db.set_character_set('utf8') filmTitile = my_dict.get('film_title') result = self.findFilmByfilmTitle(filmTitile) if len(result) == 0: self.insertData(my_dict) return else: pass def findFilmByfilmTitle(self, filmTitile): """通过影片名称去数据库中查询""" self.db.set_character_set('utf8') sql = "select * from maoyan where film_title='%s'" % (filmTitile) self.cur.execute(sql) results = self.cur.fetchall() self.db.commit() return results 4、 万事俱备只欠启动了，跑啊跑啊。想必大家都想到了，猫眼肯定是不可能随便让人这样爬取他们的数据，而且短时间大量的请求对服务器的负载也是相当大的，不出意料爬取了不到300条数据，就被停止了。(ಥ _ ಥ)处理这种情况一般有两种方式，一种通过延缓请求，一种是通过代理ip的方式。第一种我尝试延缓5、10s，但并没有用。因而尝试用第二种方式，代理ip池的方式。代理ip如何使用在下一篇文章中记录。 下面是本次的完成代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133#!/usr/bin/env python# coding: utf-8import urllib2import urllibimport requestsfrom bs4 import BeautifulSoupfrom lxml import etreeimport MySQLdbimport timeimport randomimport sysreload(sys)sys.setdefaultencoding('utf8')"""猫眼不需要http请求的头部信息，添加上反而会出错"""class Maoyan(): # 获取当前时间 def getCurrentTime(self): return time.strftime('[%Y-%m-%d %H:%M:%S]', time.localtime(time.time())) def __init__(self): self.time = time self.base_url = "https://maoyan.com/films?showType=1&amp;sortId=2" try: self.db = MySQLdb.connect('localhost', 'flyer_user', 'Tu4a0X9hOPKz6jS!e', 'flyer_db',charset='utf8') self.cur = self.db.cursor() except MySQLdb.Error, e: print self.getCurrentTime(), "连接数据库错误，原因%d: %s" % (e.args[0], e.args[1]) # 插入数据 def insertData(self, my_dict): try: self.db.set_character_set('utf8') cols = ', '.join(my_dict.keys()) print cols values = '","'.join(my_dict.values()) print values sql = "INSERT INTO maoyan (%s) VALUES (%s)" % (cols, '"' + values + '"') try: result = self.cur.execute(sql) insert_id = self.db.insert_id() self.db.commit() # 判断是否执行成功 if result: return insert_id else: return 0 except MySQLdb.Error, e: # 发生错误时回滚 self.db.rollback() # 主键唯一，无法插入 if "key 'PRIMARY'" in e.args[1]: print self.getCurrentTime(), "数据已存在，未插入数据" else: print self.getCurrentTime(), "插入数据失败，原因 %d: %s" % (e.args[0], e.args[1]) except MySQLdb.Error, e: print self.getCurrentTime(), "数据库错误，原因%d: %s" % (e.args[0], e.args[1]) def dealData(self, my_dict): """先从数据库中查询这个影片名的记录，如果没有直接插入； 若是数据库中已经存在数据，那么什么都不做""" self.db.set_character_set('utf8') filmTitile = my_dict.get('film_title') result = self.findFilmByfilmTitle(filmTitile) if len(result) == 0: self.insertData(my_dict) return else: pass def findFilmByfilmTitle(self, filmTitile): """通过影片名称去数据库中查询""" self.db.set_character_set('utf8') sql = "select * from maoyan where film_title='%s'" % (filmTitile) self.cur.execute(sql) results = self.cur.fetchall() self.db.commit() return results def getHtml(self,url,proxys): """获取html""" req = urllib2.Request(url) html = urllib2.urlopen(req).read().decode('utf-8') return html def get_detail_url(self, targetUrl, proxys): """从概览页面中去获取剧透影片的详情页面""" html = self.getHtml(targetUrl, proxys) print html soup = BeautifulSoup(html, 'lxml') urls = soup.find_all('a', attrs=&#123;'data-act':'movie-click'&#125;) dd = soup.find_all('dd') print dd for url in urls: detailPath = etree.HTML(str(url)) detail_url = detailPath.xpath(r'//a/@href') for durl in detail_url: self.getContents('https://maoyan.com' + durl, proxys) def getContents(self, url, proxys): """从详情页面中获取所需data""" tmp1 = [] tmp2 = [] html = self.getHtml(url,proxys) detailPath = etree.HTML(html) film_name = detailPath.xpath(r'/html/body/div[3]/div/div[2]/div[1]/h3/text()') for name in film_name: tmp1.append("film_title") tmp2.append(name) pic_url = detailPath.xpath(r'/html/body/div[3]/div/div[1]/div/img/@src') for url in pic_url: strs = url.split('@') tmp1.append("pic_url") tmp2.append(strs[0]) release_time = detailPath.xpath(r'/html/body/div[3]/div/div[2]/div[1]/ul/li[3]/text()') for time in release_time: tmp1.append("release_time") tmp2.append(time[:10]) #将两个列表转换为字典，方便插入数据库 result_dict = dict(zip(tmp1, tmp2)) self.dealData(result_dict) def main(self): self.get_detail_url(self.base_url, proxys) for i in range(50): targetUrl = self.base_url + '&amp;offset=' + str((i+1)*30) self.get_detail_url(targetUrl, proxys)if __name__ == '__main__': maoyan = Maoyan() maoyan.main()]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python爬虫如何获取到只需要的数据]]></title>
    <url>%2F2017%2F09%2F18%2Fpython%E7%88%AC%E8%99%AB%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96%E5%88%B0%E5%8F%AA%E9%9C%80%E8%A6%81%E7%9A%84%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[python爬虫如何获取到只需要的数据 概述：在爬虫的学习过程中，最麻烦的也许就是如何从一大堆html标签中过滤出我们所需要的，在这里有三种方法：re正则表达式、BeautifulSoup、使用lxml中的xpath。 1 、先介绍如何使用lxml中的xpath(1) 我们需要安装相应的依赖，然后在项目中导入相应的模块 12from lxml import etreeimport lxml (2) 之后再代码中转化成xpath12detailPath = etree.HTML(html) questions_titile = detailPath.xpath('//ul[@class="list-group"]/li/div/div[@class="question-title"]/a/text()') 注意上边的两个单引号之间的一串，前面两个’//‘是表示从这里开始，然后一级一级的去寻找所需要的元素，如果想获取属性值，例如是图片的url，最后可以使用@href，如果想获取里边的文本值，那就使用text()。 (3) 特殊用法12345678910from lxml import etreehtml=&quot;&quot;&quot; &lt;body&gt; &lt;div id=&quot;aa&quot;&gt;aa&lt;/div&gt; &lt;div id=&quot;ab&quot;&gt;ab&lt;/div&gt; &lt;div id=&quot;ac&quot;&gt;ac&lt;/div&gt; &lt;/body&gt; &quot;&quot;&quot;selector=etree.HTML(html)content=selector.xpath(&apos;//div[starts-with(@id,&quot;a&quot;)]/text()&apos;) #这里使用starts-with方法提取div的id标签属性值开头为a的div标签 这篇文章有简单应用：http://blog.csdn.net/winterto1990/article/details/47903653介绍 最后一个大绝招：直接在chrome中开发者模式，右键找到你想寻找的元素标签，copy xpath就能将这个元素对应的xpath找到！ 1 、python中提供的re模块 正则表达式是一种更为强大的字符串匹配、字符串查找、字符串替换等操作工具。上篇讲解了正则表达式的基本概念和语法以及re模块的基本使用方式，这节来详细说说 re 模块作为 Python 正则表达式引擎提供了哪些便利性操作。正则表达式的所有操作都是围绕着匹配对象(Match)进行的，只有表达式与字符串匹配才有可能进行后续操作。判断匹配与否有两个方法，分别是 re.match() 和 re.search()，两者有什么区别呢？ 区别：match 方法从字符串的起始位置开始检查，如果刚好有一个子字符串与正则表达式相匹配，则返回一个Match对象，只要起始位置不匹配则退出，不再往后检查了，返回 None 1234&gt;&gt;&gt; re.match(r&quot;b.r&quot;, &quot;foobar&quot;) # 不匹配&gt;&gt;&gt; re.match(r&quot;b.r&quot;, &quot;barfoo&quot;) # 匹配&lt;_sre.SRE_Match object at 0x102f05b28&gt;&gt;&gt;&gt; search 方法虽然也是从起始位置开始检查，但是它在起始位置不匹配的时候会一直尝试往后检查，直到匹配为止，如果到字符串的末尾还没有匹配，则返回 None123&gt;&gt;&gt; re.search(r&quot;b.r&quot;, &quot;foobar&quot;) # 匹配&lt;_sre.SRE_Match object at 0x000000000254D578&gt;&gt;&gt;&gt; re.match(r&quot;b.r&quot;, &quot;foobr&quot;) # 不匹配 但是二者都是匹配到就停止匹配了，哪怕还有更多能匹配的也不管。 参考：https://foofish.net/re-tutorial.html 、https://foofish.net/crawler-re-second.html]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[上传本地jar包到nexus仓库中]]></title>
    <url>%2F2017%2F09%2F16%2F%E4%B8%8A%E4%BC%A0%E6%9C%AC%E5%9C%B0jar%E5%8C%85%E5%88%B0nexus%E4%BB%93%E5%BA%93%E4%B8%AD%2F</url>
    <content type="text"><![CDATA[#上传本地jar包到nexus仓库中 概述：有时在我们的nexus仓库中没有项目中所需的jar包，这个时候就需要从网络中下载，然后上传到nexus仓库中，然后我们就可以嗨森的敲代码啦。 1、首先需要是对应的repository有上传的权限 2、我们这里讲jar包上传到3rd party仓库中。先点击artifact upload然后选择GAV definition，手动将group id、artifact、version填入，然后packaging选择jar。 3、然后点击select artifact（s） to upload将本地的jar加入，点击add artifact，然后点击upload artifact。 4、最后刷新就能看到刚才加入的jar包。 此外需要注意的是，将入到3rd仓库之后可能对应的坐标会发生变化。如果前后坐标没有对应上，那么在pom文件中还是找不到相应的依赖。]]></content>
      <categories>
        <category>java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[hive如何实现自定义函数]]></title>
    <url>%2F2017%2F09%2F14%2Fhive%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[hive如何实现自定义函数 概述：我们在使用hive的时候，很多时候执行复杂的sql时，hive提供的函数无法满足我们的需求。这个时候就需要我们自己去定义一个函数。幸好，hive为我们允许使用java来子线自定义函数。 1、实现自定义UDF函数必须满足两个条件。 （1） 必须是org.apache.Hadoop.hive.ql.exec.UDF的子类 （2） 必须实现evaluate函数。 2、实现方法具体如下： （1）将$HIVE_HOME/lib中的两个jar包，分别是hive-contrib-2.1.1.jar和hive-exec-2.1.1.jar，把两个jar包放置到java的目录下，然后新建一个类，继承UDF。 123456789101112131415161718package org.apache.hadoop.hive.contrib.udf.example;import org.apache.commons.lang.StringUtils;import org.apache.hadoop.hive.ql.exec.UDF;import java.util.Arrays;public class ReturnMin extends UDF&#123; public static Integer evaluate(String str, String separator) &#123; int result = 100; if(StringUtils.isEmpty(str))&#123; return result; &#125; String[] strArray = str.split(separator); int[] intArray = new int[strArray.length]; for(int i = 0 ; i &lt; strArray.length ; i++) &#123; intArray[i] = Integer.parseInt(strArray[i]); &#125; Arrays.sort(intArray); return intArray[0]; &#125; (2) 然后将ReturnMin.class文件拷贝到hive-contrib-2.1.1.jar\org\apache\hadoop\hive\contrib\udf\example目录中，然后替换hive select在hive的cli中执行：CREATE FUNCTION ReturnMin AS ‘org.apache.hadoop.hive.contrib.udf.example.ReturnMin’;然后重新进入cli中，给这个函数传递一个数字的字符串和分隔符就能够将这个字符串最小的值返回。 hive删除一个自定义函数： drop function cutString;]]></content>
      <categories>
        <category>hive</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[hive中sql总结]]></title>
    <url>%2F2017%2F09%2F14%2Fhive%E4%B8%ADsql%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[hive中sql总结1、对指定的某一个列去重1insert into table allrecord partition (year=2017, month=08, day=25) select t.id, t.tssend, t.apmac, t.mac, t.rssi from (select id, tssend, apmac ,mac, rssi, row_number() over(distribute by mac sort by tssend )as rn from allrecords) t where t.rn=1; 上面试对字段mac去重，然后对tssend排序后插入allrecord这张表 2、删除分区1ALTER TABLE macinfo_2017 DROP IF EXISTS PARTITION (year=2017, month=03, day=22); 3、删除一张表，如果启动了回收站功能，那么这张表会保存在回收站里，也是能够恢复信息的。1drop table tablename; 若是表创建为外部表，则只会删除相应的原信息，对数据没有影响。 4、 创建一个partition_table002的表，使得表结构和partition_table001一样。1create table if not exists partition_table002 like partition_table001; 5、使用load加载数据到表中1load data local inpath 't_student1.txt' overwrite inte table t_student1; 6、创建外部表create EXTERNAL table IF NOT EXISTS macinfo_20170221( tssend string COMMENT 'Probe upload data time', apmac string COMMENT 'Probe MAC address', mac string COMMENT 'Mobile device MAC address', rssi string COMMENT 'WiFi signal strength of mobile devices') partitioned by (year int, month int, day int) ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe' WITH SERDEPROPERTIES ( "mac"="$.mac", "apmac"="$.apmac", "rssi"="$.rssi", "tssend"="$.tssend" ) STORED AS TEXTFILE LOCATION '/opt/local/software/a.txt'; 注意：这里创建外部表时会有坑，主要是数据存储格式的问题，我的数据存储为json格式，而hive并不直接支持读取json格式数据，需要添加相应的jar包才能解析数据。 7、hive外部表创建分区关联相应的目录（必须是目录）ALTER TABLE macinfo_2017 ADD PARTITION (year = 2017, month = 02, day = 21); 未完待续。。。。]]></content>
      <categories>
        <category>hive</category>
      </categories>
  </entry>
</search>
